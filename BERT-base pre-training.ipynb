{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSOLETE NOTEBOOK. REMOVED IN THE FUTURE\n",
    "==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "# ner_list = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\",\n",
    " 'These facts seemed to me to throw some light on the origin of species--that mystery of mysteries, as it has been called by one of our greatest philosophers.',\n",
    " 'On my return home, it occurred to me, in 1837, that something might perhaps be made out on this question by patiently accumulating and reflecting on all sorts of facts which could possibly have any bearing on it.',\n",
    " \"After five years' work I allowed myself to speculate on the subject, and drew up some short notes; these I enlarged in 1844 into a sketch of the conclusions, which then seemed to me probable: from that period to the present day I have steadily pursued the same object.\",\n",
    " 'I hope that I may be excused for entering on these personal details, as I give them to show that I have not been hasty in coming to a decision.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_MO_tokenization_and_masking(tokenizer, nlp_model, sequence: str):\n",
    "        \"\"\"\n",
    "        posoi: Part-Of-Speech of interest\n",
    "        \n",
    "        Performs whole-word-masking based on selected posoi.\n",
    "        \n",
    "        POS possibilities:['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "                             \n",
    "        TODO: What if no tokens are masked?\n",
    "        \n",
    "        \"\"\"\n",
    "        print('loading:', datetime.now().time())\n",
    "        spacy_sentence = nlp_model(sequence, disable=[\"parser\"])\n",
    "        \n",
    "        POS_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "        NER_list = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', \n",
    "                    'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "        NER_pairs = ['']\n",
    "        \n",
    "        input_ids = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        print(sequence)\n",
    "        print(input_tokens)\n",
    "        sequence_pos_list = [token.pos_ for token in spacy_sentence]\n",
    "        sequence_pos_frequency = {pos: sequence_pos_list.count(pos) for pos in sequence_pos_list}\n",
    "        \n",
    "        modified_input_list = []\n",
    "        \n",
    "        #POS-masking\n",
    "        print('pos-start:', datetime.now().time())\n",
    "        for posoi in sequence_pos_frequency.keys():\n",
    "            posoi_vocab = [token.text.lower() for token in spacy_sentence if token.pos_ == posoi]\n",
    "            \n",
    "            mask_indices = []\n",
    "            composite_word_indices = []\n",
    "            composite_word_tokens = []\n",
    "            for (i, token) in enumerate(input_tokens):\n",
    "                if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                    continue\n",
    "                elif token.startswith(\"##\"):\n",
    "                    composite_word_indices.append(i)\n",
    "                    composite_word_tokens.append(token)\n",
    "                    print(\"\".join([x.strip(\"##\") for x in composite_word_tokens]))\n",
    "                    if \"\".join([x.strip(\"##\") for x in composite_word_tokens]) in posoi_vocab:\n",
    "                        mask_indices = mask_indices + composite_word_indices\n",
    "\n",
    "                elif token in posoi_vocab:\n",
    "                    mask_indices.append(i)\n",
    "                else:\n",
    "                    composite_word_indices = [i]\n",
    "                    composite_word_tokens = [token]\n",
    "\n",
    "            mask_labels = [1 if i in mask_indices else 0 for i in range(len(input_tokens))]\n",
    "            masked_tokens = [x if mask_labels[i] == 0 else 103 for i,x in enumerate(input_ids)]\n",
    "            masked_input = tokenizer.decode(masked_tokens)         \n",
    "            modified_input_list.append(masked_input)\n",
    "\n",
    "        #POS-based lemmatization\n",
    "        replacement_tuples = [(token.text, token.lemma_) for token in spacy_sentence if token.text.lower() != token.lemma_]\n",
    "        #print(replacement_tuples)\n",
    "        pos_replaced_sentence = sequence\n",
    "        for replacement in replacement_tuples:\n",
    "            pos_replaced_sentence = re.sub(r'\\b' + replacement[0] + r'\\b', replacement[1], pos_replaced_sentence)\n",
    "\n",
    "        pos_replaced_sentence = pos_replaced_sentence.replace(\"  \", \" \")\n",
    "        print('Lemma', pos_replaced_sentence)\n",
    "        modified_input_list.append(pos_replaced_sentence)\n",
    "        \n",
    "        #NER-based swapping of time-place (if present)\n",
    "        print('ner-start:', datetime.now().time())\n",
    "        ner_swapped_sentence = spacy_sentence.text\n",
    "        for ent in spacy_sentence.ents:\n",
    "            if ent.label_ == 'TIME':\n",
    "                time_substring = ner_swapped_sentence[ent.start_char:ent.end_char].split(\" \")\n",
    "                time_substring.reverse()\n",
    "                ner_swapped_sentence = ner_swapped_sentence.replace(ner_swapped_sentence[ent.start_char:ent.end_char], \" \".join(time_substring))\n",
    "        print('NER', ner_swapped_sentence)\n",
    "        modified_input_list.append(ner_swapped_sentence)\n",
    "        \n",
    "        \n",
    "        #TODO future ideas\n",
    "        #\n",
    "        #\n",
    "        \n",
    "    \n",
    "        #actually tokenize input\n",
    "        inputs = tokenizer(modified_input_list, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        inputs['labels'] = tokenizer([sequence for i in range(0,inputs['input_ids'].shape[0])], \n",
    "                                     return_attention_mask=False, \n",
    "                                     return_token_type_ids=False,\n",
    "                                     return_tensors='pt', padding=True)['input_ids']\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:35:24.390461\n",
      "loading: 16:35:24.391457\n",
      "Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\n",
      "['anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'me', '.']\n",
      "pos-start: 16:35:24.401430\n",
      "heij\n",
      "heijn\n",
      "heij\n",
      "heijn\n",
      "heij\n",
      "heijn\n",
      "heij\n",
      "heijn\n",
      "heij\n",
      "heijn\n",
      "heij\n",
      "heijn\n",
      "heij\n",
      "heijn\n",
      "heij\n",
      "heijn\n",
      "heij\n",
      "heijn\n",
      "Lemma Anne go to the Albert Heijn at 5 o'clock to buy some milk for I.\n",
      "ner-start: 16:35:24.403425\n",
      "NER Anne went to the Albert Heijn at o'clock 5 to buy some milk for me.\n",
      "16:35:24.413399\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().time())\n",
    "test_sentence = \"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\"\n",
    "example_sentence_inputs = whole_word_MO_tokenization_and_masking(tokenizer=tokenizer, nlp_model=nlp, sequence=test_sentence)\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anne PROPN\n",
      "went VERB\n",
      "to ADP\n",
      "the DET\n",
      "Albert PROPN\n",
      "Heijn PROPN\n",
      "at ADP\n",
      "5 NUM\n",
      "o'clock NOUN\n",
      "to PART\n",
      "buy VERB\n",
      "some DET\n",
      "milk NOUN\n",
      "for ADP\n",
      "me PRON\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "spacy_sentence = nlp(\"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\")\n",
    "\n",
    "for token in spacy_sentence:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,  2253,  2000,  1996,   103,   103,   103,   103,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,   103,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,   103,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,   103,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,   103,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,   103,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,   103,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "           103,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,   103,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,   103,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "           103,   102],\n",
       "        [  101,  4776,  2175,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  1045,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1051,  1005,  5119,  1019,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   103,  2253,  2000,  1996,   103,   103,   103,   103,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,   103,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,   103,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,   103,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,   103,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,   103,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,   103,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "           103,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,   103,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,   103,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "           103,   102],\n",
       "        [  101,  4776,  2175,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  1045,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1051,  1005,  5119,  1019,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= 'On their very first meeting, Gilbert had not been pleasantly impressed with Hardy. But he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of Mexico\\'s casual visitors, and was ready to protect not only his own interests but those of any newcomers. He seemed to have the spirit of fair-mindedness; and he believed firmly in the possibilities of this magic land, particularly for young men. \"It\\'s God\\'s country,\" he told Gilbert on more than one occasion. \"Get into the soil all you can. Dig--and dig deep.\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: 16:35:24.483213\n",
      "On their very first meeting, Gilbert had not been pleasantly impressed with Hardy. But he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of Mexico's casual visitors, and was ready to protect not only his own interests but those of any newcomers. He seemed to have the spirit of fair-mindedness; and he believed firmly in the possibilities of this magic land, particularly for young men. \"It's God's country,\" he told Gilbert on more than one occasion. \"Get into the soil all you can. Dig--and dig deep.\"\n",
      "['on', 'their', 'very', 'first', 'meeting', ',', 'gilbert', 'had', 'not', 'been', 'pleasantly', 'impressed', 'with', 'hardy', '.', 'but', 'he', 'soon', 'saw', 'that', 'the', 'man', 'had', 'a', 'certain', 'rugged', 'strength', ',', 'and', 'there', 'was', 'no', 'doubt', 'he', 'had', 'suffered', 'from', 'the', 'de', '##pre', '##dation', '##s', 'of', 'mexico', \"'\", 's', 'casual', 'visitors', ',', 'and', 'was', 'ready', 'to', 'protect', 'not', 'only', 'his', 'own', 'interests', 'but', 'those', 'of', 'any', 'newcomers', '.', 'he', 'seemed', 'to', 'have', 'the', 'spirit', 'of', 'fair', '-', 'minded', '##ness', ';', 'and', 'he', 'believed', 'firmly', 'in', 'the', 'possibilities', 'of', 'this', 'magic', 'land', ',', 'particularly', 'for', 'young', 'men', '.', '\"', 'it', \"'\", 's', 'god', \"'\", 's', 'country', ',', '\"', 'he', 'told', 'gilbert', 'on', 'more', 'than', 'one', 'occasion', '.', '\"', 'get', 'into', 'the', 'soil', 'all', 'you', 'can', '.', 'dig', '-', '-', 'and', 'dig', 'deep', '.', '\"']\n",
      "pos-start: 16:35:24.521112\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "depre\n",
      "depredation\n",
      "depredations\n",
      "mindedness\n",
      "Lemma On their very first meeting, Gilbert have not be pleasantly impressed with Hardy. But he soon see that the man have a certain rugged strength, and there be no doubt he have suffer from the depredation of Mexicobe casual visitor, and be ready to protect not only his own interest but those of any newcomer. He seem to have the spirit of fair-mindedness; and he believe firmly in the possibility of this magic land, particularly for young man. \"Itbe Godbe country,\" he tell Gilbert on more than one occasion. \"Get into the soil all you can. Dig--and dig deep.\"\n",
      "ner-start: 16:35:24.530086\n",
      "NER On their very first meeting, Gilbert had not been pleasantly impressed with Hardy. But he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of Mexico's casual visitors, and was ready to protect not only his own interests but those of any newcomers. He seemed to have the spirit of fair-mindedness; and he believed firmly in the possibilities of this magic land, particularly for young men. \"It's God's country,\" he told Gilbert on more than one occasion. \"Get into the soil all you can. Dig--and dig deep.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  103, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006,  103,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        ...,\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ...,    0,    0,    0],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        ...,\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_word_MO_tokenization_and_masking(tokenizer=tokenizer, nlp_model=nlp, sequence=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: 16:35:24.608878\n",
      "\"Sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border,\" Pell was telling her. \"Some important men want to talk things over with me at Bisbee.I want to get started in a day or two.Don't take your maid.It's a rough country, but you'll be all right.Just old clothes.You can ride a lot, so bring your habit.I'll be busy most of the time; but I think you'll like the trip.Never been down that way, have you?\"\n",
      "['\"', 'stu', '##rg', '##is', 'telegraph', '##ed', 'me', 'that', 'there', 'was', 'a', 'big', 'possibility', 'of', 'a', 'new', 'vein', 'of', 'oil', 'down', 'on', 'the', 'border', ',', '\"', 'pe', '##ll', 'was', 'telling', 'her', '.', '\"', 'some', 'important', 'men', 'want', 'to', 'talk', 'things', 'over', 'with', 'me', 'at', 'bis', '##bee', '.', 'i', 'want', 'to', 'get', 'started', 'in', 'a', 'day', 'or', 'two', '.', 'don', \"'\", 't', 'take', 'your', 'maid', '.', 'it', \"'\", 's', 'a', 'rough', 'country', ',', 'but', 'you', \"'\", 'll', 'be', 'all', 'right', '.', 'just', 'old', 'clothes', '.', 'you', 'can', 'ride', 'a', 'lot', ',', 'so', 'bring', 'your', 'habit', '.', 'i', \"'\", 'll', 'be', 'busy', 'most', 'of', 'the', 'time', ';', 'but', 'i', 'think', 'you', \"'\", 'll', 'like', 'the', 'trip', '.', 'never', 'been', 'down', 'that', 'way', ',', 'have', 'you', '?', '\"']\n",
      "pos-start: 16:35:24.636802\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "sturg\n",
      "sturgis\n",
      "telegraphed\n",
      "pell\n",
      "bisbee\n",
      "Lemma \"Sturgis telegraph I that there be a big possibility of a new vein of oil down on the border,\" Pell be tell she. \"Some important man want to talk thing over with I at Bisbee.I want to get start in a day or two.Don't take your maid.Itbe a rough country, but you'll be all right.Just old clothe.You can ride a lot, so bring your habit.I'll be busy most of the time; but I think you'll like the trip.Never be down that way, have you?\"\n",
      "ner-start: 16:35:24.646775\n",
      "NER \"Sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border,\" Pell was telling her. \"Some important men want to talk things over with me at Bisbee.I want to get started in a day or two.Don't take your maid.It's a rough country, but you'll be all right.Just old clothes.You can ride a lot, so bring your habit.I'll be busy most of the time; but I think you'll like the trip.Never been down that way, have you?\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103, 24646,  ...,   103,   103,   102],\n",
       "        [  101,  1000,   103,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        ...,\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1000,   102,     0],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        ...,\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = '\"Sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border,\" Pell was telling her. \"Some important men want to talk things over with me at Bisbee.I want to get started in a day or two.Don\\'t take your maid.It\\'s a rough country, but you\\'ll be all right.Just old clothes.You can ride a lot, so bring your habit.I\\'ll be busy most of the time; but I think you\\'ll like the trip.Never been down that way, have you?\"'\n",
    "whole_word_MO_tokenization_and_masking(tokenizer=tokenizer, nlp_model=nlp, sequence=text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny_config = {\"hidden_size\": 128, \n",
    "                    \"hidden_act\": \"gelu\", \n",
    "                    \"initializer_range\": 0.02, \n",
    "                    \"vocab_size\": 30522, \n",
    "                    \"hidden_dropout_prob\": 0.1, \n",
    "                    \"num_attention_heads\": 2, \n",
    "                    \"type_vocab_size\": 2, \n",
    "                    \"max_position_embeddings\": 512, \n",
    "                    \"num_hidden_layers\": 2, \n",
    "                    \"intermediate_size\": 512, \n",
    "                    \"attention_probs_dropout_prob\": 0.1}\n",
    "\n",
    "\n",
    "model = BertForMaskedLM(config=BertConfig(**bert_tiny_config))\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**example_sentence_inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(10.2835, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.0200,  0.1465, -0.3637,  ...,  0.0395, -0.0656, -0.3321],\n",
       "         [ 0.0484, -0.2371, -0.4776,  ...,  0.1257,  0.0007, -0.1336],\n",
       "         [ 0.1460,  0.0925, -0.4497,  ..., -0.2280, -0.1593, -0.1718],\n",
       "         ...,\n",
       "         [-0.0520,  0.1370, -0.1459,  ..., -0.1432,  0.1006, -0.1996],\n",
       "         [-0.4075, -0.0481, -0.0996,  ..., -0.4267,  0.1172,  0.0081],\n",
       "         [ 0.1919,  0.5225, -0.4625,  ..., -0.0241,  0.0064, -0.0537]],\n",
       "\n",
       "        [[ 0.0859,  0.0773, -0.3543,  ...,  0.0837,  0.0407, -0.4331],\n",
       "         [-0.1709, -0.2265, -0.0413,  ..., -0.2139,  0.2773, -0.3632],\n",
       "         [ 0.1152,  0.0283, -0.6541,  ...,  0.0077, -0.0244,  0.0622],\n",
       "         ...,\n",
       "         [-0.0490,  0.2417, -0.0900,  ..., -0.1173,  0.0537, -0.1346],\n",
       "         [-0.1855, -0.0206, -0.2053,  ..., -0.6487,  0.2193, -0.1390],\n",
       "         [ 0.2551,  0.5884, -0.4756,  ..., -0.0008,  0.0868, -0.0447]],\n",
       "\n",
       "        [[ 0.0328,  0.0431, -0.3068,  ...,  0.1619,  0.0320, -0.3845],\n",
       "         [-0.0981, -0.0736, -0.0254,  ..., -0.2103,  0.1814, -0.3002],\n",
       "         [ 0.1531,  0.1814, -0.4593,  ..., -0.2603, -0.2520, -0.2548],\n",
       "         ...,\n",
       "         [-0.1563,  0.1633, -0.0087,  ..., -0.2039,  0.0708, -0.1751],\n",
       "         [-0.2668, -0.0247, -0.2652,  ..., -0.5092,  0.1892, -0.0336],\n",
       "         [ 0.1866,  0.4473, -0.5047,  ...,  0.0098,  0.0727, -0.0291]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0078,  0.1454, -0.2802,  ...,  0.1397,  0.1384, -0.3057],\n",
       "         [-0.1508, -0.2849,  0.0124,  ..., -0.2481,  0.1889, -0.3729],\n",
       "         [ 0.1030,  0.2591, -0.3431,  ..., -0.1514, -0.0982, -0.2426],\n",
       "         ...,\n",
       "         [-0.1164,  0.3944, -0.0503,  ..., -0.1908,  0.1224, -0.0462],\n",
       "         [-0.0120,  0.0167, -0.6175,  ...,  0.0244,  0.1759,  0.1588],\n",
       "         [ 0.2138,  0.6101, -0.3999,  ..., -0.0604, -0.1094,  0.2455]],\n",
       "\n",
       "        [[ 0.0028,  0.1832, -0.2891,  ...,  0.0553,  0.0671, -0.3734],\n",
       "         [-0.1614, -0.1778,  0.0059,  ..., -0.1459,  0.3156, -0.4027],\n",
       "         [-0.1497,  0.2848, -0.4498,  ..., -0.4812, -0.2517, -0.0952],\n",
       "         ...,\n",
       "         [ 0.0090,  0.1418, -0.2570,  ...,  0.1615,  0.1645,  0.0801],\n",
       "         [-0.1509,  0.0878, -0.2492,  ..., -0.6001,  0.1751, -0.0609],\n",
       "         [ 0.1824,  0.4644, -0.5950,  ...,  0.0459, -0.1348, -0.0518]],\n",
       "\n",
       "        [[ 0.0101,  0.2206, -0.3100,  ...,  0.1610,  0.0528, -0.3438],\n",
       "         [-0.1411, -0.1270, -0.0524,  ..., -0.2169,  0.2251, -0.2795],\n",
       "         [ 0.1564,  0.0174, -0.3196,  ..., -0.2359, -0.1417, -0.1465],\n",
       "         ...,\n",
       "         [-0.0834,  0.3513, -0.1837,  ..., -0.2103,  0.1264,  0.0233],\n",
       "         [-0.2484,  0.0464, -0.2418,  ..., -0.6482,  0.1683, -0.2133],\n",
       "         [ 0.2516,  0.6015, -0.5253,  ..., -0.0868,  0.0413, -0.0122]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(10.2790, grad_fn=<NllLossBackward>), logits=tensor([[[-5.9162e-02,  2.9790e-01, -4.1157e-01,  ...,  1.2211e-01,\n",
       "          -5.7689e-02, -4.1296e-01],\n",
       "         [ 5.6311e-02, -1.5754e-01, -4.6341e-01,  ...,  3.0909e-01,\n",
       "           1.1902e-02, -1.3802e-01],\n",
       "         [ 1.4994e-01,  1.5642e-01, -4.9551e-01,  ..., -1.0557e-01,\n",
       "          -1.2618e-01, -4.2685e-01],\n",
       "         ...,\n",
       "         [-4.0045e-01,  1.8661e-01,  6.2309e-02,  ..., -3.6598e-01,\n",
       "           2.0491e-01, -2.4264e-02],\n",
       "         [-2.5544e-01, -2.8214e-02, -2.7622e-01,  ..., -4.7163e-01,\n",
       "           1.0190e-01, -4.8512e-02],\n",
       "         [ 2.6299e-01,  5.7057e-01, -5.0931e-01,  ..., -9.6850e-02,\n",
       "          -8.2659e-02, -1.0471e-02]],\n",
       "\n",
       "        [[-1.4571e-01,  2.3586e-01, -4.0504e-01,  ...,  2.0158e-01,\n",
       "           1.2172e-01, -3.0752e-01],\n",
       "         [-1.4247e-01, -1.9875e-01, -1.2193e-02,  ..., -1.7029e-01,\n",
       "           3.3950e-01, -3.0286e-01],\n",
       "         [ 2.2047e-02,  6.3670e-02, -5.4502e-01,  ..., -1.1023e-01,\n",
       "           9.3503e-02,  1.1238e-01],\n",
       "         ...,\n",
       "         [-2.1735e-01,  1.4305e-01,  3.1804e-01,  ..., -2.7117e-01,\n",
       "           3.5800e-01,  6.2103e-02],\n",
       "         [-1.4273e-01,  3.1674e-02, -4.4365e-01,  ..., -5.6570e-01,\n",
       "           1.5561e-01,  1.2595e-01],\n",
       "         [ 6.2724e-02,  4.1092e-01, -4.1920e-01,  ..., -5.3445e-02,\n",
       "           9.1543e-02, -7.0227e-02]],\n",
       "\n",
       "        [[-6.5412e-02,  1.6927e-01, -3.8238e-01,  ...,  3.9359e-02,\n",
       "           1.3991e-02, -3.7519e-01],\n",
       "         [-1.5852e-01, -1.3722e-01, -1.0622e-01,  ..., -1.7385e-01,\n",
       "           2.0720e-01, -4.2916e-01],\n",
       "         [ 1.3872e-01,  1.7389e-01, -5.6565e-01,  ..., -3.2323e-01,\n",
       "          -1.5463e-01, -2.0415e-01],\n",
       "         ...,\n",
       "         [-7.4947e-02,  3.2813e-01, -8.7184e-02,  ..., -1.3059e-01,\n",
       "           1.3381e-01, -6.1130e-02],\n",
       "         [-2.6506e-01, -1.7069e-02, -2.1052e-01,  ..., -5.6429e-01,\n",
       "           1.7555e-01, -1.1316e-01],\n",
       "         [ 4.7674e-02,  4.9098e-01, -4.5367e-01,  ...,  4.0934e-02,\n",
       "           3.9199e-03, -7.7458e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-5.5113e-02,  1.8176e-01, -4.0541e-01,  ...,  1.3440e-02,\n",
       "          -3.1872e-02, -3.8048e-01],\n",
       "         [-3.0209e-01, -1.5687e-01, -1.1439e-01,  ..., -8.9923e-02,\n",
       "           2.3578e-01, -3.4901e-01],\n",
       "         [-7.7932e-03,  2.1503e-01, -3.8921e-01,  ..., -3.3148e-01,\n",
       "          -2.0314e-01, -3.2257e-01],\n",
       "         ...,\n",
       "         [ 1.2979e-02,  2.8104e-01, -5.6634e-04,  ..., -2.1187e-01,\n",
       "           1.0482e-01, -1.5147e-01],\n",
       "         [-2.0232e-02, -7.5761e-02, -5.2825e-01,  ...,  1.2960e-01,\n",
       "           1.5509e-01,  1.1990e-01],\n",
       "         [ 5.6913e-02,  5.5887e-01, -3.8615e-01,  ..., -3.5190e-02,\n",
       "          -1.4526e-01,  4.1315e-02]],\n",
       "\n",
       "        [[ 5.0508e-02,  3.0654e-01, -4.1537e-01,  ...,  2.2781e-02,\n",
       "          -1.5410e-02, -3.9141e-01],\n",
       "         [-1.5703e-01, -2.0132e-01,  4.3807e-02,  ..., -2.6268e-01,\n",
       "           1.8906e-01, -3.5518e-01],\n",
       "         [-7.4444e-02,  2.4531e-01, -5.5590e-01,  ..., -1.7197e-01,\n",
       "          -3.2284e-01, -2.3503e-01],\n",
       "         ...,\n",
       "         [-8.4524e-03,  2.2237e-01, -2.5488e-01,  ...,  2.1276e-01,\n",
       "           1.5486e-01,  6.6074e-02],\n",
       "         [-3.0376e-01, -9.2710e-02, -2.3534e-01,  ..., -5.0921e-01,\n",
       "           2.2491e-01, -7.6953e-02],\n",
       "         [ 1.2758e-01,  3.8072e-01, -3.1790e-01,  ...,  4.2882e-02,\n",
       "           1.0407e-01, -1.1761e-01]],\n",
       "\n",
       "        [[ 7.3516e-02,  2.6042e-01, -3.0411e-01,  ...,  8.4342e-02,\n",
       "           9.4266e-02, -3.7149e-01],\n",
       "         [-9.1222e-02, -1.1979e-01, -7.3050e-02,  ..., -7.8862e-02,\n",
       "           1.6719e-01, -2.2658e-01],\n",
       "         [ 9.5044e-02,  3.0523e-02, -3.4661e-01,  ..., -1.5828e-01,\n",
       "          -1.5279e-01, -3.3730e-01],\n",
       "         ...,\n",
       "         [-8.7435e-02,  3.3232e-01,  3.3870e-02,  ..., -1.4137e-01,\n",
       "           1.8811e-01, -6.9756e-02],\n",
       "         [-3.8105e-01, -1.5205e-01, -3.0890e-01,  ..., -7.6369e-01,\n",
       "          -5.6405e-03, -9.2499e-02],\n",
       "         [ 2.4288e-01,  5.1184e-01, -3.9056e-01,  ..., -2.5560e-02,\n",
       "           7.8428e-02, -5.3153e-02]]], grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**example_sentence_inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MODataset at 0x15ebfb5dd60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = {key: val for key, val in encodings.items() if key != 'labels'}\n",
    "        self.labels = encodings['labels']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MODataset(example_sentence_inputs)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=256,  # batch size per device during training\n",
    "    per_device_eval_batch_size=256,   # batch size for evaluation\n",
    "    learning_rate=1e-5,     \n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=None            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdcaf82b2dd4dacb9cb7dc7de82641b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c1f3bd80764dc087a2894ead371bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e541aff7e0a419d82cc9e51a494e95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a75363dbaf4534861fb2c6254f1836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=10.275548299153646)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom tokenizer\n",
    "====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer.tokenizer import StrategizedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_tokenizer = StrategizedTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  103, 2253,  ...,    0,    0,    0],\n",
       "        [ 101, 4776,  103,  ...,    0,    0,    0],\n",
       "        [ 101, 4776, 2253,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 4776, 2253,  ...,    0,    0,    0],\n",
       "        [ 101, 4776, 2175,  ...,    0,    0,    0],\n",
       "        [ 101, 4776, 2253,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 101, 4776, 2253,  ...,    0,    0,    0],\n",
       "        [ 101, 4776, 2253,  ...,    0,    0,    0],\n",
       "        [ 101, 4776, 2253,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 4776, 2253,  ...,    0,    0,    0],\n",
       "        [ 101, 4776, 2253,  ...,    0,    0,    0],\n",
       "        [ 101, 4776, 2253,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = ST_tokenizer.tokenize(\"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get() missing 1 required positional argument: 'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-855a23b39f32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: get() missing 1 required positional argument: 'key'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleaner_utils import super_cleaner\n",
    "from pretraining_data_utils import book_properties, make_df_book_properties\n",
    "from gutenberg.acquire import load_etext\n",
    "from tokenizer.tokenizer import StrategizedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = super_cleaner(load_etext(16968), -1, verify_deletions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny_config = {\"hidden_size\": 128, \n",
    "                    \"hidden_act\": \"gelu\", \n",
    "                    \"initializer_range\": 0.02, \n",
    "                    \"vocab_size\": 30522, \n",
    "                    \"hidden_dropout_prob\": 0.1, \n",
    "                    \"num_attention_heads\": 2, \n",
    "                    \"type_vocab_size\": 2, \n",
    "                    \"max_position_embeddings\": 512, \n",
    "                    \"num_hidden_layers\": 2, \n",
    "                    \"intermediate_size\": 512, \n",
    "                    \"attention_probs_dropout_prob\": 0.1}\n",
    "\n",
    "model = BertForMaskedLM(config=BertConfig(**bert_tiny_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.encodings['input_ids'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('89796.0 KB', '87.69140625 MB')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Size in KB\n",
    "#Number of entries\n",
    "#Tensor size\n",
    "#2 tensors, input_ids and labels\n",
    "#4 bytes\n",
    "#divide by 1024 to go to kilobytes\n",
    "\n",
    "def compute_filesize(entries, tensor_size):\n",
    "    return '{} KB'.format(entries*tensor_size*2*4/1024), '{} MB'.format(entries*tensor_size*2*4/(1024*1024))\n",
    "\n",
    "compute_filesize(22449, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('400000.0 KB', '390.625 MB')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_filesize(1e5, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4000000.0 KB', '3906.25 MB')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_filesize(1e6, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_footprint(parameters):\n",
    "    return '{} KB'.format(parameters*4/1024), '{} MB'.format(parameters*4/(1024*1024)), '{} GB'.format(parameters*4/(1024*1024*1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BERT-tiny: ', ('17187.5 KB', '16.78466796875 MB', '0.016391277313232422 GB'))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'BERT-tiny: ', memory_footprint(4.4 * 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BERT-base: ',\n",
       " ('430078.125 KB', '419.9981689453125 MB', '0.41015446186065674 GB'))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'BERT-base: ', memory_footprint(110.1 *1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GPT3 ', ('683593750.0 KB', '667572.021484375 MB', '651.925802230835 GB'))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'GPT3 ', memory_footprint(175 *1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing some classification model\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# From paper:\n",
    "# lr: 1e-4\n",
    "# Beta1 = 0.9 (default)\n",
    "# Beta2 = 0.999 (default)\n",
    "# L2 weight decay = 0.01\n",
    "\n",
    "# Longer sequences are disproportionately expensive\n",
    "# because attention is quadratic to the sequence\n",
    "# length. To speed up pretraing in our experiments,\n",
    "# we pre-train the model with sequence length of\n",
    "# 128 for 90% of the steps. Then, we train the rest\n",
    "# 10% of the steps of sequence of 512 to learn the\n",
    "# positional embeddings.\n",
    "\n",
    "\n",
    "\n",
    "#Batch size 256 for 1e6 steps\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = nlp(\"That's a lot better. He was finally walking to the beaches. There he had a meeting with his father. Afterwards, he read a book. The fishing rod that he used was really old\")\n",
    "doc = nlp(\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"San Francisco is a long drive away from here. Ah, I forgot what I was doing. He had to get a new pair of shoes.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
