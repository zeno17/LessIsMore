{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\",\n",
    " 'These facts seemed to me to throw some light on the origin of species--that mystery of mysteries, as it has been called by one of our greatest philosophers.',\n",
    " 'On my return home, it occurred to me, in 1837, that something might perhaps be made out on this question by patiently accumulating and reflecting on all sorts of facts which could possibly have any bearing on it.',\n",
    " \"After five years' work I allowed myself to speculate on the subject, and drew up some short notes; these I enlarged in 1844 into a sketch of the conclusions, which then seemed to me probable: from that period to the present day I have steadily pursued the same object.\",\n",
    " 'I hope that I may be excused for entering on these personal details, as I give them to show that I have not been hasty in coming to a decision.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_pos_tokenization_and_masking(sequence: str, nlp_model=None, posoi=\"VERB\"):\n",
    "        \"\"\"\n",
    "        posoi: Part-Of-Speech of interest\n",
    "        \n",
    "        Performs whole-word-masking based on selected posoi.\n",
    "        \n",
    "        POS possibilities:['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "                             \n",
    "        TODO: What if no tokens are masked?\n",
    "        \n",
    "        \"\"\"\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        spacy_sentence = nlp(sequence)\n",
    "        posoi_vocab = [token.text for token in spacy_sentence if token.pos_ == posoi]\n",
    "        input_ids = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        mask_indices = []\n",
    "        composite_word_indices = []\n",
    "        composite_word_tokens = []\n",
    "        for (i, token) in enumerate(input_tokens):\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                continue\n",
    "            elif token.startswith(\"##\"):\n",
    "                composite_word_indices.append(i)\n",
    "                composite_word_tokens.append(token)\n",
    "                if \"\".join([x.strip(\"##\") for x in composite_word_tokens]) in posoi_vocab:\n",
    "                    mask_indices = mask_indices + composite_word_indices\n",
    "                    \n",
    "            elif token in posoi_vocab:\n",
    "                mask_indices.append(i)\n",
    "            else:\n",
    "                composite_word_indices = [i]\n",
    "                composite_word_tokens = [token]\n",
    "                \n",
    "        mask_labels = [1 if i in mask_indices else 0 for i in range(len(input_tokens))]\n",
    "        masked_tokens = [x if mask_labels[i] == 0 else 103 for i,x in enumerate(input_ids)]\n",
    "        masked_input = tokenizer.decode(masked_tokens)\n",
    "        print(sequence)\n",
    "        print(masked_input)\n",
    "        \n",
    "        inputs = tokenizer(masked_input, return_tensors=\"pt\")\n",
    "        inputs['labels'] = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\n",
      "when on board h. m. s.'beagle,'as naturalist, i was much [MASK] with certain facts in the distribution of the inhabitants of south america, and in the geological relations of the present to the past inhabitants of that continent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2043,  2006,  2604,  1044,  1012,  1049,  1012,  1055,  1012,\n",
       "          1005, 26892,  9354,  1010,  1005,  2004, 19176,  1010,  1045,  2001,\n",
       "          2172,   103,  2007,  3056,  8866,  1999,  1996,  4353,  1997,  1996,\n",
       "          4864,  1997,  2148,  2637,  1010,  1998,  1999,  1996,  9843,  4262,\n",
       "          1997,  1996,  2556,  2000,  1996,  2627,  4864,  1997,  2008,  9983,\n",
       "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]]), 'labels': tensor([[  101,  2043,  2006,  2604,  1044,  1012,  1049,  1012,  1055,  1012,\n",
       "          1005, 26892,  9354,  1010,  1005,  2004, 19176,  1010,  1045,  2001,\n",
       "          2172,  4930,  2007,  3056,  8866,  1999,  1996,  4353,  1997,  1996,\n",
       "          4864,  1997,  2148,  2637,  1010,  1998,  1999,  1996,  9843,  4262,\n",
       "          1997,  1996,  2556,  2000,  1996,  2627,  4864,  1997,  2008,  9983,\n",
       "          1012,   102]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_inputs = whole_word_pos_tokenization_and_masking(\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\")\n",
    "example_sentence_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(config=BertConfig())\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**example_sentence_inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(10.4737, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.1085, -0.5772,  0.1293,  ...,  1.0368,  0.5574,  0.9810],\n",
       "         [-0.2183,  0.2934, -1.4217,  ...,  1.0051,  0.2181,  0.2821],\n",
       "         [ 0.1343, -0.0310, -0.2285,  ...,  0.5672,  0.5362,  0.3733],\n",
       "         ...,\n",
       "         [-0.0724,  0.0114, -0.5355,  ...,  1.0788,  0.3760,  1.0045],\n",
       "         [ 0.4110, -0.6894, -0.2067,  ...,  0.6501,  0.1130, -0.1410],\n",
       "         [ 0.1350,  0.8632, -0.0531,  ...,  0.8801, -0.3061,  0.2938]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(10.0098, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.3021,  0.2153,  0.3544,  ...,  0.4808,  0.4257,  0.3227],\n",
       "         [ 0.3361,  0.2841, -1.6514,  ...,  0.7961,  0.1002,  0.5828],\n",
       "         [-0.6459, -0.2280, -0.3719,  ...,  0.2865,  0.4029,  0.3031],\n",
       "         ...,\n",
       "         [ 0.2780,  0.7134, -0.7532,  ...,  1.0657,  0.1708,  0.6355],\n",
       "         [-0.3745,  0.1551, -0.5077,  ...,  0.7912,  0.3046, -0.2200],\n",
       "         [-0.8340, -0.1227, -0.5316,  ...,  0.2119,  0.2569,  0.1668]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**example_sentence_inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MODataset at 0x1b3cf5284f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = {key: val for key, val in encodings.items() if key != 'labels'}\n",
    "        self.labels = encodings['labels']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MODataset(example_sentence_inputs)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=256,  # batch size per device during training\n",
    "    per_device_eval_batch_size=256,   # batch size for evaluation\n",
    "    learning_rate=1e-5,     \n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=None            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb045c02ef44833a0f937057b1eb892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059f372fa0b74dc0b93bde42e69b6aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6078ea5fe1df405b809e90b62993a5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476489798bdd4ea0a5cb966422ec7b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=9.672528584798178)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_MO_tokenization_and_masking(sequence: str, nlp_model=None):\n",
    "        \"\"\"\n",
    "        posoi: Part-Of-Speech of interest\n",
    "        \n",
    "        Performs whole-word-masking based on selected posoi.\n",
    "        \n",
    "        POS possibilities:['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "                             \n",
    "        TODO: What if no tokens are masked?\n",
    "        \n",
    "        \"\"\"\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        # Disable some parts for efficiency\n",
    "        spacy_sentence = nlp(sequence, disable=[\"parser\"])\n",
    "        \n",
    "        POS_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "        NER_list = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', \n",
    "                    'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "        NER_pairs = ['']\n",
    "        \n",
    "        input_ids = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        sequence_pos_list = [token.pos_ for token in spacy_sentence]\n",
    "        sequence_pos_frequency = {pos: sequence_pos_list.count(pos) for pos in sequence_pos_list}\n",
    "        \n",
    "        modified_input_list = []\n",
    "        \n",
    "        #POS-masking\n",
    "        for posoi in sequence_pos_frequency.keys():\n",
    "            posoi_vocab = [token.text for token in spacy_sentence if token.pos_ == posoi]\n",
    "            \n",
    "            mask_indices = []\n",
    "            composite_word_indices = []\n",
    "            composite_word_tokens = []\n",
    "            for (i, token) in enumerate(input_tokens):\n",
    "                if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                    continue\n",
    "                elif token.startswith(\"##\"):\n",
    "                    composite_word_indices.append(i)\n",
    "                    composite_word_tokens.append(token)\n",
    "                    if \"\".join([x.strip(\"##\") for x in composite_word_tokens]) in posoi_vocab:\n",
    "                        mask_indices = mask_indices + composite_word_indices\n",
    "\n",
    "                elif token in posoi_vocab:\n",
    "                    mask_indices.append(i)\n",
    "                else:\n",
    "                    composite_word_indices = [i]\n",
    "                    composite_word_tokens = [token]\n",
    "\n",
    "            mask_labels = [1 if i in mask_indices else 0 for i in range(len(input_tokens))]\n",
    "            masked_tokens = [x if mask_labels[i] == 0 else 103 for i,x in enumerate(input_ids)]\n",
    "            masked_input = tokenizer.decode(masked_tokens)\n",
    "            \n",
    "            modified_input_list.append((posoi, masked_input))\n",
    "        \n",
    "        #POS-based lemmatization\n",
    "        replacement_tuples = [(token.text, token.lemma_) for token in spacy_sentence if token.text.lower() != token.lemma_]\n",
    "        print(replacement_tuples)\n",
    "        replaced_sentence = sequence\n",
    "        for replacement in replacement_tuples:\n",
    "            replaced_sentence = re.sub(r'\\b' + replacement[0] + r'\\b', replacement[1], replaced_sentence, flags=re.IGNORECASE)\n",
    "        replaced_sentence = replaced_sentence.replace(\"  \", \" \")\n",
    "        modified_input_list.append(('Lemma', replaced_sentence))\n",
    "        \n",
    "        #NER-based swapping of time-place (if present)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for mask in modified_input_list:\n",
    "            print(mask)\n",
    "        print(sequence)\n",
    "        inputs = tokenizer(modified_input_list, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        \n",
    "        inputs['labels'] = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "    \n",
    "def lemmatize_sequence(sequence, posoi='VERB'):\n",
    "    \"\"\"\n",
    "    TODO add probability?\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sequence)\n",
    "    replacement_tuples = [(token.text, token.lemma_) for token in doc if token.text.lower() != token.lemma_ and token.pos_ == posoi]\n",
    "    replaced_sentence = sequence\n",
    "    for replacement in replacement_tuples:\n",
    "        replaced_sentence = replaced_sentence.replace(replacement[0], \" \" + replacement[1])\n",
    "    replaced_sentence = replaced_sentence.replace(\"  \", \" \")\n",
    "    return sequence, replaced_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Anne', 'Anne'), ('went', 'go'), ('Albert', 'Albert'), ('Heijn', 'Heijn'), ('me', 'I'), ('went', 'go')]\n",
      "('PROPN', '[MASK] went to the [MASK] [MASK] [MASK] [MASK] to buy some milk. After that, me and my buddy went to the bathhouse.')\n",
      "('VERB', 'Anne [MASK] to the Albert Heijn to [MASK] some milk. After that, me and my buddy [MASK] to the bathhouse.')\n",
      "('ADP', 'Anne went [MASK] the Albert Heijn [MASK] buy some milk. [MASK] that, me and my buddy went [MASK] the bathhouse.')\n",
      "('DET', 'Anne went to [MASK] Albert Heijn to buy [MASK] milk. After [MASK], me and my buddy went to [MASK] bathhouse.')\n",
      "('PART', 'Anne went [MASK] the Albert Heijn [MASK] buy some milk. After that, me and my buddy went [MASK] the bathhouse.')\n",
      "('NOUN', 'Anne went to the Albert Heijn to buy some [MASK]. After that, me and my [MASK] went to the [MASK] [MASK].')\n",
      "('PUNCT', 'Anne went to the Albert Heijn to buy some milk [MASK] After that [MASK] me and my buddy went to the bathhouse [MASK]')\n",
      "('PRON', 'Anne went to the Albert Heijn to buy some milk. After that, [MASK] and [MASK] buddy went to the bathhouse.')\n",
      "('CCONJ', 'Anne went to the Albert Heijn to buy some milk. After that, me [MASK] my buddy went to the bathhouse.')\n",
      "('Lemma', 'Anne go to the Albert Heijn to buy some milk. After that, I and my buddy go to the bathhouse.')\n",
      "Anne went to the Albert Heijn to buy some milk. After that, me and my buddy went to the bathhouse.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11629, 17195,  2249,   102,   103,  1355,  1106,  1103,   103,\n",
       "           103,   103,   103,  1106,  4417,  1199,  6831,   119,  1258,  1115,\n",
       "           117,  1143,  1105,  1139, 16723,  1355,  1106,  1103, 10919,  3255,\n",
       "           119,   102],\n",
       "        [  101,   159,  9637,  2064,   102,  3967,   103,  1106,  1103,  3986,\n",
       "          1124,  1182, 22923,  1106,   103,  1199,  6831,   119,  1258,  1115,\n",
       "           117,  1143,  1105,  1139, 16723,   103,  1106,  1103, 10919,  3255,\n",
       "           119,   102],\n",
       "        [  101,  5844,  2101,   102,  3967,  1355,   103,  1103,  3986,  1124,\n",
       "          1182, 22923,   103,  4417,  1199,  6831,   119,   103,  1115,   117,\n",
       "          1143,  1105,  1139, 16723,  1355,   103,  1103, 10919,  3255,   119,\n",
       "           102,     0],\n",
       "        [  101, 18581,  1942,   102,  3967,  1355,  1106,   103,  3986,  1124,\n",
       "          1182, 22923,  1106,  4417,   103,  6831,   119,  1258,   103,   117,\n",
       "          1143,  1105,  1139, 16723,  1355,  1106,   103, 10919,  3255,   119,\n",
       "           102,     0],\n",
       "        [  101,  8544, 10460,   102,  3967,  1355,   103,  1103,  3986,  1124,\n",
       "          1182, 22923,   103,  4417,  1199,  6831,   119,  1258,  1115,   117,\n",
       "          1143,  1105,  1139, 16723,  1355,   103,  1103, 10919,  3255,   119,\n",
       "           102,     0],\n",
       "        [  101, 24819, 27370,   102,  3967,  1355,  1106,  1103,  3986,  1124,\n",
       "          1182, 22923,  1106,  4417,  1199,   103,   119,  1258,  1115,   117,\n",
       "          1143,  1105,  1139,   103,  1355,  1106,  1103,   103,   103,   119,\n",
       "           102,     0],\n",
       "        [  101,   153, 27370, 16647,   102,  3967,  1355,  1106,  1103,  3986,\n",
       "          1124,  1182, 22923,  1106,  4417,  1199,  6831,   103,  1258,  1115,\n",
       "           103,  1143,  1105,  1139, 16723,  1355,  1106,  1103, 10919,  3255,\n",
       "           103,   102],\n",
       "        [  101, 11629, 11414,   102,  3967,  1355,  1106,  1103,  3986,  1124,\n",
       "          1182, 22923,  1106,  4417,  1199,  6831,   119,  1258,  1115,   117,\n",
       "           103,  1105,   103, 16723,  1355,  1106,  1103, 10919,  3255,   119,\n",
       "           102,     0],\n",
       "        [  101, 21362, 11414,  4538,   102,  3967,  1355,  1106,  1103,  3986,\n",
       "          1124,  1182, 22923,  1106,  4417,  1199,  6831,   119,  1258,  1115,\n",
       "           117,  1143,   103,  1139, 16723,  1355,  1106,  1103, 10919,  3255,\n",
       "           119,   102],\n",
       "        [  101,  3180, 12917,   102,  3967,  1301,  1106,  1103,  3986,  1124,\n",
       "          1182, 22923,  1106,  4417,  1199,  6831,   119,  1258,  1115,   117,\n",
       "           146,  1105,  1139, 16723,  1301,  1106,  1103, 10919,  3255,   119,\n",
       "           102,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0]]), 'labels': tensor([[  101,  3967,  1355,  1106,  1103,  3986,  1124,  1182, 22923,  1106,\n",
       "          4417,  1199,  6831,   119,  1258,  1115,   117,  1143,  1105,  1139,\n",
       "         16723,  1355,  1106,  1103, 10919,  3255,   119,   102]])}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_word_MO_tokenization_and_masking(\"Anne went to the Albert Heijn to buy some milk. After that, me and my buddy went to the bathhouse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('forces', 'force'), ('was', 'be'), ('Britain', 'Britain'), ('negotiated', 'negotiate'), ('July', 'July'), ('began', 'begin'), ('Luftwaffe', 'Luftwaffe'), ('targeting', 'target'), ('convoys', 'convoy'), ('ports', 'port'), ('centres', 'centre'), ('Portsmouth', 'Portsmouth'), ('August', 'August'), ('Luftwaffe', 'Luftwaffe'), ('was', 'be'), ('directed', 'direct'), ('RAF', 'RAF'), ('incapacitating', 'incapacitate'), ('RAF', 'RAF'), ('Fighter', 'Fighter'), ('Command', 'Command'), ('days', 'day'), ('shifted', 'shift'), ('attacks', 'attack'), ('RAF', 'RAF'), ('airfields', 'airfield')]\n",
      "('DET', '[MASK] primary objective of [MASK] German forces was to compel Britain to agree to [MASK] negotiated peace settlement. In July 1940, [MASK] air and sea blockade began, with [MASK] Luftwaffe mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, [MASK] Luftwaffe was directed to achieve air superiority over [MASK] RAF, with [MASK] aim of incapacitating RAF Fighter Command ; 12 days later, it shifted [MASK] attacks to RAF airfields and infrastructure.')\n",
      "('ADJ', 'The [MASK] objective of the [MASK] forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting [MASK] - shipping convoys, as well as ports and shipping centres [MASK] as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days later, it shifted the attacks to RAF airfields and infrastructure.')\n",
      "('NOUN', 'The primary [MASK] of the German [MASK] was to compel Britain to agree to a negotiated [MASK] [MASK]. In July 1940, the [MASK] and [MASK] [MASK] began, with the Luftwaffe mainly targeting coastal - [MASK] [MASK], as well as [MASK] and [MASK] [MASK] such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve [MASK] [MASK] over the RAF, with the [MASK] of incapacitating RAF Fighter Command ; 12 [MASK] later, it shifted the [MASK] to RAF [MASK] [MASK] and [MASK].')\n",
      "('ADP', 'The primary objective [MASK] the German forces was [MASK] compel Britain [MASK] agree [MASK] a negotiated peace settlement. [MASK] July 1940, the air and sea blockade began, [MASK] the Luftwaffe mainly targeting coastal - shipping convoys, [MASK] well [MASK] ports and shipping centres such [MASK] Portsmouth. [MASK] 1 August, the Luftwaffe was directed [MASK] achieve air superiority [MASK] the RAF, [MASK] the aim [MASK] incapacitating RAF Fighter Command ; 12 days later, it shifted the attacks [MASK] RAF airfields and infrastructure.')\n",
      "('AUX', 'The primary objective of the German forces [MASK] to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe [MASK] directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days later, it shifted the attacks to RAF airfields and infrastructure.')\n",
      "('PART', 'The primary objective of the German forces was [MASK] compel Britain [MASK] agree [MASK] a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed [MASK] achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days later, it shifted the attacks [MASK] RAF airfields and infrastructure.')\n",
      "('VERB', 'The primary objective of the German forces was to [MASK] [MASK] Britain to [MASK] to a [MASK] peace settlement. In July 1940, the air and sea blockade [MASK], with the Luftwaffe mainly [MASK] coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was [MASK] to [MASK] air superiority over the RAF, with the aim of [MASK] [MASK] [MASK] [MASK] RAF Fighter Command ; 12 days later, it [MASK] the attacks to RAF airfields and infrastructure.')\n",
      "('PROPN', 'The primary objective of the German forces was to compel [MASK] to agree to a negotiated peace settlement. In [MASK] 1940, the air and sea blockade began, with the [MASK] mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as [MASK]. On 1 [MASK], the [MASK] was directed to achieve air superiority over the [MASK], with the aim of incapacitating [MASK] [MASK] [MASK] ; 12 days later, it shifted the attacks to [MASK] airfields and infrastructure.')\n",
      "('PUNCT', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement [MASK] In July 1940 [MASK] the air and sea blockade began [MASK] with the Luftwaffe mainly targeting coastal [MASK] shipping convoys [MASK] as well as ports and shipping centres such as Portsmouth [MASK] On 1 August [MASK] the Luftwaffe was directed to achieve air superiority over the RAF [MASK] with the aim of incapacitating RAF Fighter Command [MASK] 12 days later [MASK] it shifted the attacks to RAF airfields and infrastructure [MASK]')\n",
      "('NUM', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July [MASK], the air and sea blockade began, with the Luftwaffe mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On [MASK] August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; [MASK] days later, it shifted the attacks to RAF airfields and infrastructure.')\n",
      "('CCONJ', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air [MASK] sea blockade began, with the Luftwaffe mainly targeting coastal - shipping convoys, as well as ports [MASK] shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days later, it shifted the attacks to RAF airfields [MASK] infrastructure.')\n",
      "('ADV', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe [MASK] targeting coastal - shipping convoys, [MASK] [MASK] [MASK] ports and shipping centres such [MASK] Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days [MASK], it shifted the attacks to RAF airfields and infrastructure.')\n",
      "('PRON', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days later, [MASK] shifted the attacks to RAF airfields and infrastructure.')\n",
      "('Lemma', 'The primary objective of the German force be to compel Britain to agree to a negotiate peace settlement. In July 1940, the air and sea blockade begin, with the Luftwaffe mainly target coastal-shipping convoy, as well as port and shipping centre such as Portsmouth. On 1 August, the Luftwaffe be direct to achieve air superiority over the RAF, with the aim of incapacitate RAF Fighter Command; 12 day later, it shift the attack to RAF airfield and infrastructure.')\n",
      "The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal-shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command; 12 days later, it shifted the attacks to RAF airfields and infrastructure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 18581,  1942,  ...,   119,   102,     0],\n",
       "        [  101,  5844,  4538,  ...,   119,   102,     0],\n",
       "        [  101, 24819, 27370,  ...,   119,   102,     0],\n",
       "        ...,\n",
       "        [  101,  5844,  2559,  ...,   119,   102,     0],\n",
       "        [  101, 11629, 11414,  ...,   119,   102,     0],\n",
       "        [  101,  3180, 12917,  ...,   102,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 0],\n",
       "        [0, 0, 0,  ..., 1, 1, 0],\n",
       "        [0, 0, 0,  ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 0],\n",
       "        [0, 0, 0,  ..., 1, 1, 0],\n",
       "        [0, 0, 0,  ..., 1, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 0, 0]]), 'labels': tensor([[  101,  1109,  2425,  7649,  1104,  1103,  1528,  2088,  1108,  1106,\n",
       "          3254, 10522,  2855,  1106,  5340,  1106,   170, 14071,  3519,  3433,\n",
       "           119,  1130,  1351,  3020,   117,  1103,  1586,  1105,  2343, 17567,\n",
       "          1310,   117,  1114,  1103, 19027,  2871, 15141,  5869,   118,  8629,\n",
       "         26626,   117,  1112,  1218,  1112,  9267,  1105,  8629,  9335,  1216,\n",
       "          1112, 10867,   119,  1212,   122,  1360,   117,  1103, 19027,  1108,\n",
       "          2002,  1106,  5515,  1586, 21378,  1166,  1103,  6733,   117,  1114,\n",
       "          1103,  6457,  1104,  1107, 25265,  7409, 20563,  6733,  7388,  5059,\n",
       "           132,  1367,  1552,  1224,   117,  1122,  4707,  1103,  3690,  1106,\n",
       "          6733, 11897,  1116,  1105,  6557,   119,   102]])}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_word_MO_tokenization_and_masking(\"The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal-shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command; 12 days later, it shifted the attacks to RAF airfields and infrastructure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy testing\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN Apple apple\n",
      "is AUX be is\n",
      "looking VERB look looking\n",
      "at ADP at at\n",
      "aggresively ADV aggresively aggresively\n",
      "buying VERB buy buying\n",
      "U.K. PROPN U.K. u.k.\n",
      "startup NOUN startup startup\n",
      "for ADP for for\n",
      "$ SYM $ $\n",
      "1.2 NUM 1.2 1.2\n",
      "billion NUM billion billion\n",
      ". PUNCT . .\n",
      "They PRON they they\n",
      "walked VERB walk walked\n",
      "5 NUM 5 5\n",
      "km NOUN km km\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#en_core_web_trf doesnt work in spacy 3.0\n",
    "spacy_sentence = nlp(\"Apple is looking at aggresively buying U.K. startup for $1.2 billion. They walked 5 km\")\n",
    "test_pos_list = []\n",
    "for token in spacy_sentence:\n",
    "    test_pos_list.append(token.pos_)\n",
    "    print(token, token.pos_, token.lemma_, token.text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 6)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x, x+len(['ADV', 'VERB'])) for x in range(len(test_pos_list)) if test_pos_list[x:x+len(['ADV', 'VERB'])] == ['ADV', 'VERB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PROPN': 2,\n",
       " 'AUX': 1,\n",
       " 'VERB': 3,\n",
       " 'ADP': 2,\n",
       " 'ADV': 1,\n",
       " 'NOUN': 2,\n",
       " 'SYM': 1,\n",
       " 'NUM': 3,\n",
       " 'PUNCT': 1,\n",
       " 'PRON': 1}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_pos_list = [token.pos_ for token in spacy_sentence]\n",
    "sequence_pos_frequency = {pos: sequence_pos_list.count(pos) for pos in sequence_pos_list}\n",
    "sequence_pos_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 39 43 GPE\n",
      "$1.2 billion 56 68 MONEY\n",
      "5 82 83 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "for ent in spacy_sentence.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for startup'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = spacy_sentence.text[44:56].split(\" \")\n",
    "time.reverse()\n",
    "' '.join(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentence.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1b3cc0d8d10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1b40b7e5a90>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1b3cbd48d00>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1b3cbc0be20>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1b414456ac0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1b513751ec0>)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing some classification model\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_batch = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3055d0a5a0e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "labels = torch.tensor([1,0]).unsqueeze(0)\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = outputs[0]\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# From paper:\n",
    "# lr: 1e-4\n",
    "# Beta1 = 0.9 (default)\n",
    "# Beta2 = 0.999 (default)\n",
    "# L2 weight decay = 0.01\n",
    "\n",
    "# Longer sequences are disproportionately expensive\n",
    "# because attention is quadratic to the sequence\n",
    "# length. To speed up pretraing in our experiments,\n",
    "# we pre-train the model with sequence length of\n",
    "# 128 for 90% of the steps. Then, we train the rest\n",
    "# 10% of the steps of sequence of 512 to learn the\n",
    "# positional embeddings.\n",
    "\n",
    "\n",
    "\n",
    "#Batch size 256 for 1e6 steps\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForPreTraining(BertConfig())\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_string = \"\"\"ADJ: adjective\n",
    "ADP: adposition\n",
    "ADV: adverb\n",
    "AUX: auxiliary\n",
    "CCONJ: coordinating conjunction\n",
    "DET: determiner\n",
    "INTJ: interjection\n",
    "NOUN: noun\n",
    "NUM: numeral\n",
    "PART: particle\n",
    "PRON: pronoun\n",
    "PROPN: proper noun\n",
    "PUNCT: punctuation\n",
    "SCONJ: subordinating conjunction\n",
    "SYM: symbol\n",
    "VERB: verb\n",
    "X: other\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "print([x.split(\":\", 1)[0] for x in pos_string.split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When ADV when\n",
      "on ADP on\n",
      "board NOUN board\n",
      "H.M.S. PROPN H.M.S.\n",
      "' PUNCT '\n",
      "Beagle PROPN Beagle\n",
      ", PUNCT ,\n",
      "' PUNCT '\n",
      "as ADP as\n",
      "naturalist ADJ naturalist\n",
      ", PUNCT ,\n",
      "I PRON I\n",
      "was AUX be\n",
      "much ADV much\n",
      "struck VERB strike\n",
      "with ADP with\n",
      "certain ADJ certain\n",
      "facts NOUN fact\n",
      "in ADP in\n",
      "the DET the\n",
      "distribution NOUN distribution\n",
      "of ADP of\n",
      "the DET the\n",
      "inhabitants NOUN inhabitant\n",
      "of ADP of\n",
      "South PROPN South\n",
      "America PROPN America\n",
      ", PUNCT ,\n",
      "and CCONJ and\n",
      "in ADP in\n",
      "the DET the\n",
      "geological ADJ geological\n",
      "relations NOUN relation\n",
      "of ADP of\n",
      "the DET the\n",
      "present NOUN present\n",
      "to ADP to\n",
      "the DET the\n",
      "past ADJ past\n",
      "inhabitants NOUN inhabitant\n",
      "of ADP of\n",
      "that DET that\n",
      "continent NOUN continent\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\""
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When on board H.M.S. 'Beagle,' as naturalist, I was much strike with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\""
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_tuples = [(token.text, token.lemma_) for token in doc if token.pos_ == 'VERB']\n",
    "for replacement in replacement_tuples:\n",
    "    replaced_sentence = sentences[0].replace(replacement[0], replacement[1])\n",
    "replaced_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sequence(sequence, posoi='VERB'):\n",
    "    \"\"\"\n",
    "    TODO add probability?\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sequence)\n",
    "    replacement_tuples = [(token.text, token.lemma_) for token in doc if token.text.lower() != token.lemma_ and token.pos_ == posoi]\n",
    "    replaced_sentence = sequence\n",
    "    for replacement in replacement_tuples:\n",
    "        replaced_sentence = replaced_sentence.replace(replacement[0], \" \" + replacement[1])\n",
    "    replaced_sentence = replaced_sentence.replace(\"  \", \" \")\n",
    "    return sequence, replaced_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Anne went to the Albert Heijn to buy some milk. After that, me and my buddy went to the bathhouse',\n",
       " 'Anne go to the Albert Heijn to buy some milk. After that, me and my buddy go to the bathhouse')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_sequence(\"Anne went to the Albert Heijn to buy some milk. After that, me and my buddy went to the bathhouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That DET that\n",
      "'s AUX be\n",
      "a DET a\n",
      "lot NOUN lot\n",
      "better ADJ well\n",
      ". PUNCT .\n",
      "He PRON he\n",
      "was AUX be\n",
      "finally ADV finally\n",
      "walking VERB walk\n",
      "to ADP to\n",
      "the DET the\n",
      "beaches NOUN beach\n",
      ". PUNCT .\n",
      "There ADV there\n",
      "he PRON he\n",
      "had VERB have\n",
      "a DET a\n",
      "meeting NOUN meeting\n",
      "with ADP with\n",
      "his PRON his\n",
      "father NOUN father\n",
      ". PUNCT .\n",
      "Afterwards ADV afterwards\n",
      ", PUNCT ,\n",
      "he PRON he\n",
      "read VERB read\n",
      "a DET a\n",
      "book NOUN book\n",
      ". PUNCT .\n",
      "The DET the\n",
      "fishing NOUN fishing\n",
      "rod NOUN rod\n",
      "that DET that\n",
      "he PRON he\n",
      "used VERB use\n",
      "was AUX be\n",
      "really ADV really\n",
      "old ADJ old\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"That's a lot better. He was finally walking to the beaches. There he had a meeting with his father. Afterwards, he read a book. The fishing rod that he used was really old\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San PROPN San\n",
      "Francisco PROPN Francisco\n",
      "is AUX be\n",
      "a DET a\n",
      "long ADJ long\n",
      "drive NOUN drive\n",
      "away ADV away\n",
      "from ADP from\n",
      "here ADV here\n",
      ". PUNCT .\n",
      "Ah INTJ ah\n",
      ", PUNCT ,\n",
      "I PRON I\n",
      "forgot VERB forget\n",
      "what PRON what\n",
      "I PRON I\n",
      "was AUX be\n",
      "doing VERB do\n",
      ". PUNCT .\n",
      "He PRON he\n",
      "had VERB have\n",
      "to PART to\n",
      "get VERB get\n",
      "a DET a\n",
      "new ADJ new\n",
      "pair NOUN pair\n",
      "of ADP of\n",
      "shoes NOUN shoe\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"San Francisco is a long drive away from here. Ah, I forgot what I was doing. He had to get a new pair of shoes.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PERSON\\tPeople, including fictional.\\nNORP\\tNationalities or religious or political groups.\\nFAC\\tBuildings, airports, highways, bridges, etc.\\nORG\\tCompanies, agencies, institutions, etc.\\nGPE\\tCountries, cities, states.\\nLOC\\tNon-GPE locations, mountain ranges, bodies of water.\\nPRODUCT\\tObjects, vehicles, foods, etc. (Not services.)\\nEVENT\\tNamed hurricanes, battles, wars, sports events, etc.\\nWORK_OF_ART\\tTitles of books, songs, etc.\\nLAW\\tNamed documents made into laws.\\nLANGUAGE\\tAny named language.\\nDATE\\tAbsolute or relative dates or periods.\\nTIME\\tTimes smaller than a day.\\nPERCENT\\tPercentage, including â€%â€œ.\\nMONEY\\tMonetary values, including unit.\\nQUANTITY\\tMeasurements, as of weight or distance.\\nORDINAL\\tâ€œfirstâ€, â€œsecondâ€, etc.\\nCARDINAL\\tNumerals that do not fall under another type.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER_text = \"\"\"PERSON\tPeople, including fictional.\n",
    "NORP\tNationalities or religious or political groups.\n",
    "FAC\tBuildings, airports, highways, bridges, etc.\n",
    "ORG\tCompanies, agencies, institutions, etc.\n",
    "GPE\tCountries, cities, states.\n",
    "LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    "PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    "EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    "WORK_OF_ART\tTitles of books, songs, etc.\n",
    "LAW\tNamed documents made into laws.\n",
    "LANGUAGE\tAny named language.\n",
    "DATE\tAbsolute or relative dates or periods.\n",
    "TIME\tTimes smaller than a day.\n",
    "PERCENT\tPercentage, including â€%â€œ.\n",
    "MONEY\tMonetary values, including unit.\n",
    "QUANTITY\tMeasurements, as of weight or distance.\n",
    "ORDINAL\tâ€œfirstâ€, â€œsecondâ€, etc.\n",
    "CARDINAL\tNumerals that do not fall under another type.\"\"\"\n",
    "NER_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n"
     ]
    }
   ],
   "source": [
    "print([line.split(\"\\t\")[0] for line in NER_text.split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building custom transformer based models for pre-training\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.file_utils import ModelOutput\n",
    "from transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When ADV when\n",
      "on ADP on\n",
      "board NOUN board\n",
      "H.M.S. PROPN H.M.S.\n",
      "' PUNCT '\n",
      "Beagle PROPN Beagle\n",
      ", PUNCT ,\n",
      "' PUNCT '\n",
      "as ADP as\n",
      "naturalist ADJ naturalist\n",
      ", PUNCT ,\n",
      "I PRON I\n",
      "was AUX be\n",
      "much ADV much\n",
      "struck VERB strike\n",
      "with ADP with\n",
      "certain ADJ certain\n",
      "facts NOUN fact\n",
      "in ADP in\n",
      "the DET the\n",
      "distribution NOUN distribution\n",
      "of ADP of\n",
      "the DET the\n",
      "inhabitants NOUN inhabitant\n",
      "of ADP of\n",
      "South PROPN South\n",
      "America PROPN America\n",
      ", PUNCT ,\n",
      "and CCONJ and\n",
      "in ADP in\n",
      "the DET the\n",
      "geological ADJ geological\n",
      "relations NOUN relation\n",
      "of ADP of\n",
      "the DET the\n",
      "present NOUN present\n",
      "to ADP to\n",
      "the DET the\n",
      "past ADJ past\n",
      "inhabitants NOUN inhabitant\n",
      "of ADP of\n",
      "that DET that\n",
      "continent NOUN continent\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPreTraining(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "[DOCS]    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        next_sentence_label=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n",
    "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
    "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
    "        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n",
    "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
    "            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n",
    "\n",
    "            - 0 indicates sequence B is a continuation of sequence A,\n",
    "            - 1 indicates sequence B is a random sequence.\n",
    "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
    "            Used to hide legacy arguments that have been deprecated.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> from transformers import BertTokenizer, BertForPreTraining\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            >>> model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "            >>> outputs = model(**inputs)\n",
    "\n",
    "            >>> prediction_logits = outputs.prediction_logits\n",
    "            >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return BertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[When,\n",
       " on,\n",
       " board,\n",
       " H.M.S.,\n",
       " ',\n",
       " Beagle,\n",
       " ,,\n",
       " ',\n",
       " as,\n",
       " naturalist,\n",
       " ,,\n",
       " I,\n",
       " was,\n",
       " much,\n",
       " 'strike',\n",
       " with,\n",
       " certain,\n",
       " facts,\n",
       " in,\n",
       " the,\n",
       " distribution,\n",
       " of,\n",
       " the,\n",
       " inhabitants,\n",
       " of,\n",
       " South,\n",
       " America,\n",
       " ,,\n",
       " and,\n",
       " in,\n",
       " the,\n",
       " geological,\n",
       " relations,\n",
       " of,\n",
       " the,\n",
       " present,\n",
       " to,\n",
       " the,\n",
       " past,\n",
       " inhabitants,\n",
       " of,\n",
       " that,\n",
       " continent,\n",
       " .]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_doc = [token if token.pos_ != 'VERB' else token.lemma_ for token in doc]\n",
    "lemmatized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMOPreTrainingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Output type of :class:`~transformers.BertForPreTraining`.\n",
    "\n",
    "    Args:\n",
    "        loss (`optional`, returned when ``labels`` is provided, ``torch.FloatTensor`` of shape :obj:`(1,)`):\n",
    "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
    "            (classification) loss.\n",
    "        prediction_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        seq_relationship_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    prediction_logits: torch.FloatTensor = None\n",
    "    seq_relationship_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertPreTrainingLoss:\n",
    "    \"\"\"\n",
    "    Loss function suitable for BERT-like pretraining, that is, the task of pretraining a language model by combining\n",
    "    NSP + MLM. .. note:: Any label of -100 will be ignored (along with the corresponding logits) in the loss\n",
    "    computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(self, labels: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "        # make sure only labels that are not equal to -100\n",
    "        # are taken into account as loss\n",
    "        masked_lm_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), -100)\n",
    "        masked_lm_reduced_logits = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=logits[0], shape=(-1, shape_list(logits[0])[2])),\n",
    "            mask=masked_lm_active_loss,\n",
    "        )\n",
    "        masked_lm_labels = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), mask=masked_lm_active_loss\n",
    "        )\n",
    "        next_sentence_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), -100)\n",
    "        next_sentence_reduced_logits = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=logits[1], shape=(-1, 2)), mask=next_sentence_active_loss\n",
    "        )\n",
    "        next_sentence_label = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), mask=next_sentence_active_loss\n",
    "        )\n",
    "        masked_lm_loss = loss_fn(y_true=masked_lm_labels, y_pred=masked_lm_reduced_logits)\n",
    "        next_sentence_loss = loss_fn(y_true=next_sentence_label, y_pred=next_sentence_reduced_logits)\n",
    "        masked_lm_loss = tf.reshape(tensor=masked_lm_loss, shape=(-1, shape_list(next_sentence_loss)[0]))\n",
    "        masked_lm_loss = tf.reduce_mean(input_tensor=masked_lm_loss, axis=0)\n",
    "\n",
    "        return masked_lm_loss + next_sentence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "[DOCS]    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
