{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "ner_list = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\",\n",
    " 'These facts seemed to me to throw some light on the origin of species--that mystery of mysteries, as it has been called by one of our greatest philosophers.',\n",
    " 'On my return home, it occurred to me, in 1837, that something might perhaps be made out on this question by patiently accumulating and reflecting on all sorts of facts which could possibly have any bearing on it.',\n",
    " \"After five years' work I allowed myself to speculate on the subject, and drew up some short notes; these I enlarged in 1844 into a sketch of the conclusions, which then seemed to me probable: from that period to the present day I have steadily pursued the same object.\",\n",
    " 'I hope that I may be excused for entering on these personal details, as I give them to show that I have not been hasty in coming to a decision.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_pos_tokenization_and_masking(sequence: str, nlp_model=None, posoi=\"VERB\"):\n",
    "        \"\"\"\n",
    "        posoi: Part-Of-Speech of interest\n",
    "        \n",
    "        Performs whole-word-masking based on selected posoi.\n",
    "        \n",
    "        POS possibilities:['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "                             \n",
    "        TODO: What if no tokens are masked?\n",
    "        \n",
    "        \"\"\"\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        spacy_sentence = nlp(sequence)\n",
    "        posoi_vocab = [token.text for token in spacy_sentence if token.pos_ == posoi]\n",
    "        input_ids = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        mask_indices = []\n",
    "        composite_word_indices = []\n",
    "        composite_word_tokens = []\n",
    "        for (i, token) in enumerate(input_tokens):\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                continue\n",
    "            elif token.startswith(\"##\"):\n",
    "                composite_word_indices.append(i)\n",
    "                composite_word_tokens.append(token)\n",
    "                if \"\".join([x.strip(\"##\") for x in composite_word_tokens]) in posoi_vocab:\n",
    "                    mask_indices = mask_indices + composite_word_indices\n",
    "                    \n",
    "            elif token in posoi_vocab:\n",
    "                mask_indices.append(i)\n",
    "            else:\n",
    "                composite_word_indices = [i]\n",
    "                composite_word_tokens = [token]\n",
    "                \n",
    "        mask_labels = [1 if i in mask_indices else 0 for i in range(len(input_tokens))]\n",
    "        masked_tokens = [x if mask_labels[i] == 0 else 103 for i,x in enumerate(input_ids)]\n",
    "        masked_input = tokenizer.decode(masked_tokens)\n",
    "        print(sequence)\n",
    "        print(masked_input)\n",
    "        \n",
    "        inputs = tokenizer(masked_input, return_tensors=\"pt\")\n",
    "        inputs['labels'] = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\n",
      "when on board h. m. s.'beagle,'as naturalist, i was much [MASK] with certain facts in the distribution of the inhabitants of south america, and in the geological relations of the present to the past inhabitants of that continent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2043,  2006,  2604,  1044,  1012,  1049,  1012,  1055,  1012,\n",
       "          1005, 26892,  9354,  1010,  1005,  2004, 19176,  1010,  1045,  2001,\n",
       "          2172,   103,  2007,  3056,  8866,  1999,  1996,  4353,  1997,  1996,\n",
       "          4864,  1997,  2148,  2637,  1010,  1998,  1999,  1996,  9843,  4262,\n",
       "          1997,  1996,  2556,  2000,  1996,  2627,  4864,  1997,  2008,  9983,\n",
       "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]]), 'labels': tensor([[  101,  2043,  2006,  2604,  1044,  1012,  1049,  1012,  1055,  1012,\n",
       "          1005, 26892,  9354,  1010,  1005,  2004, 19176,  1010,  1045,  2001,\n",
       "          2172,  4930,  2007,  3056,  8866,  1999,  1996,  4353,  1997,  1996,\n",
       "          4864,  1997,  2148,  2637,  1010,  1998,  1999,  1996,  9843,  4262,\n",
       "          1997,  1996,  2556,  2000,  1996,  2627,  4864,  1997,  2008,  9983,\n",
       "          1012,   102]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_inputs = whole_word_pos_tokenization_and_masking(\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\")\n",
    "example_sentence_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(config=BertConfig())\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**example_sentence_inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(10.4704, grad_fn=<NllLossBackward>), logits=tensor([[[-0.3646,  0.5917, -0.1603,  ..., -0.7111, -0.0527,  0.4167],\n",
       "         [-1.0825,  0.4580, -0.3780,  ..., -0.4931, -0.5675,  0.1373],\n",
       "         [-0.1531,  0.0095, -0.2495,  ..., -0.3558, -0.2613,  0.1726],\n",
       "         ...,\n",
       "         [-0.5535,  0.4676,  0.1559,  ..., -0.1072, -0.4580, -0.3126],\n",
       "         [ 0.1005,  1.0893,  0.4038,  ...,  0.1370, -0.3725,  0.0565],\n",
       "         [-0.0343,  0.5544, -0.1182,  ..., -0.7318, -0.2411,  0.0796]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(10.0311, grad_fn=<NllLossBackward>), logits=tensor([[[-0.0676,  0.5329,  0.1456,  ..., -0.5788,  0.4360,  0.5184],\n",
       "         [-0.5390,  0.2936,  0.0171,  ..., -0.5895, -0.6560, -0.1490],\n",
       "         [-0.6003,  0.3584,  0.0722,  ..., -0.1600, -0.6146,  0.0249],\n",
       "         ...,\n",
       "         [-0.2442,  0.7161, -0.2941,  ..., -0.3834, -0.2195,  0.4626],\n",
       "         [-0.4728,  0.8677,  0.8608,  ...,  0.4586, -0.1241,  0.6362],\n",
       "         [-0.7090,  0.9102, -0.5073,  ...,  0.2590, -0.7039,  0.1553]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**example_sentence_inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MODataset at 0x224c7212160>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = {key: val for key, val in encodings.items() if key != 'labels'}\n",
    "        self.labels = encodings['labels']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MODataset(example_sentence_inputs)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=256,  # batch size per device during training\n",
    "    per_device_eval_batch_size=256,   # batch size for evaluation\n",
    "    learning_rate=1e-5,     \n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=None            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ebc159a12442d59349033dd86005cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a54815c6b349928adec3110137179c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aace0ee7366e44eea59f972308e3bb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868d45ad201f46f8b550372bc038c374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=9.627637227376303)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_MO_tokenization_and_masking(tokenizer, nlp_model, sequence: str):\n",
    "        \"\"\"\n",
    "        posoi: Part-Of-Speech of interest\n",
    "        \n",
    "        Performs whole-word-masking based on selected posoi.\n",
    "        \n",
    "        POS possibilities:['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "                             \n",
    "        TODO: What if no tokens are masked?\n",
    "        \n",
    "        \"\"\"\n",
    "        print('loading:', datetime.now().time())\n",
    "        spacy_sentence = nlp_model(sequence, disable=[\"parser\"])\n",
    "        \n",
    "        POS_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "        NER_list = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', \n",
    "                    'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "        NER_pairs = ['']\n",
    "        \n",
    "        input_ids = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        print(input_tokens)\n",
    "        sequence_pos_list = [token.pos_ for token in spacy_sentence]\n",
    "        print()\n",
    "        sequence_pos_frequency = {pos: sequence_pos_list.count(pos) for pos in sequence_pos_list}\n",
    "        \n",
    "        modified_input_list = []\n",
    "        \n",
    "        #POS-masking\n",
    "        print('pos-start:', datetime.now().time())\n",
    "        for posoi in sequence_pos_frequency.keys():\n",
    "            posoi_vocab = [token.text for token in spacy_sentence if token.pos_ == posoi]\n",
    "            \n",
    "            mask_indices = []\n",
    "            composite_word_indices = []\n",
    "            composite_word_tokens = []\n",
    "            for (i, token) in enumerate(input_tokens):\n",
    "                if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                    continue\n",
    "                elif token.startswith(\"##\"):\n",
    "                    composite_word_indices.append(i)\n",
    "                    composite_word_tokens.append(token)\n",
    "                    if \"\".join([x.strip(\"##\") for x in composite_word_tokens]) in posoi_vocab:\n",
    "                        mask_indices = mask_indices + composite_word_indices\n",
    "\n",
    "                elif token in posoi_vocab:\n",
    "                    mask_indices.append(i)\n",
    "                else:\n",
    "                    composite_word_indices = [i]\n",
    "                    composite_word_tokens = [token]\n",
    "\n",
    "            mask_labels = [1 if i in mask_indices else 0 for i in range(len(input_tokens))]\n",
    "            masked_tokens = [x if mask_labels[i] == 0 else 103 for i,x in enumerate(input_ids)]\n",
    "            masked_input = tokenizer.decode(masked_tokens)\n",
    "        \n",
    "            modified_input_list.append((posoi, masked_input))\n",
    "        \n",
    "        print('lemma-start:', datetime.now().time())\n",
    "        #POS-based lemmatization\n",
    "        replacement_tuples = [(token.text, token.lemma_) for token in spacy_sentence if token.text.lower() != token.lemma_]\n",
    "        pos_replaced_sentence = sequence\n",
    "        for replacement in replacement_tuples:\n",
    "            pos_replaced_sentence = re.sub(r'\\b' + replacement[0] + r'\\b', replacement[1], pos_replaced_sentence)\n",
    "\n",
    "        pos_replaced_sentence = pos_replaced_sentence.replace(\"  \", \" \")\n",
    "        modified_input_list.append(('Lemma', pos_replaced_sentence))\n",
    "        \n",
    "        #NER-based swapping of time-place (if present)\n",
    "        print('ner-start:', datetime.now().time())\n",
    "        ner_swapped_sentence = spacy_sentence.text\n",
    "        for ent in spacy_sentence.ents:\n",
    "            if ent.label_ == 'TIME':\n",
    "                time_substring = ner_swapped_sentence[ent.start_char:ent.end_char].split(\" \")\n",
    "                time_substring.reverse()\n",
    "                ner_swapped_sentence = ner_swapped_sentence.replace(ner_swapped_sentence[ent.start_char:ent.end_char], \" \".join(time_substring))\n",
    "                \n",
    "        modified_input_list.append(('NER', ner_swapped_sentence))\n",
    "        \n",
    "        \n",
    "        #TODO future ideas\n",
    "        \n",
    "        #Show all resulting sequences    \n",
    "        for mask in modified_input_list:\n",
    "            print(mask)            \n",
    "        print(sequence)\n",
    "        inputs = tokenizer(modified_input_list, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        \n",
    "        inputs['labels'] = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:55:18.417153\n",
      "loading: 12:55:18.417153\n",
      "['Anne', 'went', 'to', 'the', 'Albert', 'He', '##i', '##jn', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'me', '.']\n",
      "\n",
      "pos-start: 12:55:18.426132\n",
      "lemma-start: 12:55:18.427129\n",
      "ner-start: 12:55:18.427129\n",
      "('PROPN', \"[MASK] went to the [MASK] [MASK] [MASK] [MASK] at 5 o'clock to buy some milk for me.\")\n",
      "('VERB', \"Anne [MASK] to the Albert Heijn at 5 o'clock to [MASK] some milk for me.\")\n",
      "('ADP', \"Anne went [MASK] the Albert Heijn [MASK] 5 o'clock [MASK] buy some milk [MASK] me.\")\n",
      "('DET', \"Anne went to [MASK] Albert Heijn at 5 o'clock to buy [MASK] milk for me.\")\n",
      "('NUM', \"Anne went to the Albert Heijn at [MASK] o'clock to buy some milk for me.\")\n",
      "('NOUN', \"Anne went to the Albert Heijn at 5 o'clock to buy some [MASK] for me.\")\n",
      "('PART', \"Anne went [MASK] the Albert Heijn at 5 o'clock [MASK] buy some milk for me.\")\n",
      "('PRON', \"Anne went to the Albert Heijn at 5 o'clock to buy some milk for [MASK].\")\n",
      "('PUNCT', \"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me [MASK]\")\n",
      "('Lemma', \"Anne go to the Albert Heijn at 5 o'clock to buy some milk for I.\")\n",
      "('NER', \"Anne went to the Albert Heijn at o'clock 5 to buy some milk for me.\")\n",
      "Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\n",
      "12:55:18.432129\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().time())\n",
    "test_sentence = \"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\"\n",
    "whole_word_MO_tokenization_and_masking(tokenizer=tokenizer, nlp_model=nlp, sequence=test_sentence)\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "whole_word_MO_tokenization_and_masking() missing 2 required positional arguments: 'nlp_model' and 'sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-607b6f9a5390>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwhole_word_MO_tokenization_and_masking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal-shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command; 12 days later, it shifted the attacks to RAF airfields and infrastructure.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: whole_word_MO_tokenization_and_masking() missing 2 required positional arguments: 'nlp_model' and 'sequence'"
     ]
    }
   ],
   "source": [
    "whole_word_MO_tokenization_and_masking(tokenizer=tokenizer, nlp_model=nlp, \"The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal-shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command; 12 days later, it shifted the attacks to RAF airfields and infrastructure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy testing\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(It, 'PRON', 'it')\n",
      "('s, 'AUX', 'be')\n",
      "(in, 'ADP', 'in')\n",
      "(Best, 'PROPN', 'Best')\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#en_core_web_trf doesnt work in spacy 3.0\n",
    "#spacy_sentence = nlp(\"Apple is looking at aggresively buying U.K. startup for $1.2 billion. They walked 5 km\")\n",
    "spacy_sentence = nlp(\"It's in Best\")\n",
    "test_pos_list = []\n",
    "for token in spacy_sentence:\n",
    "    test_pos_list.append(token.pos_)\n",
    "    print((token, token.pos_, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'VERB', 'ADP', 'DET', 'PROPN', 'PROPN', 'ADP', 'NUM', 'NOUN', 'PART', 'VERB', 'DET', 'NOUN', 'ADP', 'PRON', 'PUNCT']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PROPN': 3,\n",
       " 'VERB': 2,\n",
       " 'ADP': 3,\n",
       " 'DET': 2,\n",
       " 'NUM': 1,\n",
       " 'NOUN': 2,\n",
       " 'PART': 1,\n",
       " 'PRON': 1,\n",
       " 'PUNCT': 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_pos_list = [token.pos_ for token in spacy_sentence]\n",
    "print(sequence_pos_list)\n",
    "sequence_pos_frequency = {pos: sequence_pos_list.count(pos) for pos in sequence_pos_list}\n",
    "sequence_pos_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anne 0 4 PERSON\n",
      "the Albert Heijn 13 29 ORG\n",
      "5 o'clock 33 42 TIME\n"
     ]
    }
   ],
   "source": [
    "for ent in spacy_sentence.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anne went to the Albert Heijn at o'clock 5 to buy some milk. After that, me and my buddy went home.\n"
     ]
    }
   ],
   "source": [
    "test_string = spacy_sentence.text\n",
    "for ent in spacy_sentence.ents:\n",
    "    if ent.label_ == 'TIME':\n",
    "        time_substring = test_string[ent.start_char:ent.end_char].split(\" \")\n",
    "        time_substring.reverse()\n",
    "        test_string = test_string.replace(test_string[ent.start_char:ent.end_char], \" \".join(time_substring))\n",
    "        \n",
    "        \n",
    "print(test_string)\n",
    "        \n",
    "        \n",
    "#print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"5 o'clock\""
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string[ent.start_char:ent.end_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m some buy o'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = spacy_sentence.text[44:56].split(\" \")\n",
    "time.reverse()\n",
    "' '.join(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentence.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1b3cc0d8d10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1b40b7e5a90>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1b3cbd48d00>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1b3cbc0be20>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1b414456ac0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1b513751ec0>)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing some classification model\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# From paper:\n",
    "# lr: 1e-4\n",
    "# Beta1 = 0.9 (default)\n",
    "# Beta2 = 0.999 (default)\n",
    "# L2 weight decay = 0.01\n",
    "\n",
    "# Longer sequences are disproportionately expensive\n",
    "# because attention is quadratic to the sequence\n",
    "# length. To speed up pretraing in our experiments,\n",
    "# we pre-train the model with sequence length of\n",
    "# 128 for 90% of the steps. Then, we train the rest\n",
    "# 10% of the steps of sequence of 512 to learn the\n",
    "# positional embeddings.\n",
    "\n",
    "\n",
    "\n",
    "#Batch size 256 for 1e6 steps\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\""
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When ADV when\n",
      "on ADP on\n",
      "board NOUN board\n",
      "H.M.S. PROPN H.M.S.\n",
      "' PUNCT '\n",
      "Beagle PROPN Beagle\n",
      ", PUNCT ,\n",
      "' PUNCT '\n",
      "as ADP as\n",
      "naturalist ADJ naturalist\n",
      ", PUNCT ,\n",
      "I PRON I\n",
      "was AUX be\n",
      "much ADV much\n",
      "struck VERB strike\n",
      "with ADP with\n",
      "certain ADJ certain\n",
      "facts NOUN fact\n",
      "in ADP in\n",
      "the DET the\n",
      "distribution NOUN distribution\n",
      "of ADP of\n",
      "the DET the\n",
      "inhabitants NOUN inhabitant\n",
      "of ADP of\n",
      "South PROPN South\n",
      "America PROPN America\n",
      ", PUNCT ,\n",
      "and CCONJ and\n",
      "in ADP in\n",
      "the DET the\n",
      "geological ADJ geological\n",
      "relations NOUN relation\n",
      "of ADP of\n",
      "the DET the\n",
      "present NOUN present\n",
      "to ADP to\n",
      "the DET the\n",
      "past ADJ past\n",
      "inhabitants NOUN inhabitant\n",
      "of ADP of\n",
      "that DET that\n",
      "continent NOUN continent\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "#doc = nlp(\"That's a lot better. He was finally walking to the beaches. There he had a meeting with his father. Afterwards, he read a book. The fishing rod that he used was really old\")\n",
    "doc = nlp(\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San PROPN San\n",
      "Francisco PROPN Francisco\n",
      "is AUX be\n",
      "a DET a\n",
      "long ADJ long\n",
      "drive NOUN drive\n",
      "away ADV away\n",
      "from ADP from\n",
      "here ADV here\n",
      ". PUNCT .\n",
      "Ah INTJ ah\n",
      ", PUNCT ,\n",
      "I PRON I\n",
      "forgot VERB forget\n",
      "what PRON what\n",
      "I PRON I\n",
      "was AUX be\n",
      "doing VERB do\n",
      ". PUNCT .\n",
      "He PRON he\n",
      "had VERB have\n",
      "to PART to\n",
      "get VERB get\n",
      "a DET a\n",
      "new ADJ new\n",
      "pair NOUN pair\n",
      "of ADP of\n",
      "shoes NOUN shoe\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"San Francisco is a long drive away from here. Ah, I forgot what I was doing. He had to get a new pair of shoes.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
