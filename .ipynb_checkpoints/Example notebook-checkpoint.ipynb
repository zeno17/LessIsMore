{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries involved in cleaning\n",
    "from cleaner_utils import super_cleaner\n",
    "from pretraining_data_utils import make_book_token_frequency, token_freq_df_to_dict, \\\n",
    "                                    all_available_tokens_from_df, optimize_book_subset_ratio\n",
    "from pretraining_data_utils import book_properties, make_df_book_properties\n",
    "from pretraining_data_utils import SentenceChunker, SentenceWriter\n",
    "from gutenberg.acquire import load_etext\n",
    "\n",
    "\n",
    "#Library utilities\n",
    "from tokenizer.tokenizer import StrategizedTokenizer\n",
    "from dataset.dataset import StrategizedTokenizerDataset\n",
    "from dataset.dataset import DefaultTokenizerDataset\n",
    "\n",
    "#Training code\n",
    "from transformers import BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from more_itertools import take\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cached_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read scraped metadata from the gutenberg metadata database \n",
    "#(Original data was scraped by using https://github.com/c-w/gutenberg)\n",
    "#The data is then further preprocessed by https://github.com/hugovk/gutenberg-metadata so it is actually usable.\n",
    "\n",
    "f = open(cache_dir + 'gutenberg-metadata.json', 'r')\n",
    "metadata = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13142"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve how many english books there in english\n",
    "english_book_keys = [key for key in metadata.keys() if metadata[key]['language'] == ['en']]\n",
    "len(english_book_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-0358ce9648a3>\", line 6, in <module>\n",
      "    super_cleaner(load_etext(14575), -1, verify_deletions=True)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gutenberg\\acquire\\text.py\", line 78, in load_etext\n",
      "    text = cache.read().decode('utf-8')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\gzip.py\", line 292, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\gzip.py\", line 470, in read\n",
      "    self._read_eof()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\gzip.py\", line 516, in _read_eof\n",
      "    raise BadGzipFile(\"CRC check failed %s != %s\" % (hex(crc32),\n",
      "gzip.BadGzipFile: CRC check failed 0x0 != 0xd0c5998f\n"
     ]
    }
   ],
   "source": [
    "# The third book cant be retrieved because of faults in retrieval. This happens sometimes.\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    super_cleaner(load_etext(14575), -1, verify_deletions=True)\n",
    "except Exception as e:\n",
    "    try:\n",
    "        exc_info = sys.exc_info()\n",
    "    finally:\n",
    "        # Display the *original* exception\n",
    "        traceback.print_exception(*exc_info)\n",
    "        del exc_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12640"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve how many english books there are actually loadable\n",
    "#If books arent cached this may take a while because it needs to scrape the books from gutenberg.org\n",
    "#Therefore i provide a pre-processed file\n",
    "if os.path.isfile(cache_dir + 'loadable_english_book_keys.pkl'):\n",
    "    with open(cache_dir + 'loadable_english_book_keys.pkl', 'rb') as f:\n",
    "        loadable_english_book_keys = pickle.load(f)              \n",
    "else:\n",
    "    loadable_english_book_keys = []\n",
    "    i = 0\n",
    "    for key in english_book_keys:\n",
    "        if i % 1000 == 0:\n",
    "            print(i, datetime.now())\n",
    "        i += 1\n",
    "        try:\n",
    "            load_etext(int(key))\n",
    "            loadable_english_book_keys.append(key)\n",
    "        except:\n",
    "            continue\n",
    "    with open(cache_dir + 'loadable_english_book_keys.pkl', 'wb') as f:\n",
    "        pickle.dump(loadable_english_book_keys, f)\n",
    "            \n",
    "len(loadable_english_book_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['17255', '1742', '14870', '14596', '23436', '22563', '15306', '15976', '1344', '13579']\n",
      "['15116', '23050', '22669', '22310', '18782', '10343', '1650', '21698', '16831', '11194', '14752', '14429', '16170', '2078', '13766', '12310', '23892', '16144', '22293', '19224']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly select 10 books that we can query\n",
    "np.random.seed(42)\n",
    "rand_10_books = [x for x in np.random.choice(loadable_english_book_keys, size=10)]\n",
    "rand_20_books = [x for x in np.random.choice(loadable_english_book_keys, size=20)]\n",
    "print(rand_10_books), print(rand_20_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17255 ['Alma-Tadema, Laurence'] ['The Wings of Icarus: Being the Life of one Emilia Fletcher']\n",
      "1742 ['Davis, Richard Harding'] ['Miss Civilization: A Comedy in One Act']\n",
      "14870 ['Hopkinson, Alfred, Sir'] ['Rebuilding Britain: A Survey of Problems of Reconstruction After the World War']\n",
      "14596 ['Inge, William Ralph'] ['Christian Mysticism']\n",
      "23436 ['Anonymous'] ['Aladdin or The Wonderful Lamp']\n"
     ]
    }
   ],
   "source": [
    "# Titles and authors for the first 5 books\n",
    "# 1 book isnt actually loadable, see below.\n",
    "for book_id in rand_10_books[:5]:\n",
    "    print(book_id, metadata[book_id]['author'], metadata[book_id]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing\n",
    "==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of John Gutenberg, by Franz von Dingelstedt\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org/license\\r\\n\\r\\n\\r\\nTitle: John Gutenberg\\r\\n       First Master Printer, His Acts and Most Remarkable\\r\\n       Discourses and his Death\\r\\n\\r\\nAuthor: Franz von Dingelstedt\\r\\n\\r\\nRelease Da'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original unprocessed text\n",
    "text = load_etext(50000)[:500]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of John Gutenberg, by Franz von Dingelstedt\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.org/license\r\n",
      "\r\n",
      "\r\n",
      "Title: John Gutenberg\r\n",
      "       First Master Printer, His Acts and Most Remarkable\r\n",
      "       Discourses and his Death\r\n",
      "\r\n",
      "Author: Franz von Dingelstedt\r\n",
      "\r\n",
      "Release Da\n"
     ]
    }
   ],
   "source": [
    "#Text with formatting\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cleaner to retrieve cleaned text from the first book of the random selection.\n",
    "The _super_cleaner_ strips a headers/disclaimers/tables that are not required for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = super_cleaner(load_etext(16968), -1, verify_deletions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \"And now for business,\" Lopez said. \"And remember zat he what tells a lie shall be right away shotted.\" In his excitement he lost the little English he had.',\n",
       " ' \"Put all ze men outside,\" Lopez ordered. Venustiano and Pedro, his chief lieutenants, obeyed at once, forcing them to march ahead of them, and standing guard over them near a great cactus bush a few feet from the adobe. \"Leave ze women with me,\" the bandit continued. \"But first, Alvarada, you find ze cook. I am \\'ongry.\"',\n",
       " ' \"Red\" Giddings had been on the ranch with Gilbert since the very beginning. He came from the North with the young man, willing to stake all on this one venture. Like young Jones, he was not afraid. He was an efficient, well-set-up young fellow, with three consuming passions: Arizona, his harmonica, and Angela Hardy. The first saw a lot of \"Red\"; the second touched his lips frequently; but as for Angela--well, perhaps the poor boy kissed his harmonica so often in order to forget her lips. But if his own music charmed \"Red,\" it failed to have that effect upon others--particularly Uncle Henry, who went into a rage whenever he heard the detested instrument. \"Red\\'s\" music had no charms to soothe the savage breast of Henry Smith.',\n",
       " ' \"What\\'s coming off?\" Gilbert said, looking about him, and not a little surprised to find a Mexican and his adherents in his adobe.',\n",
       " ' A heavy silence fell upon the men who were left in the room. The bandit, unconcerned, puffed his cigarette. Hardy and Pell felt like rats in a trap. Only Uncle Henry was passive. In the tense stillness, the clock could be heard ticking on and on. Pell was beginning to crack beneath the strain. Suddenly he began to pace the floor, his hands behind his back. No tiger in a cage was ever more impatient in his captivity.',\n",
       " ' A story of the Werewolves, made wonderfully credible and told with great skill and feeling. This is far from being an ordinary detective novel. Mr. Biss is on brand new ground and will puzzle every reader till the mystery is at last solved by the right man--the mystery of the baffling murders on the Brighton road.',\n",
       " ' A well-known English critic said of The Untamed--\"There are in it passages of extraordinary power--the whole conception is very bold.\" And no less bold nor less powerful is its sequel The Night Horseman. Once again we ride in company with \"Whistlin\\' Dan,\" the fearless, silent, mysterious chap who shares the instincts of wild things, and once again we engage with him in his desperate adventures, hair-breadth escapes, and whirlwind triumphs. A novel thrilling in its reality, which will not be put down by lovers of exciting fiction.',\n",
       " ' Deeper and deeper grew the darkness. Outside, indeed, the first stars had begun to shine, and soon the heavens were a miraculous glory. But there was no moon. Every road was hushed, and the trees waved their long arms in the gloom. The little machine that took Angela and her father home, rolled down the quiet valley. Its chug-chug was the only sound for miles around. \"Red\" was happy in the cool night. He rode all the way out to the Hardy ranch. He and Angela sang an old song, and let Jasper Hardy sit at the wheel and whirl them to the lights of home.',\n",
       " \" Immediately after, Lucia came in. She saw the body of her husband, the legs drawn up a bit, the arms stretched out, the wounded head turned so that the blood flowing from the forehead could not be seen. Only a few moments before, this limp, pitiful object had been speaking to her--calling her by name. It seemed incredible that Pell was powerless now to harm her. Brute though he had been, he gained, in this awesome instant, a strange glory, as the dead always do. The splendor of that universal experience was suddenly his; and, even lying there like a discarded meal-sack, he took on something of the pomp of a cardinal who had died. Never, of course, had she respected him more; and though she could not bring herself to shed a tear, she looked down at the still body, huddled in a heap, and craved one more word with him. No matter what has happened between a man and a woman; no matter what tragic hours they have known, when the moment of separation comes, there is always that wish to have explained a little more, to have taken a different course in all one's previous actions. It was not that she blamed herself; she had nothing on her conscience. But there was an instinctive dread at meeting the certain pain of this crisis.\",\n",
       " ' It was high noon, two days later. Gilbert again had been about the ranch looking things over. He had his dreamy moments, but he was far too practical to let the poet in him rule his life. One sensed, by the most cursory glance, that here was a type of virile young American who could not only dream, but make his dreams come true. No idler he! And he had no use for idlers. He had dared to come to this far country, establish himself on a ranch, and seek to win out in the face of overwhelming odds.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text is now a list of paragraphs\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"No.\"',\n",
       " '\"Gun?\"',\n",
       " '\"Why?\"',\n",
       " '\"Yes.\"',\n",
       " '\"Pells?\"',\n",
       " '\"A what?\"',\n",
       " '\"I have?\"',\n",
       " '\"Joking?\"',\n",
       " '\"Really?\"',\n",
       " '\"I ain\\'t!\"',\n",
       " '\"Kiss me!\"',\n",
       " '\"Uh--huh!\"',\n",
       " '\"In a way.\"',\n",
       " '\"What for?\"',\n",
       " '\"Yes, sir!\"',\n",
       " '\"Yes; why?\"',\n",
       " 'She nodded.',\n",
       " '\"All those?\"',\n",
       " '\"You won\\'t?\"',\n",
       " 'She started.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with some short sentences\n",
    "sorted(sentences, key=len)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \"And now for business,\" Lopez said. \"And remember zat he what tells a lie shall be right away shotted.\" In his excitement he lost the little English he had.',\n",
       " ' \"Put all ze men outside,\" Lopez ordered. Venustiano and Pedro, his chief lieutenants, obeyed at once, forcing them to march ahead of them, and standing guard over them near a great cactus bush a few feet from the adobe. \"Leave ze women with me,\" the bandit continued. \"But first, Alvarada, you find ze cook. I am \\'ongry.\"',\n",
       " ' \"Red\" Giddings had been on the ranch with Gilbert since the very beginning. He came from the North with the young man, willing to stake all on this one venture. Like young Jones, he was not afraid. He was an efficient, well-set-up young fellow, with three consuming passions: Arizona, his harmonica, and Angela Hardy. The first saw a lot of \"Red\"; the second touched his lips frequently; but as for Angela--well, perhaps the poor boy kissed his harmonica so often in order to forget her lips. But if his own music charmed \"Red,\" it failed to have that effect upon others--particularly Uncle Henry, who went into a rage whenever he heard the detested instrument. \"Red\\'s\" music had no charms to soothe the savage breast of Henry Smith.',\n",
       " ' \"What\\'s coming off?\" Gilbert said, looking about him, and not a little surprised to find a Mexican and his adherents in his adobe.',\n",
       " ' A heavy silence fell upon the men who were left in the room. The bandit, unconcerned, puffed his cigarette. Hardy and Pell felt like rats in a trap. Only Uncle Henry was passive. In the tense stillness, the clock could be heard ticking on and on. Pell was beginning to crack beneath the strain. Suddenly he began to pace the floor, his hands behind his back. No tiger in a cage was ever more impatient in his captivity.',\n",
       " ' A story of the Werewolves, made wonderfully credible and told with great skill and feeling. This is far from being an ordinary detective novel. Mr. Biss is on brand new ground and will puzzle every reader till the mystery is at last solved by the right man--the mystery of the baffling murders on the Brighton road.',\n",
       " ' A well-known English critic said of The Untamed--\"There are in it passages of extraordinary power--the whole conception is very bold.\" And no less bold nor less powerful is its sequel The Night Horseman. Once again we ride in company with \"Whistlin\\' Dan,\" the fearless, silent, mysterious chap who shares the instincts of wild things, and once again we engage with him in his desperate adventures, hair-breadth escapes, and whirlwind triumphs. A novel thrilling in its reality, which will not be put down by lovers of exciting fiction.',\n",
       " ' Deeper and deeper grew the darkness. Outside, indeed, the first stars had begun to shine, and soon the heavens were a miraculous glory. But there was no moon. Every road was hushed, and the trees waved their long arms in the gloom. The little machine that took Angela and her father home, rolled down the quiet valley. Its chug-chug was the only sound for miles around. \"Red\" was happy in the cool night. He rode all the way out to the Hardy ranch. He and Angela sang an old song, and let Jasper Hardy sit at the wheel and whirl them to the lights of home.',\n",
       " \" Immediately after, Lucia came in. She saw the body of her husband, the legs drawn up a bit, the arms stretched out, the wounded head turned so that the blood flowing from the forehead could not be seen. Only a few moments before, this limp, pitiful object had been speaking to her--calling her by name. It seemed incredible that Pell was powerless now to harm her. Brute though he had been, he gained, in this awesome instant, a strange glory, as the dead always do. The splendor of that universal experience was suddenly his; and, even lying there like a discarded meal-sack, he took on something of the pomp of a cardinal who had died. Never, of course, had she respected him more; and though she could not bring herself to shed a tear, she looked down at the still body, huddled in a heap, and craved one more word with him. No matter what has happened between a man and a woman; no matter what tragic hours they have known, when the moment of separation comes, there is always that wish to have explained a little more, to have taken a different course in all one's previous actions. It was not that she blamed herself; she had nothing on her conscience. But there was an instinctive dread at meeting the certain pain of this crisis.\",\n",
       " ' It was high noon, two days later. Gilbert again had been about the ranch looking things over. He had his dreamy moments, but he was far too practical to let the poet in him rule his life. One sensed, by the most cursory glance, that here was a type of virile young American who could not only dream, but make his dreams come true. No idler he! And he had no use for idlers. He had dared to come to this far country, establish himself on a ranch, and seek to win out in the face of overwhelming odds.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sentences)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2041, 5, 1532, 75140, 353]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find some properties about the book\n",
    "book_properties(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization in practice\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize custom tokenizer\n",
    "ST_tokenizer = StrategizedTokenizer(padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,  2253,  2000,  1996,   103,   103,   103,   103,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,   103,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,   103,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,   103,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,   103,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,   103,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,   103,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "           103,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,   103,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,   103,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "           103,   102],\n",
       "        [  101,  4776,  2175,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  1045,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1051,  1005,  5119,  1019,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = ST_tokenizer.tokenize(\"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', 'went', 'to', 'the', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', '[MASK]', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', '[MASK]', 'some', 'milk', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', '[MASK]', 'the', 'albert', 'he', '##ij', '##n', '[MASK]', '5', 'o', \"'\", 'clock', '[MASK]', 'buy', 'some', 'milk', '[MASK]', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', '[MASK]', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', '[MASK]', 'milk', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '[MASK]', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', '[MASK]', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', '[MASK]', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', '[MASK]', 'buy', 'some', 'milk', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', '[MASK]', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'me', '[MASK]', '[SEP]']\n",
      "['[CLS]', 'anne', 'go', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'i', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', 'o', \"'\", 'clock', '5', 'to', 'buy', 'some', 'milk', 'for', 'me', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#Masks are at different places\n",
    "for masked_line in ST_tokenizer.convert_ids_to_tokens(inputs['input_ids']):\n",
    "    print(masked_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load original tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [MASK] went to the [MASK] [MASK] [MASK] [MASK] at 5 o ' clock to buy some milk for me . [SEP]\n",
      "[CLS] anne [MASK] to the albert heijn at 5 o ' clock to [MASK] some milk for me . [SEP]\n",
      "[CLS] anne went [MASK] the albert heijn [MASK] 5 o ' clock [MASK] buy some milk [MASK] me . [SEP]\n",
      "[CLS] anne went to [MASK] albert heijn at 5 o ' clock to buy [MASK] milk for me . [SEP]\n",
      "[CLS] anne went to the albert heijn at [MASK] o ' clock to buy some milk for me . [SEP]\n",
      "[CLS] anne went to the albert heijn at 5 o ' clock to buy some [MASK] for me . [SEP]\n",
      "[CLS] anne went [MASK] the albert heijn at 5 o ' clock [MASK] buy some milk for me . [SEP]\n",
      "[CLS] anne went to the albert heijn at 5 o ' clock to buy some milk for [MASK] . [SEP]\n",
      "[CLS] anne went to the albert heijn at 5 o ' clock to buy some milk for me [MASK] [SEP]\n",
      "[CLS] anne go to the albert heijn at 5 o ' clock to buy some milk for i . [SEP]\n",
      "[CLS] anne went to the albert heijn at o ' clock 5 to buy some milk for me . [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in ST_tokenizer.convert_ids_to_tokens(inputs['input_ids']):\n",
    "    print(tokenizer.convert_tokens_to_string(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gutenberg book-selection\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-26 10:04:41.379072\n",
      "2021-05-26 10:05:14.122497\n"
     ]
    }
   ],
   "source": [
    "#Setting to ignore warnings about sequences being longer than BERT can handle\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "df_books_10 = make_df_book_properties(rand_10_books)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Shortest sentence (char)</th>\n",
       "      <th>Longest sentence (char)</th>\n",
       "      <th>Total tokens</th>\n",
       "      <th>Longest sequence (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17255</td>\n",
       "      <td>677</td>\n",
       "      <td>8</td>\n",
       "      <td>2083</td>\n",
       "      <td>41672</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1742</td>\n",
       "      <td>239</td>\n",
       "      <td>15</td>\n",
       "      <td>1051</td>\n",
       "      <td>8481</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14870</td>\n",
       "      <td>317</td>\n",
       "      <td>22</td>\n",
       "      <td>5189</td>\n",
       "      <td>62696</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14596</td>\n",
       "      <td>528</td>\n",
       "      <td>18</td>\n",
       "      <td>7685</td>\n",
       "      <td>115181</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23436</td>\n",
       "      <td>11</td>\n",
       "      <td>41</td>\n",
       "      <td>293</td>\n",
       "      <td>579</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22563</td>\n",
       "      <td>358</td>\n",
       "      <td>6</td>\n",
       "      <td>2344</td>\n",
       "      <td>16372</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15306</td>\n",
       "      <td>390</td>\n",
       "      <td>9</td>\n",
       "      <td>1672</td>\n",
       "      <td>40223</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15976</td>\n",
       "      <td>1419</td>\n",
       "      <td>10</td>\n",
       "      <td>1724</td>\n",
       "      <td>78126</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1344</td>\n",
       "      <td>263</td>\n",
       "      <td>6</td>\n",
       "      <td>6391</td>\n",
       "      <td>27092</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13579</td>\n",
       "      <td>870</td>\n",
       "      <td>13</td>\n",
       "      <td>1939</td>\n",
       "      <td>65500</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id num_sentences Shortest sentence (char) Longest sentence (char)  \\\n",
       "0   17255           677                        8                    2083   \n",
       "1    1742           239                       15                    1051   \n",
       "2   14870           317                       22                    5189   \n",
       "3   14596           528                       18                    7685   \n",
       "4   23436            11                       41                     293   \n",
       "5   22563           358                        6                    2344   \n",
       "6   15306           390                        9                    1672   \n",
       "7   15976          1419                       10                    1724   \n",
       "8    1344           263                        6                    6391   \n",
       "9   13579           870                       13                    1939   \n",
       "\n",
       "  Total tokens Longest sequence (tokens)  \n",
       "0        41672                       522  \n",
       "1         8481                       253  \n",
       "2        62696                      1022  \n",
       "3       115181                      1887  \n",
       "4          579                        71  \n",
       "5        16372                       530  \n",
       "6        40223                       383  \n",
       "7        78126                       453  \n",
       "8        27092                      1536  \n",
       "9        65500                       447  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Shortest sentence (char)</th>\n",
       "      <th>Longest sentence (char)</th>\n",
       "      <th>Total tokens</th>\n",
       "      <th>Longest sequence (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22563</td>\n",
       "      <td>358</td>\n",
       "      <td>6</td>\n",
       "      <td>2344</td>\n",
       "      <td>16372</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1344</td>\n",
       "      <td>263</td>\n",
       "      <td>6</td>\n",
       "      <td>6391</td>\n",
       "      <td>27092</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17255</td>\n",
       "      <td>677</td>\n",
       "      <td>8</td>\n",
       "      <td>2083</td>\n",
       "      <td>41672</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15306</td>\n",
       "      <td>390</td>\n",
       "      <td>9</td>\n",
       "      <td>1672</td>\n",
       "      <td>40223</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15976</td>\n",
       "      <td>1419</td>\n",
       "      <td>10</td>\n",
       "      <td>1724</td>\n",
       "      <td>78126</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13579</td>\n",
       "      <td>870</td>\n",
       "      <td>13</td>\n",
       "      <td>1939</td>\n",
       "      <td>65500</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1742</td>\n",
       "      <td>239</td>\n",
       "      <td>15</td>\n",
       "      <td>1051</td>\n",
       "      <td>8481</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14596</td>\n",
       "      <td>528</td>\n",
       "      <td>18</td>\n",
       "      <td>7685</td>\n",
       "      <td>115181</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14870</td>\n",
       "      <td>317</td>\n",
       "      <td>22</td>\n",
       "      <td>5189</td>\n",
       "      <td>62696</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23436</td>\n",
       "      <td>11</td>\n",
       "      <td>41</td>\n",
       "      <td>293</td>\n",
       "      <td>579</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id num_sentences Shortest sentence (char) Longest sentence (char)  \\\n",
       "5   22563           358                        6                    2344   \n",
       "8    1344           263                        6                    6391   \n",
       "0   17255           677                        8                    2083   \n",
       "6   15306           390                        9                    1672   \n",
       "7   15976          1419                       10                    1724   \n",
       "9   13579           870                       13                    1939   \n",
       "1    1742           239                       15                    1051   \n",
       "3   14596           528                       18                    7685   \n",
       "2   14870           317                       22                    5189   \n",
       "4   23436            11                       41                     293   \n",
       "\n",
       "  Total tokens Longest sequence (tokens)  \n",
       "5        16372                       530  \n",
       "8        27092                      1536  \n",
       "0        41672                       522  \n",
       "6        40223                       383  \n",
       "7        78126                       453  \n",
       "9        65500                       447  \n",
       "1         8481                       253  \n",
       "3       115181                      1887  \n",
       "2        62696                      1022  \n",
       "4          579                        71  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sort df and account for the fact that the column has both text and numbers\n",
    "df_books_10.sort_values(by='Shortest sentence (char)')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Shortest sentence (char)</th>\n",
       "      <th>Longest sentence (char)</th>\n",
       "      <th>Total tokens</th>\n",
       "      <th>Longest sequence (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14596</td>\n",
       "      <td>528</td>\n",
       "      <td>18</td>\n",
       "      <td>7685</td>\n",
       "      <td>115181</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15976</td>\n",
       "      <td>1419</td>\n",
       "      <td>10</td>\n",
       "      <td>1724</td>\n",
       "      <td>78126</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13579</td>\n",
       "      <td>870</td>\n",
       "      <td>13</td>\n",
       "      <td>1939</td>\n",
       "      <td>65500</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14870</td>\n",
       "      <td>317</td>\n",
       "      <td>22</td>\n",
       "      <td>5189</td>\n",
       "      <td>62696</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17255</td>\n",
       "      <td>677</td>\n",
       "      <td>8</td>\n",
       "      <td>2083</td>\n",
       "      <td>41672</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id num_sentences Shortest sentence (char) Longest sentence (char)  \\\n",
       "3   14596           528                       18                    7685   \n",
       "7   15976          1419                       10                    1724   \n",
       "9   13579           870                       13                    1939   \n",
       "2   14870           317                       22                    5189   \n",
       "0   17255           677                        8                    2083   \n",
       "\n",
       "  Total tokens Longest sequence (tokens)  \n",
       "3       115181                      1887  \n",
       "7        78126                       453  \n",
       "9        65500                       447  \n",
       "2        62696                      1022  \n",
       "0        41672                       522  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some books have very few tokens.\n",
    "df_books_10.sort_values(by='Total tokens', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Shortest sentence (char)</th>\n",
       "      <th>Longest sentence (char)</th>\n",
       "      <th>Total tokens</th>\n",
       "      <th>Longest sequence (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17255</td>\n",
       "      <td>677</td>\n",
       "      <td>8</td>\n",
       "      <td>2083</td>\n",
       "      <td>41672</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14870</td>\n",
       "      <td>317</td>\n",
       "      <td>22</td>\n",
       "      <td>5189</td>\n",
       "      <td>62696</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13579</td>\n",
       "      <td>870</td>\n",
       "      <td>13</td>\n",
       "      <td>1939</td>\n",
       "      <td>65500</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15976</td>\n",
       "      <td>1419</td>\n",
       "      <td>10</td>\n",
       "      <td>1724</td>\n",
       "      <td>78126</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14596</td>\n",
       "      <td>528</td>\n",
       "      <td>18</td>\n",
       "      <td>7685</td>\n",
       "      <td>115181</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id num_sentences Shortest sentence (char) Longest sentence (char)  \\\n",
       "0   17255           677                        8                    2083   \n",
       "2   14870           317                       22                    5189   \n",
       "9   13579           870                       13                    1939   \n",
       "7   15976          1419                       10                    1724   \n",
       "3   14596           528                       18                    7685   \n",
       "\n",
       "  Total tokens Longest sequence (tokens)  \n",
       "0        41672                       522  \n",
       "2        62696                      1022  \n",
       "9        65500                       447  \n",
       "7        78126                       453  \n",
       "3       115181                      1887  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books_10.sort_values(by='Total tokens').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-26 10:05:14.300538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:11<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-26 10:05:29.490304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Retrieve token occurences per book in a dataframe and another dataframe with total number of tokens\n",
    "print(datetime.now())\n",
    "df_book_token_freq_10, df_10_total_tokens = make_book_token_frequency(rand_10_books)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[PAD]</th>\n",
       "      <th>[unused0]</th>\n",
       "      <th>[unused1]</th>\n",
       "      <th>[unused2]</th>\n",
       "      <th>[unused3]</th>\n",
       "      <th>[unused4]</th>\n",
       "      <th>[unused5]</th>\n",
       "      <th>[unused6]</th>\n",
       "      <th>[unused7]</th>\n",
       "      <th>[unused8]</th>\n",
       "      <th>...</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17255</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14870</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23436</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22563</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15306</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15976</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13579</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  30522 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      [PAD] [unused0] [unused1] [unused2] [unused3] [unused4] [unused5]  \\\n",
       "17255     0         0         0         0         0         0         0   \n",
       "1742      0         0         0         0         0         0         0   \n",
       "14870     0         0         0         0         0         0         0   \n",
       "14596     0         0         0         0         0         0         0   \n",
       "23436     0         0         0         0         0         0         0   \n",
       "22563     0         0         0         0         0         0         0   \n",
       "15306     0         0         0         0         0         0         0   \n",
       "15976     0         0         0         0         0         0         0   \n",
       "1344      0         0         0         0         0         0         0   \n",
       "13579     0         0         0         0         0         0         0   \n",
       "\n",
       "      [unused6] [unused7] [unused8]  ... ## ## ## ## ## ## ## ## ##  \\\n",
       "17255         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "1742          0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "14870         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "14596         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "23436         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "22563         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "15306         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "15976         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "1344          0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "13579         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "\n",
       "      ##  \n",
       "17255   0  \n",
       "1742    0  \n",
       "14870   0  \n",
       "14596   0  \n",
       "23436   0  \n",
       "22563   0  \n",
       "15306   0  \n",
       "15976   0  \n",
       "1344    0  \n",
       "13579   0  \n",
       "\n",
       "[10 rows x 30522 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame is obviously very sparse\n",
    "df_book_token_freq_10[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17255     41679.0\n",
       "1742       8491.0\n",
       "14870     62696.0\n",
       "14596    115181.0\n",
       "23436       579.0\n",
       "22563     16372.0\n",
       "15306     40223.0\n",
       "15976     78170.0\n",
       "1344      27092.0\n",
       "13579     65500.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of tokens per book\n",
    "df_10_total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455983.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total number of tokens in our small set\n",
    "df_10_total_tokens.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  999,  1000,  1002, ..., 29645, 29664, 29667], dtype=int64), 15198)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All tokens which are present in our subsample of 20 books\n",
    "all_present_tokens_10 = all_available_tokens_from_df(df_book_token_freq_10)\n",
    "all_present_tokens_10, len(all_present_tokens_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('17255',\n",
       "  {'tokens': array([  999,  1000,  1005, ..., 29591, 29602, 29667], dtype=int64),\n",
       "   'total_tokens': 41679.0}),\n",
       " ('1742',\n",
       "  {'tokens': array([  999,  1000,  1005, ..., 28838, 29122, 29586], dtype=int64),\n",
       "   'total_tokens': 8491.0}),\n",
       " ('14870',\n",
       "  {'tokens': array([  999,  1000,  1005, ..., 29598, 29602, 29609], dtype=int64),\n",
       "   'total_tokens': 62696.0})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show first 3 entries\n",
    "tokens_per_book_10 = token_freq_df_to_dict(df_book_token_freq_10, df_10_total_tokens)\n",
    "take(3, tokens_per_book_10.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book best:  22563 new tokens:  3650 book_total_tokens:  16372.0 ratio:  0.22294160762277057\n",
      "book best:  23436 new tokens:  125 book_total_tokens:  579.0 ratio:  0.2158894645941278\n",
      "book best:  1344 new tokens:  2211 book_total_tokens:  27092.0 ratio:  0.08161080761848516\n",
      "book best:  1742 new tokens:  368 book_total_tokens:  8491.0 ratio:  0.0433400070663055\n",
      "book best:  15306 new tokens:  2157 book_total_tokens:  40223.0 ratio:  0.053626034855679586\n",
      "{'subset_booklist': ['22563', '23436', '1344', '1742', '15306'], 'subset_total_tokens': 92757.0, 'subset_present_tokens': array([  999.,  1000.,  1005., ..., 29602., 29664., 29667.]), 'subset_unique_tokens': 8511}\n"
     ]
    }
   ],
   "source": [
    "print(optimize_book_subset_ratio(all_present_tokens_10, tokens_per_book_10, threshold = 1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[PAD]</th>\n",
       "      <th>[unused0]</th>\n",
       "      <th>[unused1]</th>\n",
       "      <th>[unused2]</th>\n",
       "      <th>[unused3]</th>\n",
       "      <th>[unused4]</th>\n",
       "      <th>[unused5]</th>\n",
       "      <th>[unused6]</th>\n",
       "      <th>[unused7]</th>\n",
       "      <th>[unused8]</th>\n",
       "      <th>...</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30522 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    [PAD]  [unused0]  [unused1]  [unused2]  [unused3]  [unused4]  [unused5]  \\\n",
       "1       0          0          0          0          0          0          0   \n",
       "10      0          0          0          0          0          0          0   \n",
       "11      0          0          0          0          0          0          0   \n",
       "12      0          0          0          0          0          0          0   \n",
       "13      0          0          0          0          0          0          0   \n",
       "\n",
       "    [unused6]  [unused7]  [unused8]  ...  ##  ##  ##  ##  ##  ##  ##  \\\n",
       "1           0          0          0  ...    0    0    0    0    0    0    0   \n",
       "10          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "11          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "12          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "13          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "\n",
       "    ##  ##  ##  \n",
       "1     0    0    0  \n",
       "10    0    0    0  \n",
       "11    0    0    0  \n",
       "12    0    0    0  \n",
       "13    0    0    0  \n",
       "\n",
       "[5 rows x 30522 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_book_token_freq = pd.read_csv(os.path.join('../LessIsMore-cache','df_book_token_freq.csv'), index_col=0)\n",
    "df_book_token_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27833"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total number of unique tokens in the data\n",
    "len(np.flatnonzero(df_book_token_freq.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       7640.0\n",
       "10    901551.0\n",
       "11     36249.0\n",
       "12     40831.0\n",
       "13      6731.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total_tokens = pd.read_csv(os.path.join('../LessIsMore-cache','df_total_tokens.csv'), index_col=0).squeeze()\n",
    "df_total_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23398     0.0\n",
       "10802     0.0\n",
       "23524     0.0\n",
       "2305      0.0\n",
       "232       2.0\n",
       "22818     4.0\n",
       "19937    15.0\n",
       "22335    22.0\n",
       "20086    25.0\n",
       "23147    34.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total_tokens.sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True \n",
      "True \n",
      "True \n",
      "True \n",
      "True  _INFANT'S CABINET_\n",
      "True  _OF_\n",
      "True  BIRDS & BEASTS.\n",
      "True  _LONDON. Printed & Sold by Harvey & Darton._ 55, Gracechurch Street, 1820. Price 6d.\n",
      "True  [Illustration: The Stork.]\n",
      "True  [Illustration: The Robin.]\n",
      "True  [Illustration: The Hyena.]\n",
      "True  [Illustration: The Lion.]\n",
      "True  [Illustration: The Rhinoceros.]\n",
      "True  [Illustration: The Camel.]\n",
      "True  [Illustration: The Swan.]\n",
      "True  [Illustration: The Vulture.]\n",
      "True  [Illustration: The Lark.]\n",
      "True  [Illustration: The Turkey.]\n",
      "True  [Illustration: The Fox.]\n",
      "True  [Illustration: The Greyhound.]\n",
      "True  [Illustration: The Elephant.]\n",
      "True  [Illustration: The Zebra.]\n",
      "True  [Illustration: The Crow.]\n",
      "True  [Illustration: The Cock.]\n",
      "True  [Illustration: The Pigeon.]\n",
      "True  [Illustration: The Goldfinch.]\n",
      "True  [Illustration: The Buffalo.]\n",
      "True  [Illustration: The Hog.]\n",
      "True  [Illustration: The Horse.]\n",
      "True  [Illustration: The Stag.]\n",
      "True  [Illustration: The Chaffinch.]\n",
      "True  [Illustration: The Peacock.]\n",
      "True  [Illustration: The Guinea Hen.]\n",
      "True  [Illustration: The Blackbird.]\n",
      "True  [Illustration: The Ox.]\n",
      "True  [Illustration: The Wolf.]\n",
      "True  [Illustration: The Tiger.]\n",
      "True  [Illustration: The Baboon.]\n",
      "True  [Illustration: The Sparrow.]\n",
      "True  [Illustration: The Eagle.]\n",
      "True \n",
      "True  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Why do certain books have 0 tokens?\n",
    "#Well because it is an illustration-only book\n",
    "super_cleaner(load_etext(23398), -1, verify_deletions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True to complying with copyright laws. PGLAF has not verified that all the eBook files on these discs meet the copyright laws in countries outside of the United States. PGLAF recommends that you verify this before using these files and requests that you advise us of any problems by email to copyright AT pglaf.org\n",
      "True ** A note on CD and DVD disc capacity. It turns out that disk drive manufacturers (including the people who make CD and DVD burners and blank discs) measure disk space differently than the rest of the computer world. To them, 1MB, which is 1 megabyte, is 1,000,000 bytes. For the rest of the computer world, 1MB is 1,046,576 bytes. We mention this because people might read their DVD disc package and expect it to hold 4.7GB, but be surprised to find it can only hold about 4.37GB as the rest of the world measures space.\n",
      "True  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or because it is a DVD-cover and we only use the .txt file\n",
    "super_cleaner(load_etext(10802), -1, verify_deletions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['produced from scanned images of public domain material from the Google Print project.)']\n",
      "['by Virgil']\n"
     ]
    }
   ],
   "source": [
    "#Some books just have very little parsable information. This is often the case with books that are really really old \n",
    "#(e.g. writtenpre 1800s). The english in these books is often much different than modern day english.\n",
    "\n",
    "print(super_cleaner(load_etext(19937), -1))\n",
    "print(super_cleaner(load_etext(232), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "939505600.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many tokens do we have in total available?\n",
    "df_total_tokens.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_100K.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_100K.pkl', 'rb') as f:\n",
    "        subset_ratio_100K = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subset_booklist': ['232', '22818', '22335', '23594', '20086', '20360', '10557', '19571', '19177', '14100', '13536', '23436', '129', '21783', '11006', '19937', '22847', '1321', '23147', '21805', '22529', '12474', '13082', '14463', '23538', '13081', '116', '18589', '23446', '23450', '17124', '16780', '23146', '18935', '12554', '17254', '23429', '13203', '17365', '22236', '16169', '18417', '22579', '19634', '24044', '104', '1567', '23315', '24269', '12358', '23880'], 'subset_total_tokens': 99974.0, 'subset_present_tokens': array([  100.,   999.,  1000., ..., 29735., 29737., 29739.]), 'subset_unique_tokens': 13040}\n"
     ]
    }
   ],
   "source": [
    "print(subset_ratio_100K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_1M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_1M.pkl', 'rb') as f:\n",
    "        subset_1M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_1M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_1M.pkl', 'rb') as f:\n",
    "        subset_ratio_1M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_10M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_10M.pkl', 'rb') as f:\n",
    "        subset_10M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_10M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_10M.pkl', 'rb') as f:\n",
    "        subset_ratio_10M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_100M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_100M.pkl', 'rb') as f:\n",
    "        subset_ratio_100M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a file with the union of all books in the subsets (easy for processing on the server)\n",
    "\n",
    "union_subsets = {'subset_booklist': np.union1d(np.union1d(np.union1d(subset_ratio_100K['subset_booklist'],\n",
    "                                                          subset_ratio_1M['subset_booklist']),\n",
    "                                                          subset_ratio_10M['subset_booklist']), \n",
    "                                               subset_ratio_100M['subset_booklist'])}\n",
    "\n",
    "with open(os.path.join(cache_dir, 'subset_meta_ratio_union.pkl'), 'wb') as f:\n",
    "    pickle.dump(union_subsets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many tokens are actually represented by the data\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load original tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of the vocabulary\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocabulary contains a bunch of [unused] tokens which allow people to add their own tokens\n",
    "num_unused = 0\n",
    "for k,v in tokenizer.vocab.items():\n",
    "    if k.startswith('[unused'):\n",
    "        num_unused += 1\n",
    "num_unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1493"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for tokens which are either alone or a continued token, e.g. 'a' or '##a'\n",
    "num_char = 0\n",
    "for k,v in tokenizer.vocab.items():\n",
    "    #'a' or '##a'\n",
    "    if not v in subset_ratio_100M['subset_present_tokens'] and (len(k) == 1 or (len(k) == 3 and k.startswith('##'))):\n",
    "        num_char += 1\n",
    "        #print(k, v)\n",
    "num_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] 0\n",
      "[CLS] 101\n",
      "[SEP] 102\n",
      "[MASK] 103\n",
      "... 2133\n",
      "km 3186\n",
      "soundtrack 6050\n",
      "remix 6136\n",
      "c 6362\n",
      "uefa 6663\n",
      "playoff 7808\n",
      "midfielder 8850\n",
      "playstation 9160\n",
      "quarterfinals 9237\n",
      "pinyin 9973\n",
      "allmusic 10477\n",
      "mlb 10901\n",
      "espn 10978\n",
      "gameplay 11247\n",
      "nsw 11524\n",
      "nascar 11838\n",
      "itunes 11943\n",
      "lgbt 12010\n",
      "mvp 12041\n",
      "xbox 12202\n",
      "eurovision 12714\n",
      "vfl 13480\n",
      "kolkata 13522\n",
      "pga 14198\n",
      "m 14241\n",
      "bundesliga 14250\n",
      "metacritic 14476\n",
      "remixes 15193\n",
      "steelers 15280\n",
      "airplay 15341\n",
      "## 15414\n",
      "paralympics 15600\n",
      "zhao 15634\n",
      "reggae 15662\n",
      "linebacker 15674\n",
      "v8 15754\n",
      "hindwings 15998\n",
      "bollywood 16046\n",
      "podcast 16110\n",
      "atletico 16132\n",
      "wwf 16779\n",
      "transgender 16824\n",
      "paralympic 17029\n",
      "postseason 17525\n",
      "vhs 17550\n",
      "campeonato 17675\n",
      "multiplayer 17762\n",
      "odz 17814\n",
      "curated 17940\n",
      "iphone 18059\n",
      "gmbh 18289\n",
      "danielle 18490\n",
      "qaeda 18659\n",
      "mixtape 18713\n",
      " 18728\n",
      "##aw 19704\n",
      "##qing 19784\n",
      "saxophonist 19977\n",
      "preseason 20038\n",
      "pmid 20117\n",
      "keyboardist 20173\n",
      "iucn 20333\n",
      "pokemon 20421\n",
      "nrl 20686\n",
      "motorsports 20711\n",
      "jaenelle 20757\n",
      "beyonce 20773\n",
      "airbus 20901\n",
      "netflix 20907\n",
      "motorsport 21044\n",
      "belgarath 21256\n",
      "iaaf 21259\n",
      "guangdong 21287\n",
      "shortlisted 21353\n",
      "seahawks 21390\n",
      "lucivar 21401\n",
      "goaltender 21437\n",
      "nanjing 21455\n",
      "crambidae 21585\n",
      "frontman 21597\n",
      "cbn 21824\n",
      "odisha 21874\n",
      "alzheimer 21901\n",
      "kinase 21903\n",
      "futsal 21921\n",
      "wta 21925\n",
      "nokia 22098\n",
      "smackdown 22120\n",
      "concacaf 22169\n",
      "oricon 22237\n",
      "twenty20 22240\n",
      "nfc 22309\n",
      "tbilisi 22406\n",
      "nrhp 22424\n",
      "showcased 22443\n",
      "nmi 22484\n",
      "kaladin 22588\n",
      "storylines 22628\n",
      "## 22646\n",
      "riaa 22716\n",
      "##a 22972\n",
      "mitochondrial 23079\n",
      "haryana 23261\n",
      "##saw 23305\n",
      "issn 23486\n",
      "saetan 23515\n",
      "rbis 23583\n",
      "deportivo 23696\n",
      "neuroscience 23700\n",
      "telangana 23764\n",
      "lviv 23814\n",
      "scientology 23845\n",
      "bwv 23860\n",
      "transitioned 23946\n",
      "yamaha 24031\n",
      "maccabi 24055\n",
      "lexie 24123\n",
      "superfamily 24169\n",
      "hapoel 24208\n",
      "wcw 24215\n",
      "soundtracks 24245\n",
      "debuting 24469\n",
      "bmg 24499\n",
      "sequencing 24558\n",
      "ho 24833\n",
      "myspace 24927\n",
      "##strae 24967\n",
      "smashwords 25151\n",
      "wrocaw 25160\n",
      "cmll 25395\n",
      "rihanna 25439\n",
      "wnba 25554\n",
      "brianna 25558\n",
      "filmfare 25648\n",
      "hezbollah 25713\n",
      "cheerleading 25721\n",
      "telenovela 25754\n",
      "shandong 25768\n",
      "## 25799\n",
      "erebidae 25875\n",
      "superliga 25922\n",
      "hyundai 25983\n",
      "springsteen 26002\n",
      "euroleague 26093\n",
      "stanisaw 26133\n",
      "tianjin 26216\n",
      "vh1 26365\n",
      "smartphone 26381\n",
      "alyssa 26442\n",
      "detainees 26485\n",
      "kayla 26491\n",
      "benfica 26542\n",
      "shenzhen 26555\n",
      "godzilla 26631\n",
      "zhejiang 26805\n",
      "2010s 26817\n",
      "cornerback 26857\n",
      "capcom 26861\n",
      "iihf 26904\n",
      "mukherjee 27040\n",
      "polgara 27041\n",
      "joyah 27098\n",
      "multicultural 27135\n",
      "mbc 27262\n",
      "jillian 27286\n",
      "## 27392\n",
      "metadata 27425\n",
      "showcasing 27696\n",
      "minogue 27736\n",
      "##rae 27807\n",
      "##1 27944\n",
      "00pm 27995\n",
      "wrestlemania 28063\n",
      "1910s 28088\n",
      "jiangsu 28091\n",
      "iqbal 28111\n",
      "biomass 28148\n",
      "prequel 28280\n",
      "fivb 28423\n",
      "kaitlyn 28584\n",
      "goalscorer 28602\n",
      "scuba 28651\n",
      "##genase 28835\n",
      "mrna 28848\n",
      "dfb 28894\n",
      "gaddafi 28924\n",
      "ghz 29066\n",
      "donetsk 29151\n",
      "britney 29168\n",
      "mentoring 29192\n",
      "postdoctoral 29272\n",
      "sql 29296\n",
      "weightlifting 29305\n",
      "quarterfinal 29380\n",
      "signage 29404\n",
      "fujian 29551\n",
      "endelle 29581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Which tokens are not represented?\n",
    "num_unrepresented = 0\n",
    "for k,v in tokenizer.vocab.items():\n",
    "    if not v in subset_ratio_100M['subset_present_tokens'] and not k.startswith('[unused') and not len(k) <= 1 and not (len(k) == 3 and k.startswith('##')):\n",
    "        num_unrepresented += 1\n",
    "        print(k, v)\n",
    "num_unrepresented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] 0\n",
      "[unused0] 1\n",
      "[unused1] 2\n",
      "[unused2] 3\n",
      "[unused3] 4\n",
      "[unused4] 5\n",
      "[unused5] 6\n",
      "[unused6] 7\n",
      "[unused7] 8\n",
      "[unused8] 9\n",
      "[unused9] 10\n",
      "[unused10] 11\n",
      "[unused11] 12\n",
      "[unused12] 13\n",
      "[unused13] 14\n",
      "[unused14] 15\n",
      "[unused15] 16\n",
      "[unused16] 17\n",
      "[unused17] 18\n",
      "[unused18] 19\n",
      "[unused19] 20\n",
      "[unused20] 21\n",
      "[unused21] 22\n",
      "[unused22] 23\n",
      "[unused23] 24\n",
      "[unused24] 25\n",
      "[unused25] 26\n",
      "[unused26] 27\n",
      "[unused27] 28\n",
      "[unused28] 29\n",
      "[unused29] 30\n",
      "[unused30] 31\n",
      "[unused31] 32\n",
      "[unused32] 33\n",
      "[unused33] 34\n",
      "[unused34] 35\n",
      "[unused35] 36\n",
      "[unused36] 37\n",
      "[unused37] 38\n",
      "[unused38] 39\n",
      "[unused39] 40\n",
      "[unused40] 41\n",
      "[unused41] 42\n",
      "[unused42] 43\n",
      "[unused43] 44\n",
      "[unused44] 45\n",
      "[unused45] 46\n",
      "[unused46] 47\n",
      "[unused47] 48\n",
      "[unused48] 49\n",
      "[unused49] 50\n",
      "[unused50] 51\n",
      "[unused51] 52\n",
      "[unused52] 53\n",
      "[unused53] 54\n",
      "[unused54] 55\n",
      "[unused55] 56\n",
      "[unused56] 57\n",
      "[unused57] 58\n",
      "[unused58] 59\n",
      "[unused59] 60\n",
      "[unused60] 61\n",
      "[unused61] 62\n",
      "[unused62] 63\n",
      "[unused63] 64\n",
      "[unused64] 65\n",
      "[unused65] 66\n",
      "[unused66] 67\n",
      "[unused67] 68\n",
      "[unused68] 69\n",
      "[unused69] 70\n",
      "[unused70] 71\n",
      "[unused71] 72\n",
      "[unused72] 73\n",
      "[unused73] 74\n",
      "[unused74] 75\n",
      "[unused75] 76\n",
      "[unused76] 77\n",
      "[unused77] 78\n",
      "[unused78] 79\n",
      "[unused79] 80\n",
      "[unused80] 81\n",
      "[unused81] 82\n",
      "[unused82] 83\n",
      "[unused83] 84\n",
      "[unused84] 85\n",
      "[unused85] 86\n",
      "[unused86] 87\n",
      "[unused87] 88\n",
      "[unused88] 89\n",
      "[unused89] 90\n",
      "[unused90] 91\n",
      "[unused91] 92\n",
      "[unused92] 93\n",
      "[unused93] 94\n",
      "[unused94] 95\n",
      "[unused95] 96\n",
      "[unused96] 97\n",
      "[unused97] 98\n",
      "[unused98] 99\n",
      "[CLS] 101\n",
      "[SEP] 102\n",
      "[MASK] 103\n",
      "[unused99] 104\n",
      "[unused100] 105\n",
      "[unused101] 106\n",
      "[unused102] 107\n",
      "[unused103] 108\n",
      "[unused104] 109\n",
      "[unused105] 110\n",
      "[unused106] 111\n",
      "[unused107] 112\n",
      "[unused108] 113\n",
      "[unused109] 114\n",
      "[unused110] 115\n",
      "[unused111] 116\n",
      "[unused112] 117\n",
      "[unused113] 118\n",
      "[unused114] 119\n",
      "[unused115] 120\n",
      "[unused116] 121\n",
      "[unused117] 122\n",
      "[unused118] 123\n",
      "[unused119] 124\n",
      "[unused120] 125\n",
      "[unused121] 126\n",
      "[unused122] 127\n",
      "[unused123] 128\n",
      "[unused124] 129\n",
      "[unused125] 130\n",
      "[unused126] 131\n",
      "[unused127] 132\n",
      "[unused128] 133\n",
      "[unused129] 134\n",
      "[unused130] 135\n",
      "[unused131] 136\n",
      "[unused132] 137\n",
      "[unused133] 138\n",
      "[unused134] 139\n",
      "[unused135] 140\n",
      "[unused136] 141\n",
      "[unused137] 142\n",
      "[unused138] 143\n",
      "[unused139] 144\n",
      "[unused140] 145\n",
      "[unused141] 146\n",
      "[unused142] 147\n",
      "[unused143] 148\n",
      "[unused144] 149\n",
      "[unused145] 150\n",
      "[unused146] 151\n",
      "[unused147] 152\n",
      "[unused148] 153\n",
      "[unused149] 154\n",
      "[unused150] 155\n",
      "[unused151] 156\n",
      "[unused152] 157\n",
      "[unused153] 158\n",
      "[unused154] 159\n",
      "[unused155] 160\n",
      "[unused156] 161\n",
      "[unused157] 162\n",
      "[unused158] 163\n",
      "[unused159] 164\n",
      "[unused160] 165\n",
      "[unused161] 166\n",
      "[unused162] 167\n",
      "[unused163] 168\n",
      "[unused164] 169\n",
      "[unused165] 170\n",
      "[unused166] 171\n",
      "[unused167] 172\n",
      "[unused168] 173\n",
      "[unused169] 174\n",
      "[unused170] 175\n",
      "[unused171] 176\n",
      "[unused172] 177\n",
      "[unused173] 178\n",
      "[unused174] 179\n",
      "[unused175] 180\n",
      "[unused176] 181\n",
      "[unused177] 182\n",
      "[unused178] 183\n",
      "[unused179] 184\n",
      "[unused180] 185\n",
      "[unused181] 186\n",
      "[unused182] 187\n",
      "[unused183] 188\n",
      "[unused184] 189\n",
      "[unused185] 190\n",
      "[unused186] 191\n",
      "[unused187] 192\n",
      "[unused188] 193\n",
      "[unused189] 194\n",
      "[unused190] 195\n",
      "[unused191] 196\n",
      "[unused192] 197\n",
      "[unused193] 198\n",
      "[unused194] 199\n",
      "[unused195] 200\n",
      "[unused196] 201\n",
      "[unused197] 202\n",
      "[unused198] 203\n",
      "[unused199] 204\n",
      "[unused200] 205\n",
      "[unused201] 206\n",
      "[unused202] 207\n",
      "[unused203] 208\n",
      "[unused204] 209\n",
      "[unused205] 210\n",
      "[unused206] 211\n",
      "[unused207] 212\n",
      "[unused208] 213\n",
      "[unused209] 214\n",
      "[unused210] 215\n",
      "[unused211] 216\n",
      "[unused212] 217\n",
      "[unused213] 218\n",
      "[unused214] 219\n",
      "[unused215] 220\n",
      "[unused216] 221\n",
      "[unused217] 222\n",
      "[unused218] 223\n",
      "[unused219] 224\n",
      "[unused220] 225\n",
      "[unused221] 226\n",
      "[unused222] 227\n",
      "[unused223] 228\n",
      "[unused224] 229\n",
      "[unused225] 230\n",
      "[unused226] 231\n",
      "[unused227] 232\n",
      "[unused228] 233\n",
      "[unused229] 234\n",
      "[unused230] 235\n",
      "[unused231] 236\n",
      "[unused232] 237\n",
      "[unused233] 238\n",
      "[unused234] 239\n",
      "[unused235] 240\n",
      "[unused236] 241\n",
      "[unused237] 242\n",
      "[unused238] 243\n",
      "[unused239] 244\n",
      "[unused240] 245\n",
      "[unused241] 246\n",
      "[unused242] 247\n",
      "[unused243] 248\n",
      "[unused244] 249\n",
      "[unused245] 250\n",
      "[unused246] 251\n",
      "[unused247] 252\n",
      "[unused248] 253\n",
      "[unused249] 254\n",
      "[unused250] 255\n",
      "[unused251] 256\n",
      "[unused252] 257\n",
      "[unused253] 258\n",
      "[unused254] 259\n",
      "[unused255] 260\n",
      "[unused256] 261\n",
      "[unused257] 262\n",
      "[unused258] 263\n",
      "[unused259] 264\n",
      "[unused260] 265\n",
      "[unused261] 266\n",
      "[unused262] 267\n",
      "[unused263] 268\n",
      "[unused264] 269\n",
      "[unused265] 270\n",
      "[unused266] 271\n",
      "[unused267] 272\n",
      "[unused268] 273\n",
      "[unused269] 274\n",
      "[unused270] 275\n",
      "[unused271] 276\n",
      "[unused272] 277\n",
      "[unused273] 278\n",
      "[unused274] 279\n",
      "[unused275] 280\n",
      "[unused276] 281\n",
      "[unused277] 282\n",
      "[unused278] 283\n",
      "[unused279] 284\n",
      "[unused280] 285\n",
      "[unused281] 286\n",
      "[unused282] 287\n",
      "[unused283] 288\n",
      "[unused284] 289\n",
      "[unused285] 290\n",
      "[unused286] 291\n",
      "[unused287] 292\n",
      "[unused288] 293\n",
      "[unused289] 294\n",
      "[unused290] 295\n",
      "[unused291] 296\n",
      "[unused292] 297\n",
      "[unused293] 298\n",
      "[unused294] 299\n",
      "[unused295] 300\n",
      "[unused296] 301\n",
      "[unused297] 302\n",
      "[unused298] 303\n",
      "[unused299] 304\n",
      "[unused300] 305\n",
      "[unused301] 306\n",
      "[unused302] 307\n",
      "[unused303] 308\n",
      "[unused304] 309\n",
      "[unused305] 310\n",
      "[unused306] 311\n",
      "[unused307] 312\n",
      "[unused308] 313\n",
      "[unused309] 314\n",
      "[unused310] 315\n",
      "[unused311] 316\n",
      "[unused312] 317\n",
      "[unused313] 318\n",
      "[unused314] 319\n",
      "[unused315] 320\n",
      "[unused316] 321\n",
      "[unused317] 322\n",
      "[unused318] 323\n",
      "[unused319] 324\n",
      "[unused320] 325\n",
      "[unused321] 326\n",
      "[unused322] 327\n",
      "[unused323] 328\n",
      "[unused324] 329\n",
      "[unused325] 330\n",
      "[unused326] 331\n",
      "[unused327] 332\n",
      "[unused328] 333\n",
      "[unused329] 334\n",
      "[unused330] 335\n",
      "[unused331] 336\n",
      "[unused332] 337\n",
      "[unused333] 338\n",
      "[unused334] 339\n",
      "[unused335] 340\n",
      "[unused336] 341\n",
      "[unused337] 342\n",
      "[unused338] 343\n",
      "[unused339] 344\n",
      "[unused340] 345\n",
      "[unused341] 346\n",
      "[unused342] 347\n",
      "[unused343] 348\n",
      "[unused344] 349\n",
      "[unused345] 350\n",
      "[unused346] 351\n",
      "[unused347] 352\n",
      "[unused348] 353\n",
      "[unused349] 354\n",
      "[unused350] 355\n",
      "[unused351] 356\n",
      "[unused352] 357\n",
      "[unused353] 358\n",
      "[unused354] 359\n",
      "[unused355] 360\n",
      "[unused356] 361\n",
      "[unused357] 362\n",
      "[unused358] 363\n",
      "[unused359] 364\n",
      "[unused360] 365\n",
      "[unused361] 366\n",
      "[unused362] 367\n",
      "[unused363] 368\n",
      "[unused364] 369\n",
      "[unused365] 370\n",
      "[unused366] 371\n",
      "[unused367] 372\n",
      "[unused368] 373\n",
      "[unused369] 374\n",
      "[unused370] 375\n",
      "[unused371] 376\n",
      "[unused372] 377\n",
      "[unused373] 378\n",
      "[unused374] 379\n",
      "[unused375] 380\n",
      "[unused376] 381\n",
      "[unused377] 382\n",
      "[unused378] 383\n",
      "[unused379] 384\n",
      "[unused380] 385\n",
      "[unused381] 386\n",
      "[unused382] 387\n",
      "[unused383] 388\n",
      "[unused384] 389\n",
      "[unused385] 390\n",
      "[unused386] 391\n",
      "[unused387] 392\n",
      "[unused388] 393\n",
      "[unused389] 394\n",
      "[unused390] 395\n",
      "[unused391] 396\n",
      "[unused392] 397\n",
      "[unused393] 398\n",
      "[unused394] 399\n",
      "[unused395] 400\n",
      "[unused396] 401\n",
      "[unused397] 402\n",
      "[unused398] 403\n",
      "[unused399] 404\n",
      "[unused400] 405\n",
      "[unused401] 406\n",
      "[unused402] 407\n",
      "[unused403] 408\n",
      "[unused404] 409\n",
      "[unused405] 410\n",
      "[unused406] 411\n",
      "[unused407] 412\n",
      "[unused408] 413\n",
      "[unused409] 414\n",
      "[unused410] 415\n",
      "[unused411] 416\n",
      "[unused412] 417\n",
      "[unused413] 418\n",
      "[unused414] 419\n",
      "[unused415] 420\n",
      "[unused416] 421\n",
      "[unused417] 422\n",
      "[unused418] 423\n",
      "[unused419] 424\n",
      "[unused420] 425\n",
      "[unused421] 426\n",
      "[unused422] 427\n",
      "[unused423] 428\n",
      "[unused424] 429\n",
      "[unused425] 430\n",
      "[unused426] 431\n",
      "[unused427] 432\n",
      "[unused428] 433\n",
      "[unused429] 434\n",
      "[unused430] 435\n",
      "[unused431] 436\n",
      "[unused432] 437\n",
      "[unused433] 438\n",
      "[unused434] 439\n",
      "[unused435] 440\n",
      "[unused436] 441\n",
      "[unused437] 442\n",
      "[unused438] 443\n",
      "[unused439] 444\n",
      "[unused440] 445\n",
      "[unused441] 446\n",
      "[unused442] 447\n",
      "[unused443] 448\n",
      "[unused444] 449\n",
      "[unused445] 450\n",
      "[unused446] 451\n",
      "[unused447] 452\n",
      "[unused448] 453\n",
      "[unused449] 454\n",
      "[unused450] 455\n",
      "[unused451] 456\n",
      "[unused452] 457\n",
      "[unused453] 458\n",
      "[unused454] 459\n",
      "[unused455] 460\n",
      "[unused456] 461\n",
      "[unused457] 462\n",
      "[unused458] 463\n",
      "[unused459] 464\n",
      "[unused460] 465\n",
      "[unused461] 466\n",
      "[unused462] 467\n",
      "[unused463] 468\n",
      "[unused464] 469\n",
      "[unused465] 470\n",
      "[unused466] 471\n",
      "[unused467] 472\n",
      "[unused468] 473\n",
      "[unused469] 474\n",
      "[unused470] 475\n",
      "[unused471] 476\n",
      "[unused472] 477\n",
      "[unused473] 478\n",
      "[unused474] 479\n",
      "[unused475] 480\n",
      "[unused476] 481\n",
      "[unused477] 482\n",
      "[unused478] 483\n",
      "[unused479] 484\n",
      "[unused480] 485\n",
      "[unused481] 486\n",
      "[unused482] 487\n",
      "[unused483] 488\n",
      "[unused484] 489\n",
      "[unused485] 490\n",
      "[unused486] 491\n",
      "[unused487] 492\n",
      "[unused488] 493\n",
      "[unused489] 494\n",
      "[unused490] 495\n",
      "[unused491] 496\n",
      "[unused492] 497\n",
      "[unused493] 498\n",
      "[unused494] 499\n",
      "[unused495] 500\n",
      "[unused496] 501\n",
      "[unused497] 502\n",
      "[unused498] 503\n",
      "[unused499] 504\n",
      "[unused500] 505\n",
      "[unused501] 506\n",
      "[unused502] 507\n",
      "[unused503] 508\n",
      "[unused504] 509\n",
      "[unused505] 510\n",
      "[unused506] 511\n",
      "[unused507] 512\n",
      "[unused508] 513\n",
      "[unused509] 514\n",
      "[unused510] 515\n",
      "[unused511] 516\n",
      "[unused512] 517\n",
      "[unused513] 518\n",
      "[unused514] 519\n",
      "[unused515] 520\n",
      "[unused516] 521\n",
      "[unused517] 522\n",
      "[unused518] 523\n",
      "[unused519] 524\n",
      "[unused520] 525\n",
      "[unused521] 526\n",
      "[unused522] 527\n",
      "[unused523] 528\n",
      "[unused524] 529\n",
      "[unused525] 530\n",
      "[unused526] 531\n",
      "[unused527] 532\n",
      "[unused528] 533\n",
      "[unused529] 534\n",
      "[unused530] 535\n",
      "[unused531] 536\n",
      "[unused532] 537\n",
      "[unused533] 538\n",
      "[unused534] 539\n",
      "[unused535] 540\n",
      "[unused536] 541\n",
      "[unused537] 542\n",
      "[unused538] 543\n",
      "[unused539] 544\n",
      "[unused540] 545\n",
      "[unused541] 546\n",
      "[unused542] 547\n",
      "[unused543] 548\n",
      "[unused544] 549\n",
      "[unused545] 550\n",
      "[unused546] 551\n",
      "[unused547] 552\n",
      "[unused548] 553\n",
      "[unused549] 554\n",
      "[unused550] 555\n",
      "[unused551] 556\n",
      "[unused552] 557\n",
      "[unused553] 558\n",
      "[unused554] 559\n",
      "[unused555] 560\n",
      "[unused556] 561\n",
      "[unused557] 562\n",
      "[unused558] 563\n",
      "[unused559] 564\n",
      "[unused560] 565\n",
      "[unused561] 566\n",
      "[unused562] 567\n",
      "[unused563] 568\n",
      "[unused564] 569\n",
      "[unused565] 570\n",
      "[unused566] 571\n",
      "[unused567] 572\n",
      "[unused568] 573\n",
      "[unused569] 574\n",
      "[unused570] 575\n",
      "[unused571] 576\n",
      "[unused572] 577\n",
      "[unused573] 578\n",
      "[unused574] 579\n",
      "[unused575] 580\n",
      "[unused576] 581\n",
      "[unused577] 582\n",
      "[unused578] 583\n",
      "[unused579] 584\n",
      "[unused580] 585\n",
      "[unused581] 586\n",
      "[unused582] 587\n",
      "[unused583] 588\n",
      "[unused584] 589\n",
      "[unused585] 590\n",
      "[unused586] 591\n",
      "[unused587] 592\n",
      "[unused588] 593\n",
      "[unused589] 594\n",
      "[unused590] 595\n",
      "[unused591] 596\n",
      "[unused592] 597\n",
      "[unused593] 598\n",
      "[unused594] 599\n",
      "[unused595] 600\n",
      "[unused596] 601\n",
      "[unused597] 602\n",
      "[unused598] 603\n",
      "[unused599] 604\n",
      "[unused600] 605\n",
      "[unused601] 606\n",
      "[unused602] 607\n",
      "[unused603] 608\n",
      "[unused604] 609\n",
      "[unused605] 610\n",
      "[unused606] 611\n",
      "[unused607] 612\n",
      "[unused608] 613\n",
      "[unused609] 614\n",
      "[unused610] 615\n",
      "[unused611] 616\n",
      "[unused612] 617\n",
      "[unused613] 618\n",
      "[unused614] 619\n",
      "[unused615] 620\n",
      "[unused616] 621\n",
      "[unused617] 622\n",
      "[unused618] 623\n",
      "[unused619] 624\n",
      "[unused620] 625\n",
      "[unused621] 626\n",
      "[unused622] 627\n",
      "[unused623] 628\n",
      "[unused624] 629\n",
      "[unused625] 630\n",
      "[unused626] 631\n",
      "[unused627] 632\n",
      "[unused628] 633\n",
      "[unused629] 634\n",
      "[unused630] 635\n",
      "[unused631] 636\n",
      "[unused632] 637\n",
      "[unused633] 638\n",
      "[unused634] 639\n",
      "[unused635] 640\n",
      "[unused636] 641\n",
      "[unused637] 642\n",
      "[unused638] 643\n",
      "[unused639] 644\n",
      "[unused640] 645\n",
      "[unused641] 646\n",
      "[unused642] 647\n",
      "[unused643] 648\n",
      "[unused644] 649\n",
      "[unused645] 650\n",
      "[unused646] 651\n",
      "[unused647] 652\n",
      "[unused648] 653\n",
      "[unused649] 654\n",
      "[unused650] 655\n",
      "[unused651] 656\n",
      "[unused652] 657\n",
      "[unused653] 658\n",
      "[unused654] 659\n",
      "[unused655] 660\n",
      "[unused656] 661\n",
      "[unused657] 662\n",
      "[unused658] 663\n",
      "[unused659] 664\n",
      "[unused660] 665\n",
      "[unused661] 666\n",
      "[unused662] 667\n",
      "[unused663] 668\n",
      "[unused664] 669\n",
      "[unused665] 670\n",
      "[unused666] 671\n",
      "[unused667] 672\n",
      "[unused668] 673\n",
      "[unused669] 674\n",
      "[unused670] 675\n",
      "[unused671] 676\n",
      "[unused672] 677\n",
      "[unused673] 678\n",
      "[unused674] 679\n",
      "[unused675] 680\n",
      "[unused676] 681\n",
      "[unused677] 682\n",
      "[unused678] 683\n",
      "[unused679] 684\n",
      "[unused680] 685\n",
      "[unused681] 686\n",
      "[unused682] 687\n",
      "[unused683] 688\n",
      "[unused684] 689\n",
      "[unused685] 690\n",
      "[unused686] 691\n",
      "[unused687] 692\n",
      "[unused688] 693\n",
      "[unused689] 694\n",
      "[unused690] 695\n",
      "[unused691] 696\n",
      "[unused692] 697\n",
      "[unused693] 698\n",
      "[unused694] 699\n",
      "[unused695] 700\n",
      "[unused696] 701\n",
      "[unused697] 702\n",
      "[unused698] 703\n",
      "[unused699] 704\n",
      "[unused700] 705\n",
      "[unused701] 706\n",
      "[unused702] 707\n",
      "[unused703] 708\n",
      "[unused704] 709\n",
      "[unused705] 710\n",
      "[unused706] 711\n",
      "[unused707] 712\n",
      "[unused708] 713\n",
      "[unused709] 714\n",
      "[unused710] 715\n",
      "[unused711] 716\n",
      "[unused712] 717\n",
      "[unused713] 718\n",
      "[unused714] 719\n",
      "[unused715] 720\n",
      "[unused716] 721\n",
      "[unused717] 722\n",
      "[unused718] 723\n",
      "[unused719] 724\n",
      "[unused720] 725\n",
      "[unused721] 726\n",
      "[unused722] 727\n",
      "[unused723] 728\n",
      "[unused724] 729\n",
      "[unused725] 730\n",
      "[unused726] 731\n",
      "[unused727] 732\n",
      "[unused728] 733\n",
      "[unused729] 734\n",
      "[unused730] 735\n",
      "[unused731] 736\n",
      "[unused732] 737\n",
      "[unused733] 738\n",
      "[unused734] 739\n",
      "[unused735] 740\n",
      "[unused736] 741\n",
      "[unused737] 742\n",
      "[unused738] 743\n",
      "[unused739] 744\n",
      "[unused740] 745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[unused741] 746\n",
      "[unused742] 747\n",
      "[unused743] 748\n",
      "[unused744] 749\n",
      "[unused745] 750\n",
      "[unused746] 751\n",
      "[unused747] 752\n",
      "[unused748] 753\n",
      "[unused749] 754\n",
      "[unused750] 755\n",
      "[unused751] 756\n",
      "[unused752] 757\n",
      "[unused753] 758\n",
      "[unused754] 759\n",
      "[unused755] 760\n",
      "[unused756] 761\n",
      "[unused757] 762\n",
      "[unused758] 763\n",
      "[unused759] 764\n",
      "[unused760] 765\n",
      "[unused761] 766\n",
      "[unused762] 767\n",
      "[unused763] 768\n",
      "[unused764] 769\n",
      "[unused765] 770\n",
      "[unused766] 771\n",
      "[unused767] 772\n",
      "[unused768] 773\n",
      "[unused769] 774\n",
      "[unused770] 775\n",
      "[unused771] 776\n",
      "[unused772] 777\n",
      "[unused773] 778\n",
      "[unused774] 779\n",
      "[unused775] 780\n",
      "[unused776] 781\n",
      "[unused777] 782\n",
      "[unused778] 783\n",
      "[unused779] 784\n",
      "[unused780] 785\n",
      "[unused781] 786\n",
      "[unused782] 787\n",
      "[unused783] 788\n",
      "[unused784] 789\n",
      "[unused785] 790\n",
      "[unused786] 791\n",
      "[unused787] 792\n",
      "[unused788] 793\n",
      "[unused789] 794\n",
      "[unused790] 795\n",
      "[unused791] 796\n",
      "[unused792] 797\n",
      "[unused793] 798\n",
      "[unused794] 799\n",
      "[unused795] 800\n",
      "[unused796] 801\n",
      "[unused797] 802\n",
      "[unused798] 803\n",
      "[unused799] 804\n",
      "[unused800] 805\n",
      "[unused801] 806\n",
      "[unused802] 807\n",
      "[unused803] 808\n",
      "[unused804] 809\n",
      "[unused805] 810\n",
      "[unused806] 811\n",
      "[unused807] 812\n",
      "[unused808] 813\n",
      "[unused809] 814\n",
      "[unused810] 815\n",
      "[unused811] 816\n",
      "[unused812] 817\n",
      "[unused813] 818\n",
      "[unused814] 819\n",
      "[unused815] 820\n",
      "[unused816] 821\n",
      "[unused817] 822\n",
      "[unused818] 823\n",
      "[unused819] 824\n",
      "[unused820] 825\n",
      "[unused821] 826\n",
      "[unused822] 827\n",
      "[unused823] 828\n",
      "[unused824] 829\n",
      "[unused825] 830\n",
      "[unused826] 831\n",
      "[unused827] 832\n",
      "[unused828] 833\n",
      "[unused829] 834\n",
      "[unused830] 835\n",
      "[unused831] 836\n",
      "[unused832] 837\n",
      "[unused833] 838\n",
      "[unused834] 839\n",
      "[unused835] 840\n",
      "[unused836] 841\n",
      "[unused837] 842\n",
      "[unused838] 843\n",
      "[unused839] 844\n",
      "[unused840] 845\n",
      "[unused841] 846\n",
      "[unused842] 847\n",
      "[unused843] 848\n",
      "[unused844] 849\n",
      "[unused845] 850\n",
      "[unused846] 851\n",
      "[unused847] 852\n",
      "[unused848] 853\n",
      "[unused849] 854\n",
      "[unused850] 855\n",
      "[unused851] 856\n",
      "[unused852] 857\n",
      "[unused853] 858\n",
      "[unused854] 859\n",
      "[unused855] 860\n",
      "[unused856] 861\n",
      "[unused857] 862\n",
      "[unused858] 863\n",
      "[unused859] 864\n",
      "[unused860] 865\n",
      "[unused861] 866\n",
      "[unused862] 867\n",
      "[unused863] 868\n",
      "[unused864] 869\n",
      "[unused865] 870\n",
      "[unused866] 871\n",
      "[unused867] 872\n",
      "[unused868] 873\n",
      "[unused869] 874\n",
      "[unused870] 875\n",
      "[unused871] 876\n",
      "[unused872] 877\n",
      "[unused873] 878\n",
      "[unused874] 879\n",
      "[unused875] 880\n",
      "[unused876] 881\n",
      "[unused877] 882\n",
      "[unused878] 883\n",
      "[unused879] 884\n",
      "[unused880] 885\n",
      "[unused881] 886\n",
      "[unused882] 887\n",
      "[unused883] 888\n",
      "[unused884] 889\n",
      "[unused885] 890\n",
      "[unused886] 891\n",
      "[unused887] 892\n",
      "[unused888] 893\n",
      "[unused889] 894\n",
      "[unused890] 895\n",
      "[unused891] 896\n",
      "[unused892] 897\n",
      "[unused893] 898\n",
      "[unused894] 899\n",
      "[unused895] 900\n",
      "[unused896] 901\n",
      "[unused897] 902\n",
      "[unused898] 903\n",
      "[unused899] 904\n",
      "[unused900] 905\n",
      "[unused901] 906\n",
      "[unused902] 907\n",
      "[unused903] 908\n",
      "[unused904] 909\n",
      "[unused905] 910\n",
      "[unused906] 911\n",
      "[unused907] 912\n",
      "[unused908] 913\n",
      "[unused909] 914\n",
      "[unused910] 915\n",
      "[unused911] 916\n",
      "[unused912] 917\n",
      "[unused913] 918\n",
      "[unused914] 919\n",
      "[unused915] 920\n",
      "[unused916] 921\n",
      "[unused917] 922\n",
      "[unused918] 923\n",
      "[unused919] 924\n",
      "[unused920] 925\n",
      "[unused921] 926\n",
      "[unused922] 927\n",
      "[unused923] 928\n",
      "[unused924] 929\n",
      "[unused925] 930\n",
      "[unused926] 931\n",
      "[unused927] 932\n",
      "[unused928] 933\n",
      "[unused929] 934\n",
      "[unused930] 935\n",
      "[unused931] 936\n",
      "[unused932] 937\n",
      "[unused933] 938\n",
      "[unused934] 939\n",
      "[unused935] 940\n",
      "[unused936] 941\n",
      "[unused937] 942\n",
      "[unused938] 943\n",
      "[unused939] 944\n",
      "[unused940] 945\n",
      "[unused941] 946\n",
      "[unused942] 947\n",
      "[unused943] 948\n",
      "[unused944] 949\n",
      "[unused945] 950\n",
      "[unused946] 951\n",
      "[unused947] 952\n",
      "[unused948] 953\n",
      "[unused949] 954\n",
      "[unused950] 955\n",
      "[unused951] 956\n",
      "[unused952] 957\n",
      "[unused953] 958\n",
      "[unused954] 959\n",
      "[unused955] 960\n",
      "[unused956] 961\n",
      "[unused957] 962\n",
      "[unused958] 963\n",
      "[unused959] 964\n",
      "[unused960] 965\n",
      "[unused961] 966\n",
      "[unused962] 967\n",
      "[unused963] 968\n",
      "[unused964] 969\n",
      "[unused965] 970\n",
      "[unused966] 971\n",
      "[unused967] 972\n",
      "[unused968] 973\n",
      "[unused969] 974\n",
      "[unused970] 975\n",
      "[unused971] 976\n",
      "[unused972] 977\n",
      "[unused973] 978\n",
      "[unused974] 979\n",
      "[unused975] 980\n",
      "[unused976] 981\n",
      "[unused977] 982\n",
      "[unused978] 983\n",
      "[unused979] 984\n",
      "[unused980] 985\n",
      "[unused981] 986\n",
      "[unused982] 987\n",
      "[unused983] 988\n",
      "[unused984] 989\n",
      "[unused985] 990\n",
      "[unused986] 991\n",
      "[unused987] 992\n",
      "[unused988] 993\n",
      "[unused989] 994\n",
      "[unused990] 995\n",
      "[unused991] 996\n",
      "[unused992] 997\n",
      "[unused993] 998\n",
      " 1102\n",
      " 1108\n",
      " 1109\n",
      " 1111\n",
      " 1113\n",
      " 1115\n",
      " 1116\n",
      " 1117\n",
      " 1118\n",
      " 1119\n",
      " 1120\n",
      " 1121\n",
      " 1123\n",
      " 1124\n",
      " 1125\n",
      " 1126\n",
      " 1127\n",
      " 1128\n",
      " 1129\n",
      " 1130\n",
      " 1131\n",
      " 1132\n",
      " 1133\n",
      " 1134\n",
      " 1135\n",
      " 1136\n",
      " 1137\n",
      " 1139\n",
      " 1140\n",
      " 1141\n",
      " 1142\n",
      " 1143\n",
      " 1144\n",
      " 1146\n",
      " 1147\n",
      " 1148\n",
      " 1149\n",
      " 1150\n",
      " 1151\n",
      " 1152\n",
      " 1153\n",
      " 1154\n",
      " 1204\n",
      " 1205\n",
      " 1206\n",
      " 1207\n",
      " 1209\n",
      " 1211\n",
      " 1214\n",
      " 1215\n",
      " 1216\n",
      " 1217\n",
      " 1218\n",
      " 1219\n",
      " 1220\n",
      " 1221\n",
      " 1222\n",
      " 1223\n",
      " 1224\n",
      " 1225\n",
      " 1226\n",
      " 1227\n",
      " 1228\n",
      " 1229\n",
      " 1230\n",
      " 1231\n",
      " 1232\n",
      " 1233\n",
      " 1234\n",
      " 1235\n",
      " 1236\n",
      " 1237\n",
      " 1238\n",
      " 1239\n",
      " 1256\n",
      " 1260\n",
      " 1262\n",
      " 1269\n",
      " 1290\n",
      " 1301\n",
      " 1306\n",
      " 1307\n",
      " 1308\n",
      " 1310\n",
      " 1311\n",
      " 1312\n",
      " 1313\n",
      " 1314\n",
      " 1315\n",
      " 1316\n",
      " 1317\n",
      " 1318\n",
      " 1319\n",
      " 1320\n",
      " 1321\n",
      " 1322\n",
      " 1323\n",
      " 1324\n",
      " 1325\n",
      " 1326\n",
      " 1327\n",
      " 1328\n",
      " 1329\n",
      " 1330\n",
      " 1331\n",
      " 1332\n",
      " 1333\n",
      " 1334\n",
      " 1335\n",
      " 1336\n",
      " 1337\n",
      " 1338\n",
      " 1339\n",
      " 1340\n",
      " 1341\n",
      " 1342\n",
      " 1343\n",
      " 1344\n",
      " 1345\n",
      " 1346\n",
      " 1347\n",
      " 1348\n",
      " 1349\n",
      " 1350\n",
      " 1351\n",
      " 1352\n",
      " 1353\n",
      " 1354\n",
      " 1355\n",
      " 1356\n",
      " 1357\n",
      " 1358\n",
      " 1359\n",
      " 1360\n",
      " 1361\n",
      " 1362\n",
      " 1363\n",
      " 1364\n",
      " 1365\n",
      " 1366\n",
      " 1367\n",
      " 1368\n",
      " 1369\n",
      " 1370\n",
      " 1371\n",
      " 1372\n",
      " 1373\n",
      " 1374\n",
      " 1375\n",
      " 1376\n",
      " 1377\n",
      " 1378\n",
      " 1379\n",
      " 1380\n",
      " 1381\n",
      " 1382\n",
      " 1383\n",
      " 1384\n",
      " 1385\n",
      " 1386\n",
      " 1387\n",
      " 1388\n",
      " 1389\n",
      " 1390\n",
      " 1391\n",
      " 1392\n",
      " 1393\n",
      " 1394\n",
      " 1395\n",
      " 1396\n",
      " 1397\n",
      " 1398\n",
      " 1399\n",
      " 1400\n",
      " 1401\n",
      " 1402\n",
      " 1403\n",
      " 1404\n",
      " 1405\n",
      " 1406\n",
      " 1407\n",
      " 1408\n",
      " 1409\n",
      " 1410\n",
      " 1411\n",
      " 1412\n",
      " 1413\n",
      " 1414\n",
      " 1415\n",
      " 1416\n",
      " 1417\n",
      " 1418\n",
      " 1419\n",
      " 1420\n",
      " 1421\n",
      " 1422\n",
      " 1423\n",
      " 1424\n",
      " 1425\n",
      " 1426\n",
      " 1427\n",
      " 1428\n",
      " 1429\n",
      " 1430\n",
      " 1431\n",
      " 1432\n",
      " 1433\n",
      " 1434\n",
      " 1435\n",
      " 1436\n",
      " 1437\n",
      " 1438\n",
      " 1439\n",
      " 1440\n",
      " 1441\n",
      " 1442\n",
      " 1443\n",
      " 1444\n",
      " 1445\n",
      " 1446\n",
      " 1447\n",
      " 1448\n",
      " 1449\n",
      " 1450\n",
      " 1451\n",
      " 1452\n",
      " 1453\n",
      " 1454\n",
      " 1455\n",
      " 1456\n",
      " 1457\n",
      " 1458\n",
      " 1459\n",
      " 1460\n",
      " 1461\n",
      " 1462\n",
      " 1463\n",
      " 1464\n",
      " 1465\n",
      " 1466\n",
      " 1467\n",
      " 1468\n",
      " 1469\n",
      " 1470\n",
      " 1471\n",
      " 1472\n",
      " 1473\n",
      " 1474\n",
      " 1475\n",
      " 1476\n",
      " 1477\n",
      " 1478\n",
      " 1479\n",
      " 1480\n",
      " 1481\n",
      " 1482\n",
      " 1483\n",
      " 1484\n",
      " 1485\n",
      " 1486\n",
      " 1487\n",
      " 1488\n",
      " 1489\n",
      " 1490\n",
      " 1491\n",
      " 1492\n",
      " 1493\n",
      " 1494\n",
      " 1495\n",
      " 1496\n",
      " 1497\n",
      " 1498\n",
      " 1499\n",
      " 1500\n",
      " 1501\n",
      " 1502\n",
      " 1503\n",
      " 1504\n",
      " 1505\n",
      " 1506\n",
      " 1507\n",
      " 1508\n",
      " 1509\n",
      " 1510\n",
      " 1511\n",
      " 1512\n",
      " 1514\n",
      " 1515\n",
      " 1533\n",
      " 1534\n",
      " 1535\n",
      " 1536\n",
      " 1537\n",
      " 1539\n",
      " 1540\n",
      " 1541\n",
      " 1542\n",
      " 1543\n",
      " 1544\n",
      " 1545\n",
      " 1547\n",
      " 1548\n",
      " 1549\n",
      " 1550\n",
      " 1551\n",
      " 1552\n",
      " 1553\n",
      " 1554\n",
      " 1555\n",
      " 1556\n",
      " 1557\n",
      " 1558\n",
      " 1559\n",
      " 1560\n",
      " 1561\n",
      " 1562\n",
      " 1563\n",
      " 1564\n",
      " 1565\n",
      " 1566\n",
      " 1567\n",
      " 1568\n",
      " 1569\n",
      " 1570\n",
      " 1571\n",
      " 1572\n",
      " 1573\n",
      " 1575\n",
      " 1576\n",
      " 1577\n",
      " 1579\n",
      " 1580\n",
      " 1583\n",
      " 1585\n",
      " 1587\n",
      " 1588\n",
      " 1589\n",
      " 1590\n",
      " 1591\n",
      " 1592\n",
      " 1593\n",
      " 1595\n",
      " 1596\n",
      " 1597\n",
      " 1598\n",
      " 1599\n",
      " 1602\n",
      " 1603\n",
      " 1604\n",
      " 1605\n",
      " 1606\n",
      " 1607\n",
      " 1608\n",
      " 1609\n",
      " 1610\n",
      " 1611\n",
      " 1612\n",
      " 1613\n",
      " 1614\n",
      " 1615\n",
      " 1616\n",
      " 1617\n",
      " 1618\n",
      " 1619\n",
      " 1620\n",
      " 1621\n",
      " 1623\n",
      " 1624\n",
      " 1625\n",
      " 1626\n",
      " 1629\n",
      " 1630\n",
      " 1631\n",
      " 1632\n",
      " 1633\n",
      " 1634\n",
      " 1635\n",
      " 1636\n",
      " 1637\n",
      " 1638\n",
      " 1639\n",
      " 1640\n",
      " 1641\n",
      " 1642\n",
      " 1643\n",
      " 1644\n",
      " 1645\n",
      " 1646\n",
      " 1647\n",
      " 1648\n",
      " 1649\n",
      " 1650\n",
      " 1651\n",
      " 1652\n",
      " 1653\n",
      " 1654\n",
      " 1655\n",
      " 1656\n",
      " 1657\n",
      " 1658\n",
      " 1659\n",
      " 1660\n",
      " 1661\n",
      " 1662\n",
      " 1663\n",
      " 1664\n",
      " 1665\n",
      " 1666\n",
      " 1667\n",
      " 1668\n",
      " 1669\n",
      " 1670\n",
      " 1671\n",
      " 1672\n",
      " 1673\n",
      " 1674\n",
      " 1675\n",
      " 1676\n",
      " 1677\n",
      " 1678\n",
      " 1679\n",
      " 1680\n",
      " 1681\n",
      " 1682\n",
      " 1683\n",
      " 1684\n",
      " 1685\n",
      " 1686\n",
      " 1687\n",
      " 1688\n",
      " 1689\n",
      " 1690\n",
      " 1691\n",
      " 1692\n",
      " 1693\n",
      " 1694\n",
      " 1695\n",
      " 1696\n",
      " 1697\n",
      " 1698\n",
      " 1699\n",
      " 1700\n",
      " 1701\n",
      " 1702\n",
      " 1703\n",
      " 1704\n",
      " 1705\n",
      " 1706\n",
      " 1707\n",
      " 1708\n",
      " 1709\n",
      " 1710\n",
      " 1711\n",
      " 1712\n",
      " 1713\n",
      " 1714\n",
      " 1715\n",
      " 1716\n",
      " 1717\n",
      " 1718\n",
      " 1719\n",
      " 1720\n",
      " 1721\n",
      " 1722\n",
      " 1723\n",
      " 1724\n",
      " 1725\n",
      " 1726\n",
      " 1727\n",
      " 1728\n",
      " 1729\n",
      " 1730\n",
      " 1731\n",
      " 1732\n",
      " 1733\n",
      " 1734\n",
      " 1735\n",
      " 1736\n",
      " 1737\n",
      " 1738\n",
      " 1739\n",
      " 1741\n",
      " 1743\n",
      " 1745\n",
      " 1746\n",
      " 1747\n",
      " 1748\n",
      " 1749\n",
      " 1750\n",
      " 1751\n",
      " 1752\n",
      " 1753\n",
      " 1754\n",
      " 1755\n",
      " 1757\n",
      " 1758\n",
      " 1759\n",
      " 1760\n",
      " 1761\n",
      " 1762\n",
      " 1763\n",
      " 1764\n",
      " 1765\n",
      " 1766\n",
      " 1767\n",
      " 1768\n",
      " 1769\n",
      " 1770\n",
      " 1773\n",
      " 1774\n",
      " 1775\n",
      " 1776\n",
      " 1777\n",
      " 1778\n",
      " 1779\n",
      " 1780\n",
      " 1781\n",
      " 1782\n",
      " 1783\n",
      " 1784\n",
      " 1785\n",
      " 1786\n",
      " 1787\n",
      " 1789\n",
      " 1790\n",
      " 1791\n",
      " 1792\n",
      " 1793\n",
      " 1794\n",
      " 1795\n",
      " 1796\n",
      " 1797\n",
      " 1798\n",
      " 1799\n",
      " 1800\n",
      " 1801\n",
      " 1802\n",
      " 1803\n",
      " 1804\n",
      " 1805\n",
      " 1806\n",
      " 1807\n",
      " 1808\n",
      " 1809\n",
      " 1812\n",
      " 1813\n",
      " 1814\n",
      " 1817\n",
      " 1818\n",
      " 1819\n",
      " 1821\n",
      " 1822\n",
      " 1823\n",
      " 1824\n",
      " 1825\n",
      " 1826\n",
      " 1827\n",
      " 1828\n",
      " 1829\n",
      " 1830\n",
      " 1832\n",
      " 1833\n",
      " 1834\n",
      " 1835\n",
      " 1836\n",
      " 1837\n",
      " 1838\n",
      " 1839\n",
      " 1840\n",
      " 1841\n",
      " 1842\n",
      " 1843\n",
      " 1844\n",
      " 1845\n",
      " 1846\n",
      " 1847\n",
      " 1848\n",
      " 1849\n",
      " 1850\n",
      " 1851\n",
      " 1852\n",
      " 1853\n",
      " 1854\n",
      " 1856\n",
      " 1857\n",
      " 1859\n",
      " 1860\n",
      " 1861\n",
      " 1862\n",
      " 1863\n",
      " 1866\n",
      " 1867\n",
      " 1868\n",
      " 1869\n",
      " 1870\n",
      " 1873\n",
      " 1874\n",
      " 1876\n",
      " 1877\n",
      " 1878\n",
      " 1882\n",
      " 1883\n",
      " 1884\n",
      " 1885\n",
      " 1886\n",
      " 1887\n",
      " 1888\n",
      " 1889\n",
      " 1890\n",
      " 1891\n",
      " 1892\n",
      " 1893\n",
      " 1894\n",
      " 1895\n",
      " 1896\n",
      " 1897\n",
      " 1898\n",
      " 1899\n",
      " 1900\n",
      " 1901\n",
      " 1902\n",
      " 1903\n",
      " 1904\n",
      " 1905\n",
      " 1906\n",
      " 1907\n",
      " 1908\n",
      " 1909\n",
      " 1910\n",
      " 1911\n",
      " 1912\n",
      " 1914\n",
      " 1916\n",
      " 1917\n",
      " 1918\n",
      " 1919\n",
      " 1920\n",
      " 1921\n",
      " 1922\n",
      " 1924\n",
      " 1925\n",
      " 1926\n",
      " 1927\n",
      " 1928\n",
      " 1929\n",
      " 1930\n",
      " 1931\n",
      " 1932\n",
      " 1933\n",
      " 1934\n",
      " 1935\n",
      " 1936\n",
      " 1937\n",
      " 1938\n",
      " 1940\n",
      " 1941\n",
      " 1942\n",
      " 1943\n",
      " 1944\n",
      " 1945\n",
      " 1946\n",
      " 1948\n",
      " 1949\n",
      " 1950\n",
      " 1951\n",
      " 1952\n",
      " 1953\n",
      " 1954\n",
      " 1955\n",
      " 1956\n",
      " 1957\n",
      " 1958\n",
      " 1959\n",
      " 1960\n",
      " 1961\n",
      " 1962\n",
      " 1963\n",
      " 1965\n",
      " 1966\n",
      " 1967\n",
      " 1968\n",
      " 1969\n",
      " 1970\n",
      " 1971\n",
      " 1972\n",
      " 1973\n",
      " 1974\n",
      " 1975\n",
      " 1976\n",
      " 1977\n",
      " 1978\n",
      " 1979\n",
      " 1980\n",
      " 1981\n",
      " 1982\n",
      " 1983\n",
      " 1984\n",
      " 1985\n",
      " 1986\n",
      " 1987\n",
      " 1988\n",
      " 1989\n",
      " 1990\n",
      " 1991\n",
      " 1992\n",
      " 1993\n",
      " 1994\n",
      " 1995\n",
      "... 2133\n",
      "km 3186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soundtrack 6050\n",
      "remix 6136\n",
      "c 6362\n",
      "uefa 6663\n",
      "playoff 7808\n",
      "midfielder 8850\n",
      "playstation 9160\n",
      "quarterfinals 9237\n",
      "pinyin 9973\n",
      "allmusic 10477\n",
      "mlb 10901\n",
      "espn 10978\n",
      "gameplay 11247\n",
      "nsw 11524\n",
      "nascar 11838\n",
      "itunes 11943\n",
      "lgbt 12010\n",
      "mvp 12041\n",
      "xbox 12202\n",
      "eurovision 12714\n",
      "## 12744\n",
      "vfl 13480\n",
      "kolkata 13522\n",
      "pga 14198\n",
      "m 14241\n",
      "bundesliga 14250\n",
      "metacritic 14476\n",
      "remixes 15193\n",
      "steelers 15280\n",
      "airplay 15341\n",
      "## 15414\n",
      "paralympics 15600\n",
      "zhao 15634\n",
      "reggae 15662\n",
      "linebacker 15674\n",
      "v8 15754\n",
      "hindwings 15998\n",
      "bollywood 16046\n",
      "podcast 16110\n",
      "atletico 16132\n",
      "wwf 16779\n",
      "transgender 16824\n",
      "paralympic 17029\n",
      "## 17110\n",
      "postseason 17525\n",
      "vhs 17550\n",
      "campeonato 17675\n",
      "multiplayer 17762\n",
      "odz 17814\n",
      "curated 17940\n",
      "iphone 18059\n",
      "gmbh 18289\n",
      "danielle 18490\n",
      "qaeda 18659\n",
      "mixtape 18713\n",
      " 18728\n",
      "## 19109\n",
      "## 19110\n",
      "##aw 19704\n",
      "##qing 19784\n",
      "saxophonist 19977\n",
      "preseason 20038\n",
      "pmid 20117\n",
      "keyboardist 20173\n",
      "iucn 20333\n",
      "pokemon 20421\n",
      "nrl 20686\n",
      "motorsports 20711\n",
      "jaenelle 20757\n",
      "beyonce 20773\n",
      "airbus 20901\n",
      "netflix 20907\n",
      "motorsport 21044\n",
      "belgarath 21256\n",
      "iaaf 21259\n",
      "guangdong 21287\n",
      "shortlisted 21353\n",
      "seahawks 21390\n",
      "lucivar 21401\n",
      "goaltender 21437\n",
      "nanjing 21455\n",
      "crambidae 21585\n",
      "frontman 21597\n",
      "cbn 21824\n",
      "odisha 21874\n",
      "alzheimer 21901\n",
      "kinase 21903\n",
      "futsal 21921\n",
      "wta 21925\n",
      "nokia 22098\n",
      "smackdown 22120\n",
      "concacaf 22169\n",
      "oricon 22237\n",
      "twenty20 22240\n",
      "nfc 22309\n",
      "tbilisi 22406\n",
      "nrhp 22424\n",
      "showcased 22443\n",
      "nmi 22484\n",
      "kaladin 22588\n",
      "storylines 22628\n",
      "## 22646\n",
      "riaa 22716\n",
      "##a 22972\n",
      "mitochondrial 23079\n",
      "haryana 23261\n",
      "##saw 23305\n",
      "## 23432\n",
      "issn 23486\n",
      "saetan 23515\n",
      "rbis 23583\n",
      "deportivo 23696\n",
      "neuroscience 23700\n",
      "telangana 23764\n",
      "lviv 23814\n",
      "scientology 23845\n",
      "bwv 23860\n",
      "transitioned 23946\n",
      "yamaha 24031\n",
      "maccabi 24055\n",
      "lexie 24123\n",
      "superfamily 24169\n",
      "hapoel 24208\n",
      "wcw 24215\n",
      "soundtracks 24245\n",
      "debuting 24469\n",
      "bmg 24499\n",
      "sequencing 24558\n",
      "ho 24833\n",
      "myspace 24927\n",
      "##strae 24967\n",
      "smashwords 25151\n",
      "wrocaw 25160\n",
      "cmll 25395\n",
      "rihanna 25439\n",
      "wnba 25554\n",
      "brianna 25558\n",
      "filmfare 25648\n",
      "hezbollah 25713\n",
      "cheerleading 25721\n",
      "telenovela 25754\n",
      "shandong 25768\n",
      "## 25799\n",
      "erebidae 25875\n",
      "superliga 25922\n",
      "hyundai 25983\n",
      "springsteen 26002\n",
      "euroleague 26093\n",
      "stanisaw 26133\n",
      "tianjin 26216\n",
      "vh1 26365\n",
      "smartphone 26381\n",
      "alyssa 26442\n",
      "detainees 26485\n",
      "kayla 26491\n",
      "benfica 26542\n",
      "shenzhen 26555\n",
      "godzilla 26631\n",
      "zhejiang 26805\n",
      "2010s 26817\n",
      "cornerback 26857\n",
      "capcom 26861\n",
      "iihf 26904\n",
      "mukherjee 27040\n",
      "polgara 27041\n",
      "joyah 27098\n",
      "multicultural 27135\n",
      "mbc 27262\n",
      "jillian 27286\n",
      "## 27392\n",
      "metadata 27425\n",
      "showcasing 27696\n",
      "minogue 27736\n",
      "##rae 27807\n",
      "##1 27944\n",
      "00pm 27995\n",
      "wrestlemania 28063\n",
      "1910s 28088\n",
      "jiangsu 28091\n",
      "iqbal 28111\n",
      "biomass 28148\n",
      "prequel 28280\n",
      "fivb 28423\n",
      "kaitlyn 28584\n",
      "goalscorer 28602\n",
      "scuba 28651\n",
      "##genase 28835\n",
      "mrna 28848\n",
      "dfb 28894\n",
      "gaddafi 28924\n",
      "ghz 29066\n",
      "donetsk 29151\n",
      "britney 29168\n",
      "mentoring 29192\n",
      "postdoctoral 29272\n",
      "sql 29296\n",
      "weightlifting 29305\n",
      "quarterfinal 29380\n",
      "signage 29404\n",
      "fujian 29551\n",
      "endelle 29581\n",
      "##! 29612\n",
      "##\" 29613\n",
      "### 29614\n",
      "##$ 29615\n",
      "##% 29616\n",
      "##& 29617\n",
      "##' 29618\n",
      "##( 29619\n",
      "##) 29620\n",
      "##* 29621\n",
      "##+ 29622\n",
      "##, 29623\n",
      "##- 29624\n",
      "##. 29625\n",
      "##/ 29626\n",
      "##: 29627\n",
      "##; 29628\n",
      "##< 29629\n",
      "##= 29630\n",
      "##> 29631\n",
      "##? 29632\n",
      "##@ 29633\n",
      "##[ 29634\n",
      "##\\ 29635\n",
      "##] 29636\n",
      "##^ 29637\n",
      "##_ 29638\n",
      "##` 29639\n",
      "##{ 29640\n",
      "##| 29641\n",
      "##} 29642\n",
      "##~ 29643\n",
      "## 29644\n",
      "## 29650\n",
      "## 29654\n",
      "## 29660\n",
      "## 29661\n",
      "## 29663\n",
      "## 29666\n",
      "## 29675\n",
      "## 29676\n",
      "## 29677\n",
      "## 29678\n",
      "## 29680\n",
      "## 29682\n",
      "## 29683\n",
      "## 29685\n",
      "## 29686\n",
      "## 29687\n",
      "## 29688\n",
      "## 29690\n",
      "## 29691\n",
      "## 29692\n",
      "## 29693\n",
      "## 29694\n",
      "## 29695\n",
      "## 29696\n",
      "## 29697\n",
      "## 29698\n",
      "## 29699\n",
      "## 29700\n",
      "## 29701\n",
      "## 29702\n",
      "## 29703\n",
      "## 29705\n",
      "## 29706\n",
      "## 29707\n",
      "## 29708\n",
      "## 29709\n",
      "## 29710\n",
      "## 29712\n",
      "## 29713\n",
      "## 29714\n",
      "## 29715\n",
      "## 29716\n",
      "## 29717\n",
      "## 29718\n",
      "## 29719\n",
      "## 29758\n",
      "## 29759\n",
      "## 29761\n",
      "## 29762\n",
      "## 29763\n",
      "## 29764\n",
      "## 29765\n",
      "## 29766\n",
      "## 29767\n",
      "## 29768\n",
      "## 29769\n",
      "## 29770\n",
      "## 29771\n",
      "## 29772\n",
      "## 29773\n",
      "## 29774\n",
      "## 29775\n",
      "## 29776\n",
      "## 29777\n",
      "## 29778\n",
      "## 29779\n",
      "## 29780\n",
      "## 29781\n",
      "## 29782\n",
      "## 29783\n",
      "## 29784\n",
      "## 29785\n",
      "## 29786\n",
      "## 29787\n",
      "## 29808\n",
      "## 29814\n",
      "## 29832\n",
      "## 29838\n",
      "## 29843\n",
      "## 29844\n",
      "## 29845\n",
      "## 29846\n",
      "## 29847\n",
      "## 29848\n",
      "## 29849\n",
      "## 29850\n",
      "## 29851\n",
      "## 29852\n",
      "## 29853\n",
      "## 29854\n",
      "## 29855\n",
      "## 29856\n",
      "## 29857\n",
      "## 29858\n",
      "## 29859\n",
      "## 29860\n",
      "## 29861\n",
      "## 29862\n",
      "## 29863\n",
      "## 29864\n",
      "## 29865\n",
      "## 29866\n",
      "## 29867\n",
      "## 29868\n",
      "## 29869\n",
      "## 29870\n",
      "## 29871\n",
      "## 29872\n",
      "## 29873\n",
      "## 29874\n",
      "## 29875\n",
      "## 29876\n",
      "## 29877\n",
      "## 29878\n",
      "## 29879\n",
      "## 29880\n",
      "## 29881\n",
      "## 29882\n",
      "## 29883\n",
      "## 29884\n",
      "## 29885\n",
      "## 29886\n",
      "## 29887\n",
      "## 29888\n",
      "## 29889\n",
      "## 29890\n",
      "## 29891\n",
      "## 29892\n",
      "## 29893\n",
      "## 29894\n",
      "## 29895\n",
      "## 29896\n",
      "## 29897\n",
      "## 29898\n",
      "## 29899\n",
      "## 29900\n",
      "## 29901\n",
      "## 29902\n",
      "## 29903\n",
      "## 29904\n",
      "## 29905\n",
      "## 29906\n",
      "## 29907\n",
      "## 29908\n",
      "## 29909\n",
      "## 29910\n",
      "## 29911\n",
      "## 29912\n",
      "## 29913\n",
      "## 29914\n",
      "## 29915\n",
      "## 29916\n",
      "## 29917\n",
      "## 29918\n",
      "## 29919\n",
      "## 29920\n",
      "## 29921\n",
      "## 29922\n",
      "## 29923\n",
      "## 29924\n",
      "## 29925\n",
      "## 29926\n",
      "## 29927\n",
      "## 29928\n",
      "## 29929\n",
      "## 29930\n",
      "## 29931\n",
      "## 29932\n",
      "## 29933\n",
      "## 29934\n",
      "## 29935\n",
      "## 29936\n",
      "## 29937\n",
      "## 29938\n",
      "## 29939\n",
      "## 29940\n",
      "## 29941\n",
      "## 29942\n",
      "## 29943\n",
      "## 29944\n",
      "## 29945\n",
      "## 29946\n",
      "## 29947\n",
      "## 29948\n",
      "## 29949\n",
      "## 29950\n",
      "## 29951\n",
      "## 29952\n",
      "## 29953\n",
      "## 29954\n",
      "## 29955\n",
      "## 29956\n",
      "## 29957\n",
      "## 29958\n",
      "## 29959\n",
      "## 29960\n",
      "## 29961\n",
      "## 29962\n",
      "## 29963\n",
      "## 29964\n",
      "## 29965\n",
      "## 29966\n",
      "## 29967\n",
      "## 29968\n",
      "## 29969\n",
      "## 29970\n",
      "## 29971\n",
      "## 29972\n",
      "## 29973\n",
      "## 29974\n",
      "## 29975\n",
      "## 29976\n",
      "## 29977\n",
      "## 29978\n",
      "## 29979\n",
      "## 29980\n",
      "## 29981\n",
      "## 29982\n",
      "## 29983\n",
      "## 29984\n",
      "## 29985\n",
      "## 29986\n",
      "## 29987\n",
      "## 29988\n",
      "## 29989\n",
      "## 29990\n",
      "## 29991\n",
      "## 29992\n",
      "## 29993\n",
      "## 29994\n",
      "## 29995\n",
      "## 29996\n",
      "## 29997\n",
      "## 29998\n",
      "## 29999\n",
      "## 30000\n",
      "## 30001\n",
      "## 30002\n",
      "## 30003\n",
      "## 30004\n",
      "## 30005\n",
      "## 30006\n",
      "## 30007\n",
      "## 30008\n",
      "## 30009\n",
      "## 30010\n",
      "## 30011\n",
      "## 30012\n",
      "## 30013\n",
      "## 30014\n",
      "## 30015\n",
      "## 30016\n",
      "## 30017\n",
      "## 30018\n",
      "## 30019\n",
      "## 30020\n",
      "## 30021\n",
      "## 30022\n",
      "## 30023\n",
      "## 30024\n",
      "## 30025\n",
      "## 30026\n",
      "## 30027\n",
      "## 30028\n",
      "## 30029\n",
      "## 30030\n",
      "## 30031\n",
      "## 30032\n",
      "## 30033\n",
      "## 30034\n",
      "## 30035\n",
      "## 30037\n",
      "## 30039\n",
      "## 30040\n",
      "## 30041\n",
      "## 30043\n",
      "## 30044\n",
      "## 30045\n",
      "## 30046\n",
      "## 30047\n",
      "## 30048\n",
      "## 30049\n",
      "## 30050\n",
      "## 30051\n",
      "## 30052\n",
      "## 30053\n",
      "## 30054\n",
      "## 30055\n",
      "## 30056\n",
      "## 30057\n",
      "## 30058\n",
      "## 30059\n",
      "## 30060\n",
      "## 30061\n",
      "## 30062\n",
      "## 30063\n",
      "## 30064\n",
      "## 30065\n",
      "## 30066\n",
      "## 30067\n",
      "## 30068\n",
      "## 30069\n",
      "## 30070\n",
      "## 30071\n",
      "## 30072\n",
      "## 30073\n",
      "## 30074\n",
      "## 30075\n",
      "## 30076\n",
      "## 30078\n",
      "## 30079\n",
      "## 30081\n",
      "## 30082\n",
      "## 30083\n",
      "## 30084\n",
      "## 30085\n",
      "## 30086\n",
      "## 30087\n",
      "## 30088\n",
      "## 30089\n",
      "## 30090\n",
      "## 30091\n",
      "## 30092\n",
      "## 30093\n",
      "## 30094\n",
      "## 30095\n",
      "## 30096\n",
      "## 30097\n",
      "## 30098\n",
      "## 30099\n",
      "## 30100\n",
      "## 30101\n",
      "## 30102\n",
      "## 30103\n",
      "## 30104\n",
      "## 30105\n",
      "## 30106\n",
      "## 30107\n",
      "## 30108\n",
      "## 30111\n",
      "## 30112\n",
      "## 30113\n",
      "## 30114\n",
      "## 30115\n",
      "## 30116\n",
      "## 30117\n",
      "## 30118\n",
      "## 30119\n",
      "## 30120\n",
      "## 30121\n",
      "## 30122\n",
      "## 30123\n",
      "## 30124\n",
      "## 30125\n",
      "## 30126\n",
      "## 30129\n",
      "## 30130\n",
      "## 30131\n",
      "## 30132\n",
      "## 30133\n",
      "## 30134\n",
      "## 30135\n",
      "## 30136\n",
      "## 30137\n",
      "## 30138\n",
      "## 30139\n",
      "## 30140\n",
      "## 30141\n",
      "## 30142\n",
      "## 30143\n",
      "## 30144\n",
      "## 30145\n",
      "## 30146\n",
      "## 30147\n",
      "## 30148\n",
      "## 30149\n",
      "## 30150\n",
      "## 30151\n",
      "## 30152\n",
      "## 30153\n",
      "## 30155\n",
      "## 30156\n",
      "## 30157\n",
      "## 30158\n",
      "## 30159\n",
      "## 30160\n",
      "## 30161\n",
      "## 30162\n",
      "## 30163\n",
      "## 30164\n",
      "## 30165\n",
      "## 30166\n",
      "## 30167\n",
      "## 30168\n",
      "## 30169\n",
      "## 30170\n",
      "## 30171\n",
      "## 30172\n",
      "## 30173\n",
      "## 30174\n",
      "## 30175\n",
      "## 30176\n",
      "## 30177\n",
      "## 30178\n",
      "## 30179\n",
      "## 30180\n",
      "## 30181\n",
      "## 30182\n",
      "## 30183\n",
      "## 30184\n",
      "## 30185\n",
      "## 30186\n",
      "## 30187\n",
      "## 30188\n",
      "## 30189\n",
      "## 30190\n",
      "## 30191\n",
      "## 30192\n",
      "## 30193\n",
      "## 30194\n",
      "## 30195\n",
      "## 30196\n",
      "## 30197\n",
      "## 30198\n",
      "## 30199\n",
      "## 30200\n",
      "## 30201\n",
      "## 30202\n",
      "## 30203\n",
      "## 30204\n",
      "## 30205\n",
      "## 30206\n",
      "## 30207\n",
      "## 30208\n",
      "## 30209\n",
      "## 30210\n",
      "## 30211\n",
      "## 30212\n",
      "## 30213\n",
      "## 30214\n",
      "## 30215\n",
      "## 30216\n",
      "## 30217\n",
      "## 30218\n",
      "## 30219\n",
      "## 30220\n",
      "## 30221\n",
      "## 30222\n",
      "## 30223\n",
      "## 30224\n",
      "## 30225\n",
      "## 30226\n",
      "## 30227\n",
      "## 30228\n",
      "## 30229\n",
      "## 30230\n",
      "## 30231\n",
      "## 30232\n",
      "## 30233\n",
      "## 30234\n",
      "## 30235\n",
      "## 30236\n",
      "## 30237\n",
      "## 30238\n",
      "## 30239\n",
      "## 30240\n",
      "## 30241\n",
      "## 30242\n",
      "## 30243\n",
      "## 30244\n",
      "## 30245\n",
      "## 30246\n",
      "## 30247\n",
      "## 30248\n",
      "## 30249\n",
      "## 30250\n",
      "## 30251\n",
      "## 30252\n",
      "## 30253\n",
      "## 30254\n",
      "## 30255\n",
      "## 30256\n",
      "## 30257\n",
      "## 30258\n",
      "## 30259\n",
      "## 30260\n",
      "## 30261\n",
      "## 30262\n",
      "## 30263\n",
      "## 30264\n",
      "## 30265\n",
      "## 30266\n",
      "## 30267\n",
      "## 30268\n",
      "## 30269\n",
      "## 30270\n",
      "## 30271\n",
      "## 30272\n",
      "## 30273\n",
      "## 30274\n",
      "## 30275\n",
      "## 30276\n",
      "## 30277\n",
      "## 30278\n",
      "## 30279\n",
      "## 30280\n",
      "## 30281\n",
      "## 30282\n",
      "## 30283\n",
      "## 30284\n",
      "## 30285\n",
      "## 30286\n",
      "## 30287\n",
      "## 30288\n",
      "## 30289\n",
      "## 30290\n",
      "## 30291\n",
      "## 30292\n",
      "## 30293\n",
      "## 30294\n",
      "## 30295\n",
      "## 30296\n",
      "## 30297\n",
      "## 30298\n",
      "## 30299\n",
      "## 30300\n",
      "## 30301\n",
      "## 30302\n",
      "## 30303\n",
      "## 30304\n",
      "## 30305\n",
      "## 30306\n",
      "## 30307\n",
      "## 30308\n",
      "## 30309\n",
      "## 30310\n",
      "## 30311\n",
      "## 30312\n",
      "## 30313\n",
      "## 30314\n",
      "## 30315\n",
      "## 30316\n",
      "## 30317\n",
      "## 30318\n",
      "## 30319\n",
      "## 30320\n",
      "## 30321\n",
      "## 30322\n",
      "## 30323\n",
      "## 30324\n",
      "## 30325\n",
      "## 30326\n",
      "## 30327\n",
      "## 30328\n",
      "## 30329\n",
      "## 30330\n",
      "## 30331\n",
      "## 30332\n",
      "## 30333\n",
      "## 30334\n",
      "## 30335\n",
      "## 30336\n",
      "## 30337\n",
      "## 30338\n",
      "## 30339\n",
      "## 30340\n",
      "## 30341\n",
      "## 30342\n",
      "## 30343\n",
      "## 30344\n",
      "## 30345\n",
      "## 30346\n",
      "## 30347\n",
      "## 30348\n",
      "## 30349\n",
      "## 30350\n",
      "## 30351\n",
      "## 30352\n",
      "## 30353\n",
      "## 30354\n",
      "## 30355\n",
      "## 30356\n",
      "## 30357\n",
      "## 30358\n",
      "## 30359\n",
      "## 30360\n",
      "## 30361\n",
      "## 30362\n",
      "## 30363\n",
      "## 30364\n",
      "## 30365\n",
      "## 30366\n",
      "## 30367\n",
      "## 30368\n",
      "## 30369\n",
      "## 30370\n",
      "## 30371\n",
      "## 30372\n",
      "## 30373\n",
      "## 30374\n",
      "## 30375\n",
      "## 30376\n",
      "## 30377\n",
      "## 30378\n",
      "## 30379\n",
      "## 30380\n",
      "## 30381\n",
      "## 30382\n",
      "## 30383\n",
      "## 30384\n",
      "## 30385\n",
      "## 30386\n",
      "## 30387\n",
      "## 30388\n",
      "## 30389\n",
      "## 30390\n",
      "## 30391\n",
      "## 30392\n",
      "## 30393\n",
      "## 30394\n",
      "## 30395\n",
      "## 30396\n",
      "## 30397\n",
      "## 30398\n",
      "## 30399\n",
      "## 30400\n",
      "## 30401\n",
      "## 30402\n",
      "## 30403\n",
      "## 30404\n",
      "## 30405\n",
      "## 30406\n",
      "## 30407\n",
      "## 30408\n",
      "## 30409\n",
      "## 30410\n",
      "## 30411\n",
      "## 30412\n",
      "## 30413\n",
      "## 30414\n",
      "## 30415\n",
      "## 30416\n",
      "## 30417\n",
      "## 30418\n",
      "## 30419\n",
      "## 30420\n",
      "## 30421\n",
      "## 30422\n",
      "## 30423\n",
      "## 30424\n",
      "## 30425\n",
      "## 30426\n",
      "## 30427\n",
      "## 30428\n",
      "## 30429\n",
      "## 30430\n",
      "## 30431\n",
      "## 30432\n",
      "## 30433\n",
      "## 30434\n",
      "## 30435\n",
      "## 30436\n",
      "## 30437\n",
      "## 30438\n",
      "## 30439\n",
      "## 30440\n",
      "## 30441\n",
      "## 30442\n",
      "## 30443\n",
      "## 30444\n",
      "## 30445\n",
      "## 30446\n",
      "## 30447\n",
      "## 30448\n",
      "## 30449\n",
      "## 30450\n",
      "## 30451\n",
      "## 30452\n",
      "## 30453\n",
      "## 30454\n",
      "## 30455\n",
      "## 30456\n",
      "## 30457\n",
      "## 30458\n",
      "## 30459\n",
      "## 30460\n",
      "## 30461\n",
      "## 30462\n",
      "## 30463\n",
      "## 30464\n",
      "## 30465\n",
      "## 30466\n",
      "## 30467\n",
      "## 30468\n",
      "## 30469\n",
      "## 30470\n",
      "## 30471\n",
      "## 30472\n",
      "## 30473\n",
      "## 30474\n",
      "## 30475\n",
      "## 30476\n",
      "## 30477\n",
      "## 30478\n",
      "## 30479\n",
      "## 30480\n",
      "## 30481\n",
      "## 30482\n",
      "## 30483\n",
      "## 30484\n",
      "## 30485\n",
      "## 30486\n",
      "## 30487\n",
      "## 30488\n",
      "## 30489\n",
      "## 30490\n",
      "## 30491\n",
      "## 30492\n",
      "## 30493\n",
      "## 30494\n",
      "## 30495\n",
      "## 30496\n",
      "## 30497\n",
      "## 30498\n",
      "## 30499\n",
      "## 30500\n",
      "## 30501\n",
      "## 30502\n",
      "## 30503\n",
      "## 30504\n",
      "## 30505\n",
      "## 30506\n",
      "## 30507\n",
      "## 30508\n",
      "## 30509\n",
      "## 30510\n",
      "## 30511\n",
      "## 30512\n",
      "## 30513\n",
      "## 30514\n",
      "## 30515\n",
      "## 30516\n",
      "## 30517\n",
      "## 30518\n",
      "## 30519\n",
      "## 30520\n",
      "## 30521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2689"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_used = 0\n",
    "for k,v in tokenizer.vocab.items():\n",
    "    if not v in subset_ratio_100M['subset_present_tokens']:\n",
    "        not_used += 1\n",
    "        print(k,v)\n",
    "not_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27833"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset_ratio_100M['subset_present_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_metadata(subset_dict):\n",
    "    '''\n",
    "    prints:\n",
    "    Number of books used in subset\n",
    "    Number of tokens present in subset\n",
    "    Number of tokens represented by subset\n",
    "    '''\n",
    "    print(len(subset_dict['subset_booklist']))\n",
    "    print(subset_dict['subset_total_tokens'])\n",
    "    print(subset_dict['subset_unique_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "99974.0\n",
      "13040\n"
     ]
    }
   ],
   "source": [
    "subset_metadata(subset_ratio_100K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "999825.0\n",
      "24294\n"
     ]
    }
   ],
   "source": [
    "subset_metadata(subset_ratio_1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656\n",
      "9977907.0\n",
      "27607\n"
     ]
    }
   ],
   "source": [
    "subset_metadata(subset_ratio_10M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828\n",
      "28660288.0\n",
      "27833\n"
     ]
    }
   ],
   "source": [
    "subset_metadata(subset_ratio_100M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' a b c d e f g h i j k l m n o p q r s t u v w x y z &.',\n",
       " 'Online Distributed Proofreading Team at http://www.pgdp.net (This file was produced from images generously made available by The Internet Archive/American Libraries.)',\n",
       " 'fi ff fl ffl ffi.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It seems the cleaner leaves in some other stuff, we leave this in given that it includes the alphabet.\n",
    "super_cleaner(load_etext(23594), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id = 22818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stored_sentences(book_id):\n",
    "    print(book_id)\n",
    "    filenames = ['sentences_8.pkl', 'sentences_32.pkl', 'sentences_128.pkl']\n",
    "    for file in filenames:\n",
    "        with open(os.path.join('../pretraining_data_chunked', str(book_id), file), 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "            print(sentences)\n",
    "    print('==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n",
      "['by Virgil']\n",
      "[]\n",
      "[]\n",
      "==============\n",
      "22818\n",
      "['   An Alphabet   of Celebrities']\n",
      "[]\n",
      "[]\n",
      "==============\n",
      "22335\n",
      "[]\n",
      "['Transcriber\\'s Note: Original spells the title \"Nursury.\" This was retained.']\n",
      "[]\n",
      "==============\n",
      "23594\n",
      "[]\n",
      "[' a b c d e f g h i j k l m n o p q r s t u v w x y z &.', 'fi ff fl ffl ffi.']\n",
      "['Online Distributed Proofreading Team at http://www.pgdp.net (This file was produced from images generously made available by The Internet Archive/American Libraries.)']\n",
      "==============\n",
      "20086\n",
      "[]\n",
      "['       Where differences between the list of illustrations and the       caption text existed in the original the most comprehensive       description was used for both.']\n",
      "[]\n",
      "==============\n",
      "20360\n",
      "[]\n",
      "[\"   Entered at Stationer's Hall\", '  P. 13, l. 7, for mighty read magick.']\n",
      "['      Go we to the Committee room,     There gleams of light conflict with gloom,     While unread rheams in chaos lye,     Our water closets to supply.', \"    Noodles{3}, who rave for abolition     Of th' African's improv'd condition{4},     At your own cost fine projects try;     Dont rob--from pure humanity.\"]\n",
      "==============\n",
      "10557\n",
      "['And improved his little Garden.', \"Didn't understand her;\", 'Got entangled with the rake', 'Had a very nasty knock;', 'Looked quite regal', 'Made a very big Meal;', 'Sang a sentimental Air,', 'Tried to paint the Roses blue', 'Used him for a pillow;', 'Went to sleep,']\n",
      "['\"We\\'ll never come again', 'And the Cockatoo Said \"Comment vous portez vous?\"', 'And the Reindeer Said: \"I\\'m sorry for your pain, dear!\"', 'But the Flamingo Talked the same lingo', 'But the Giraffe Was inclined to laugh;', 'Of Johnny Crow and his Garden.', 'Said: \"Wake me if for talk you pine!\"', 'So the Chimpanzee Put the Kettle on for Tea;', 'Then they picked the Flowers, and Wandered in the Maze, And before they went their several ways', 'They all joined together In a Hearty Vote of Praise', 'Till the Camel Swallowed the Enamel.']\n",
      "[]\n",
      "==============\n",
      "19571\n",
      "[' Audio formats available:']\n",
      "[' This audio reading of A Noiseless Patient Spider is read by', 'Librivox volunteers bring you eight different readings of Walt Whitmans A Noiseless Patient Spider, a weekly poetry project.']\n",
      "['128kbit MP3 - MP3 subfolder 64kbit Ogg Vorbis (variable bit rate) - OGG subfolder Apple AAC audiobook (16kbit mono) - M4B subfolder Speex - SPX subfolder', 'Dedicator recognizes that, once placed in the public domain, the Work may be freely reproduced, distributed, transmitted, used, modified, built upon, or otherwise exploited by anyone for any purpose, commercial or non-commercial, and in any way, including by methods that have not yet been invented or conceived.']\n",
      "==============\n",
      "19177\n",
      "['13 Come Lasses and Lads', '7 The Queen of Hearts']\n",
      "['10 Hey-Diddle-Diddle and Baby Bunting', '14 Ride a Cock Horse to Banbury Cross, &c.', '3 The Babes in the Wood', '6 Sing a Song for Sixpence', 'Collection of Pictures and Songs No. 1 containing the first 8 books listed above with their Color Pictures and numerous Outline Sketches', 'Collection of Pictures and Songs No. 2 containing the second 8 books listed above with their Color Pictures and numerous Outline Sketches']\n",
      "[' \"The humour of Randolph Caldecott\\'s drawings is simply irresistible, no healthy-minded man, woman, or child could look at them without laughing.\"']\n",
      "==============\n",
      "14100\n",
      "[]\n",
      "[]\n",
      "[\"With throbbing bosoms shall the wanderers tread The hallowed mansions of the silent dead, Shall enter the long isle and vaulted dome Where Genius and where Valour find a home; Awe-struck, midst chill sepulchral marbles breathe, Where all above is still, as all beneath; Bend at each antique shrine, and frequent turn To clasp with fond delight some sculptured urn, The ponderous mass of Johnson's form to greet, Or breathe the prayer at Howard's sainted feet.\", \"Yes, thou must droop; thy Midas dream is o'er; The golden tide of Commerce leaves thy shore, Leaves thee to prove the alternate ills that haunt      [6] Enfeebling Luxury and ghastly Want; Leaves thee, perhaps, to visit distant lands, And deal the gifts of Heaven with equal hands.\", \"Bounteous in vain, with frantic man at strife, Glad Nature pours the means--the joys of life;In vain with orange blossoms scents the gale, The hills with olives clothes, with corn the vale; Man calls to Famine, nor invokes in vain, Disease and Rapine follow in her train; The tramp of marching hosts disturbs the plough, The sword, not sickle, reaps the harvest now, And where the Soldier gleans the scant supply.The helpless Peasant but retires to die; No laws his hut from licensed outrage shield,           [3] And war's least horror is the ensanguined field.\", '', \"There walks a Spirit o'er the peopled earth, Secret his progress is, unknown his birth; Moody and viewless as the changing wind, No force arrests his foot, no chains can bind; Where'er he turns, the human brute awakes, And, roused to better life, his sordid hut forsakes: He thinks, he reasons, glows with purer fires, Feels finer wants, and burns with new desires: Obedient Nature follows where he leads; The steaming marsh is changed to fruitful meads; The beasts retire from man's asserted reign, And prove his kingdom was not given in vain.\", \"Then from its bed is drawn the ponderous ore,           [18]Then Commerce pours her gifts on every shore, Then Babel's towers and terrassed gardens rise, And pointed obelisks invade the skies; The prince commands, in Tyrian purple drest, And gypt's virgins weave the linen vest. Then spans the graceful arch the roaring tide, And stricter bounds the cultured fields divide.Then kindles Fancy, then expands the heart, Then blow the flowers of Genius and of Art; Saints, Heroes, Sages, who the land adorn, Seem rather to descend than to be born;\", 'Whilst History, midst the rolls consigned to fame, With pen of adamant inscribes their name.']\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "for book_id in subset_ratio_100K['subset_booklist'][:10]:\n",
    "    stored_sentences(book_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original plan for tokenization may not work as well as desired for certain books with specific text entries\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take for example book 23880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23880\n",
      "[]\n",
      "['Lasiurus borealis ornatus new subspecies', 'The Mexican red bat, thus, is left without a name, and for it I propose', 'University of Kansas Publications Museum of Natural History Volume 5, No. 14, pp. 223-226 December 15, 1951', 'Volume 5, No. 14, pp. 223-226 December 15, 1951']\n",
      "['Accordingly, the name A[talapha]. mexicana Saussure 1861 falls as a synonym of Lasiurus cinereus cinereus (Beauvois 1796); if the hoary bat of the southern end of the Mexican table land should prove to be subspecifically separable, the name Lasiurus cinereus mexicanus would be available for it.', 'As may be readily seen by comparing specimens of L. borealis and L. cinereus from Mexico (or also from any place in North America north of Mexico), the description by Saussure applies to the hoary bat (Lasiurus cinereus) and not to the red bat (Lasiurus borealis).', 'Long inrolled tail; femoral patagium as in the vespertilios. Teeth 4/2, 1/1, 4/5 or 5/5.']\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "stored_sentences(23880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101.,   103.,  1010.,  ...,     0.,     0.,     0.],\n",
       "         [  101., 11914.,   103.,  ...,     0.,     0.,     0.],\n",
       "         [  101., 11914.,  1010.,  ...,     0.,     0.,     0.],\n",
       "         ...,\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.],\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.],\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.]]),\n",
       " 'attention_mask': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]]),\n",
       " 'labels': tensor([[  101., 11914.,  1010.,  ...,     0.,     0.,     0.],\n",
       "         [  101., 11914.,  1010.,  ...,     0.,     0.,     0.],\n",
       "         [  101., 11914.,  1010.,  ...,     0.,     0.,     0.],\n",
       "         ...,\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.],\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.],\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.]])}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors = torch.load('../pretraining_data_chunked/23880/tensors_128.pt')\n",
    "tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the ratios as given at the end of one of the longer sentences gets masked because of how we replace text by masks in the sentence (a result of the whole-word mask strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] as may be readily seen by comparing specimens of l . borealis and l . cinereus from mexico ( or also from any place in north america north of mexico ) , the description by saussure applies to the hoary bat ( lasiurus cinereus ) and [MASK] to the red bat ( lasiurus borealis ) . [SEP]\n",
      "[CLS] as may be readily see by compare specimen of l . boreali and l . cinereus from mexico ( or also from any place in north america north of mexico ) , the description by saussure apply to the hoary bat ( lasiurus cinereus ) and not to the red bat ( lasiurus boreali ) . [SEP]\n",
      "[CLS] as may be readily seen by comparing specimens of l . borealis and l . cinereus from mexico ( or also from any place in north america north of mexico ) , the description by saussure applies to the hoary bat ( lasiurus cinereus ) and not to the red bat ( lasiurus borealis ) . [SEP]\n",
      "[CLS] [MASK] inrolled tail ; femoral patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long [MASK] [MASK] [MASK] tail ; femoral patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled [MASK] ; femoral [MASK] [MASK] [MASK] as in the [MASK] [MASK] [MASK] [MASK] [MASK] . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail [MASK] femoral patagium as in the vespertilios [MASK] teeth 4 / 2 [MASK] 1 / 1 [MASK] 4 / 5 or 5 / 5 [MASK] [SEP]\n",
      "[CLS] long inrolled tail ; [MASK] [MASK] patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long [MASK] rolled tail ; femoral patagium [MASK] [MASK] the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail ; femoral patagium as in [MASK] vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail ; femoral patagium as in the vespertilios . [MASK] 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail ; femoral patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail ; femoral patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 [MASK] 5 / 5 . [SEP]\n",
      "[CLS] long inrolle tail ; femoral patagium as in the vespertilio . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail ; femoral patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n"
     ]
    }
   ],
   "source": [
    "for row in tensors['input_ids'][-15:]:\n",
    "    print(tokenizer.convert_tokens_to_string([x for x in tokenizer.convert_ids_to_tokens(row) if x != '[PAD]']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Long inrolled tail; femoral patagium as in the vespertilios. Teeth 4/2, 1/1, 4/5 or 5/5.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'long', 'in', '##roll', '##ed', 'tail', ';', 'fe', '##moral', 'pat', '##agi', '##um', 'as', 'in', 'the', 've', '##sper', '##ti', '##lio', '##s', '.', 'teeth', '4', '/', '2', ',', '1', '/', '1', ',', '4', '/', '5', 'or', '5', '/', '5', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenizer('Long inrolled tail; femoral patagium as in the vespertilios. Teeth 4/2, 1/1, 4/5 or 5/5.')['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long | ADV | long\n",
      "inrolled | VERB | inrolle\n",
      "tail | NOUN | tail\n",
      "; | PUNCT | ;\n",
      "femoral | ADJ | femoral\n",
      "patagium | NOUN | patagium\n",
      "as | ADP | as\n",
      "in | ADP | in\n",
      "the | DET | the\n",
      "vespertilios | NOUN | vespertilio\n",
      ". | PUNCT | .\n",
      "Teeth | PROPN | Teeth\n",
      "4/2 | NUM | 4/2\n",
      ", | PUNCT | ,\n",
      "1/1 | NUM | 1/1\n",
      ", | PUNCT | ,\n",
      "4/5 | NUM | 4/5\n",
      "or | CCONJ | or\n",
      "5/5 | NUM | 5/5\n",
      ". | PUNCT | .\n"
     ]
    }
   ],
   "source": [
    "#Text is parsed in 1 go by Spacy, but is recognized as seperate tokens by BERT\n",
    "for token in doc:\n",
    "    print(token.text, '|', token.pos_, '|', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10625', '22', '19447', '19217', '15476']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_ratio_100M['subset_booklist'][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''He is really a good man, and is lucky enough, or the reverse, to win the hand of a delightful young lady whose charms, however, do not command the unanimous approval of the parishioners. ssession of high musical attainments makes her temperament all the more interesting, and accounts for the presence in so remote a district of her German friend whose acute sense of the rius leads to such untoward results. It is hard to say whether the author's talents are best evinced by her true pathos or by the delicate touches of humour which pervade the book.Another cable feature of the novel is an alert skill in construction which stamps it as a thoroughly artistic production.'''\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is really a good man, and is lucky enough, or the reverse, to win the hand of a delightful young lady whose charms, however, do not command the unanimous approval of the parishioners.\n",
      "ssession of high musical attainments makes her temperament all the more interesting, and accounts for the presence in so remote a district of her German friend whose acute sense of the rius leads to such untoward results.\n",
      "It is hard to say whether the author's talents are best evinced by her true pathos or by the delicate touches of humour which pervade the book.\n",
      "Another cable feature of the novel is an alert skill in construction which stamps it as a thoroughly artistic production.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro M1200'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 4776, 2253, 2000, 1996, 4789, 2002, 28418, 2078, 2012, 1019, 1051, 1005, 5119, 2000, 4965, 2070, 6501, 2005, 2033, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "default_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,  2253,  2000,  1996,   103,   103,   103,   103,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,   103,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,   103,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,   103,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,   103,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,   103,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,   103,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "           103,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,   103,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,   103,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "           103,   102],\n",
       "        [  101,  4776,  2175,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  1045,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1051,  1005,  5119,  1019,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]])}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ST_tokenizer = StrategizedTokenizer(padding=True)\n",
    "inputs = ST_tokenizer.tokenize(text)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1344\n",
    "#14596\n",
    "\n",
    "test_book = super_cleaner(load_etext(14596), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#longest_sentence = str(test_book[np.argmax([len(par) for par in test_book])])\n",
    "longest_sentence = text = '''He is really a good man, and is lucky enough, or the reverse, to win the hand of a delightful young lady whose charms, however, do not command the unanimous approval of the parishioners. ssession of high musical attainments makes her temperament all the more interesting, and accounts for the presence in so remote a district of her German friend whose acute sense of the rius leads to such untoward results. It is hard to say whether the author's talents are best evinced by her true pathos or by the delicate touches of humour which pervade the book.Another cable feature of the novel is an alert skill in construction which stamps it as a thoroughly artistic production.'''\n",
    "len(longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC = SentenceChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens, sentences = SC.sentence_chunker(longest_sentence, 128, return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"He is really a good man, and is lucky enough, or the reverse, to win the hand of a delightful young lady whose charms, however, do not command the unanimous approval of the parishioners. ssession of high musical attainments makes her temperament all the more interesting, and accounts for the presence in so remote a district of her German friend whose acute sense of the rius leads to such untoward results. It is hard to say whether the author's talents are best evinced by her true pathos or by the delicate touches of humour which pervade the book.Another cable feature of the novel is an alert skill in construction which stamps it as a thoroughly artistic production.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['232', '22818', '22335', '23594', '20086', '20360', '10557', '19571', '19177', '14100', '13536', '23436', '129', '21783', '11006', '19937', '22847', '1321', '23147', '21805', '22529', '12474', '13082', '14463', '23538', '13081', '116', '18589', '23446', '23450', '17124', '16780', '23146', '18935', '12554', '17254', '23429', '13203', '17365', '22236', '16169', '18417', '22579', '19634', '24044', '104', '1567', '23315', '24269', '12358', '23880']\n"
     ]
    }
   ],
   "source": [
    "print(subset_ratio_100K['subset_booklist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-26 10:08:59.539493\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_data_splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-3ccf67576d31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext_splits_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_data_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14596\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_lengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'chunk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtext_splits_trunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_data_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14596\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_lengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_data_splits' is not defined"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "text_splits_chunk = make_data_splits(14596, max_seq_lengths=[8,32,128], truncate='chunk')\n",
    "text_splits_trunc = make_data_splits(14596, max_seq_lengths=[8,32,128], truncate=True)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../pretraining_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        examples = torch.tensor(np.array([[101, 1996, 2622, 9535, 11029, 26885, 1997, 102, 0, 0, 0, 0, 0], \n",
    "                             [101,2198, 9535, 11029, 1010, 2011, 8965, 3854, 22033, 9050, 3064, 102, 0],\n",
    "                             [101, 2102, 2023, 26885, 2003, 2005, 1996, 2224, 1997, 3087, 5973, 2012, 102]])).long()\n",
    "        self.encodings = examples\n",
    "        self.labels = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {'input_ids': self.encodings[i],\n",
    "                'labels': self.labels[i]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ExampleListDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny_config = {\"hidden_size\": 128, \n",
    "                    \"hidden_act\": \"gelu\", \n",
    "                    \"initializer_range\": 0.02, \n",
    "                    \"vocab_size\": 30522, \n",
    "                    \"hidden_dropout_prob\": 0.1, \n",
    "                    \"num_attention_heads\": 2, \n",
    "                    \"type_vocab_size\": 2, \n",
    "                    \"max_position_embeddings\": 128, \n",
    "                    \"num_hidden_layers\": 2, \n",
    "                    \"intermediate_size\": 512, \n",
    "                    \"attention_probs_dropout_prob\": 0.1}\n",
    "\n",
    "model = BertForMaskedLM(config=BertConfig(**bert_tiny_config))\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dir = './test_experiment'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= os.path.join(output_dir, 'model2'),          # output directory\n",
    "    overwrite_output_dir = True,\n",
    "    save_strategy='no',  #dont make checkpoints, easier to just retrain than continu given the experiment\n",
    "    max_steps = 3,\n",
    "    per_device_train_batch_size=2,  # batch size per device during training\n",
    "    #per_device_eval_batch_size=256,   # batch size for evaluation\n",
    "    learning_rate=1e-5,     \n",
    "    logging_dir= os.path.join(output_dir, 'model2', 'logs/'),            # directory for storing logs\n",
    "    logging_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    model=model,                         # the instantiated  Transformers model to be trained\n",
    "    train_dataset=train_data,         # training dataset\n",
    "    eval_dataset=None            # evaluation dataset\n",
    ")\n",
    "\n",
    "train_output = trainer.train()\n",
    "#trainer.save_model(os.path.join(output_dir, 'model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(output_dir, 'model2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import DataCollatorForWholeWordMask\n",
    "from dataset.dataset import DefaultTokenizerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 Loaded books:  [24269]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DefaultTokenizerDataset(datadir='../pretraining_data_chunked', max_seq_length=128)\n",
    "train_dataset.populate(book_list=[24269])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          truncation=True, \n",
    "                                          max_length=128,\n",
    "                                          padding='max_length')\n",
    "data_collator = data_collator = DataCollatorForWholeWordMask(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny_config = {\"hidden_size\": 128, \n",
    "                    \"hidden_act\": \"gelu\", \n",
    "                    \"initializer_range\": 0.02, \n",
    "                    \"vocab_size\": 30522, \n",
    "                    \"hidden_dropout_prob\": 0.1, \n",
    "                    \"num_attention_heads\": 2, \n",
    "                    \"type_vocab_size\": 2, \n",
    "                    \"max_position_embeddings\": 128, \n",
    "                    \"num_hidden_layers\": 2, \n",
    "                    \"intermediate_size\": 512, \n",
    "                    \"attention_probs_dropout_prob\": 0.1}\n",
    "\n",
    "model = BertForMaskedLM(config=BertConfig(**bert_tiny_config))\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../test-default_bert\",\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=4,\n",
    "    save_strategy='no',\n",
    "    per_device_train_batch_size=1,\n",
    "    logging_steps=1,\n",
    "    \n",
    "    #Hyper parameters as per BERT-paper which are not default values in TrainingArguments\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.329400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.387700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=10.369400262832642, metrics={'train_runtime': 39.6518, 'train_samples_per_second': 0.101, 'total_flos': 9853183776.0, 'epoch': 0.1, 'init_mem_cpu_alloc_delta': 498556928, 'init_mem_gpu_alloc_delta': 17471488, 'init_mem_cpu_peaked_delta': 14729216, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': -108642304, 'train_mem_gpu_alloc_delta': 52412928, 'train_mem_cpu_peaked_delta': 108793856, 'train_mem_gpu_peaked_delta': 65876992})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([  101,  1162, 14608, 16177, 14608, 29734, 15297,  1158, 29723,  1174,\n",
       "          29730, 18199,  1159, 29729,  1155, 29727, 15297,  1155, 29735, 29734,\n",
       "          29739,  1155, 29720, 29727, 24824, 29737, 29732, 15297,  1166, 14608,\n",
       "          29727, 14608,  1174, 29730, 18199, 15297,  1159, 29727, 29723, 29735,\n",
       "          29733, 29723, 29734, 14608, 18199,  1012,   102])},\n",
       " {'input_ids': tensor([  101,  2665,  1005,  1055,  2460,  2381,  1997,  1996,  2394,  2111,\n",
       "           1010,  5824,  2581,  1010,  5824,  2620,  1012,  1996,  8416,  3179,\n",
       "           2003,  1999,  1016, 18709,  1012,  2030,  1015,  5285,  1012,  2035,\n",
       "           2060,  6572,  2024,  1999,  1015,  5285,  1012,   102])},\n",
       " {'input_ids': tensor([  101,  3602,  1011,  1011,  1996,  2206,  3616,  2024,  2012,  2556,\n",
       "           2041,  1997,  6140,  1024,  7287,  1010, 11118,  1010, 16333,  1010,\n",
       "          22238,  1010, 24194,  1010, 17528,  1010, 20024,  1010,  4601,  2620,\n",
       "           1010,  5354,  2581,   102])},\n",
       " {'input_ids': tensor([  101, 10625,  1005,  1055,  9706, 12898, 10440,  4013, 19300, 10514,\n",
       "           2050,  1010,  6191,  2575,  1000,  2006,  1996,  9531,  1998,  3267,\n",
       "           1997,  2118,  2495,  1010,  1998,  1037,  3259,  2006,  7988,  1998,\n",
       "           4045,  4812,  1010,  5824,  2509,   102])},\n",
       " {'input_ids': tensor([  101, 23923,  1012,  1060,  1012,  1048,  1012,  7886,  1011, 10114,\n",
       "           1006,  7570,  2213,  1012,  1060,  1012,  1048,  1012,  6282,  1011,\n",
       "           6564,  1007,  1012,  1011,  1011,  2009,  2003,  2218,  2085,  2008,\n",
       "           2023,  6019,  2323,  2022,  4541,  2011,  1996, 10514,  9397, 19234,\n",
       "           2008,  1996, 11525,  2594, 22759,  2015,  2018,  2657,  7122,  1997,\n",
       "           2642, 15250,  2015,  1010,  2073,  1010,  1999,  2621,  1011,  2051,\n",
       "           1010,  1996,  4768,  2001,  2061,  2460,  2008,  3944,  2001,  2628,\n",
       "           2471,  2012,  2320,  2011,  2851,  1012,  2947,  1996, 28822,  2386,\n",
       "           2746,  2188,  1999,  1996, 13132,  2012,  2028,  2154,  1005,  1055,\n",
       "           2485,  2453,  3113,  1998, 16889,  1996, 11133,  2040,  2001,  3225,\n",
       "           6655, 14428,  2015,  2005,  1996,  2279,  2154,  1005,  1055,  2147,\n",
       "           1012,   102])},\n",
       " {'input_ids': tensor([  101,  7327,  9153, 15222,  2271,  1010,  1998,  8359,  2044,  2032,\n",
       "           1010,  3305,  2019,  9706, 20049, 17635,  6190,  2182,  1010,  2004,\n",
       "           2065,  1996,  5882,  3214,  2000,  2360,  1011,  1011,  2054,  2065,\n",
       "           2045,  2323,  2022,  1029,  2030,  1011,  1011,  6814,  2045,  2323,\n",
       "           2022,  1029,  2021,  1996,  6251,  3849,  2000,  2991,  1999,  2488,\n",
       "           2007,  2054,  4076, 10009,  2004,  2682,  1010,  1998,  2009,  2003,\n",
       "           1037,  3168,  1997,  1996,  6019,  2025,  4895,  9028, 17884,  2098,\n",
       "           2011,  1996,  5448,  1997,  2060, 15957,  1012,  2156,  8040,  3270,\n",
       "          16093,  2884, 14859,  2271,  1012,   102])},\n",
       " {'input_ids': tensor([  101,  1999,  1037,  2473,  1997,  1996,  5932,  1010, 27383,  4455,\n",
       "           2037,  3086,  2000, 22784,  1010,  2145,  1037, 17677,  2121,  1012,\n",
       "           2027, 10663,  2000,  3946,  2032,  1037,  3647,  2709,  2000, 27939,\n",
       "           1012, 27383, 23328,  2000,  8627, 10093, 14545, 16194,  1010,  1998,\n",
       "           1999,  1996,  2433,  1997,  2273,  4570, 23303,  2032,  1999,  2054,\n",
       "           5450,  2000, 10838,  1012,  2802,  2023,  2338,  1996,  4469,  3567,\n",
       "           5289,  3401,  1998, 11268, 14715,  5666,  1997,  1996,  4848,  5668,\n",
       "           2024,  5681,  4081,  1012,   102])},\n",
       " {'input_ids': tensor([  101,  2240,  6564,  1999,  1996,  3306, 11276,  2763,  2000,  2022,\n",
       "           5421,  1010,  1000,  2005,  1996, 10425,  1997,  2305,  1998,  2154,\n",
       "           2024,  2485,  2362,  1010,  1000,  1045,  1012,  1041,  1012,  1010,\n",
       "           1996,  4211,  1997,  2154,  4076,  2524,  2006,  1996,  4211,  1997,\n",
       "           2305,  1012,   102])},\n",
       " {'input_ids': tensor([  101,  8714,  6468,  2000, 10250, 22571,  6499,  1037,  3094,  2013,\n",
       "          13035,  2008,  2016, 19776, 22784,  1012,  2016,  1010,  2044,  2070,\n",
       "           2128, 16563,  6494,  5897,  2015,  1010, 10659, 22645,  1010,  1998,\n",
       "           6519, 24014,  2229,  2032,  2007,  5693,  1998,  4475,  1010,  2007,\n",
       "           2029,  2002,  9570,  2015,  1037, 21298,  1012,  2002,  8046,  2015,\n",
       "          10250, 22571,  6499,  1005,  1055,  2479,  1025,  2003, 27666,  2011,\n",
       "          21167,  2007, 21794, 22553,  2015,  1010,  2021,  2011,  1996,  5375,\n",
       "           1997,  1037,  2712,  6396,  8737,  2232,  1010,  2044,  2383,  2439,\n",
       "           2010, 21298,  1010,  2003,  9124,  2000,  9880,  2000,  6887, 29667,\n",
       "           6305,  2401,  1012,   102])},\n",
       " {'input_ids': tensor([  101,  8714, 17976,  1996,  9293,  1997,  1996,  4848,  5668,  2091,\n",
       "           2000,  4748,  2229,  1012, 22784,  9418,  2370,  2000,  2474,  8743,\n",
       "           2229,  1010,  1998, 10861, 12718,  1010,  2011,  1996,  4681,  1997,\n",
       "          27383,  1010,  2019, 27860,  1997,  1996,  2111, 24501, 26951,  1996,\n",
       "           2331,  1997,  1996,  4848,  5668,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 27383, 12697,  2019,  4357,  2090,  1996,  2684,  1997,  2632,\n",
       "          21081,  2271,  1998, 22784,  1010,  4748,  8202,  4509,  2229,  2014,\n",
       "           1999,  1037,  3959,  2000,  4287,  2091,  2014,  4253,  2000,  1996,\n",
       "           2314,  1010,  2008,  2016,  2089,  9378,  2068,  1010,  1998,  2191,\n",
       "           2068,  3201,  2005,  2014,  8455, 16371, 13876, 26340,  1012,  2008,\n",
       "           4708,  2864,  1010,  1996,  4615,  1998,  2014,  3345,  2572,  8557,\n",
       "           3209,  2007,  2377,  1025,  2011,  4926,  2027,  8300, 22784,  1025,\n",
       "           2002,  3310,  5743,  2013,  1996,  3536,  1010,  1998, 12033,  2370,\n",
       "           2007,  2172,  4769,  2000,  6583,  2271,  5555,  2050,  1010,  2040,\n",
       "          15398,  5844,  2010, 24305,  4650,  1010,  1998,  2108,  2172,  5360,\n",
       "           2011,  1996, 13372,  1997,  2010,  3311,  1010,  5426,  2370,  1999,\n",
       "           2010,  7927,  1010,  1998, 17976,  2032,  2000,  1996,  2103,  1012,\n",
       "            102])},\n",
       " {'input_ids': tensor([  101, 27383,  6010,  2032,  2006,  1996,  5370,  1010, 12939,  2032,\n",
       "           2000, 28667, 14511, 22471,  2010,  2406,  1010,  2029,  1010,  6229,\n",
       "           4372,  7138,  6675,  2011,  2014,  1010,  2002,  3373,  2000,  2022,\n",
       "           1037,  2406,  4326,  2000,  2032,  1010,  1998,  2027,  4164,  2362,\n",
       "           1996,  2965,  1997,  9846,  1996,  4848,  5668,  1012,  1996,  7804,\n",
       "           2059, 10315,  2000, 21251,  2000,  2655, 23166, 10093, 14545, 16194,\n",
       "           1010,  1998, 22784,  1010,  2011,  2014,  4681, 17330,  2066,  1037,\n",
       "          11693,  6843,  1010, 10951,  2875,  1996,  9151,  1997,  7327,  2213,\n",
       "          29667,  2271,  1012,   102])},\n",
       " {'input_ids': tensor([  101,  6583,  2271,  5555,  2050,  5651,  2013,  1996,  2314,  1010,\n",
       "           3183, 22784,  4076,  1012,  2002,  9190,  2015,  1010,  2011,  2014,\n",
       "           3257,  1010,  2012,  1037,  2235,  3292,  2013,  1996,  4186,  1010,\n",
       "           2029,  2012,  1037, 14057,  2051,  2002,  8039,  1012,  2002,  2003,\n",
       "           2092,  2363,  2011,  2632, 21081,  2271,  1998,  2010,  3035,  1025,\n",
       "           1998,  2383,  3141,  2000,  2068,  1996,  5450,  1997,  2010,  2108,\n",
       "           3459,  2006,  1996,  5370,  1997,  8040,  5886,  2401,  1010,  1998,\n",
       "           2363,  2013,  2632, 21081,  2271,  1996,  4872,  1997,  3647,  6204,\n",
       "           2188,  1010, 11036,  2015,  2000,  2717,  1012,   102])},\n",
       " {'input_ids': tensor([  101,  2500, 17637,  1010,  1000,  1998,  2013,  1996,  2712,  4618,\n",
       "          15177,  2219,  2331,  2272,  1010,  1000,  9104,  2008, 22784,  2044,\n",
       "           2035,  2001,  2439,  2012,  2712,  1012,  2023,  2003,  1996, 14259,\n",
       "           2628,  2011,  2702,  4890,  3385,  1999,  2010,  5961,  1000, 22784,\n",
       "           1000,  1006,  1998,  2156,  9649,  1010, 21848,  1010,  2064,  3406,\n",
       "          22038,  5737,  1012,  1007,  1012,  2009,  2003,  1037,  2062,  3019,\n",
       "           5449,  1997,  1996,  3306,  1010,  1998,  3957,  1037,  2521,  2062,\n",
       "           6919, 13005,  2005,  1996,  2485,  1997,  1996, 17677,  2121,  1005,\n",
       "           1055,  2166,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 18641, 17146,  2000,  1996,  4848,  5668,  1037,  5049,  2007,\n",
       "           1996,  6812,  1010,  2841,  1996,  3396,  1012,  2027,  6011,  4039,\n",
       "           2000,  8815,  1996,  6812,  1025,  2043, 22784,  2383,  2007,  2070,\n",
       "           7669,  8679,  2370,  1997,  2009,  1010,  9020,  2009,  2007,  1996,\n",
       "          27917,  7496,  1010,  1998, 18365,  2229,  2010,  8612,  2083,  4376,\n",
       "           7635,  7019,  2005,  1996,  3979,  1012,   102])},\n",
       " {'input_ids': tensor([  101,  4831,  2038,  2445,  2053,  5449,  1997,  2023,  2240,  1999,\n",
       "           1996,  3793,  1997,  2010,  2147,  1010,  2021,  2038,  5421,  2009,\n",
       "           1999,  1037,  3602,  1012,  2009,  2003, 17611, 10009,  2011, 15957,\n",
       "           1025,  1996,  3168,  2029,  2003,  2182,  2445,  1997,  2009,  2003,\n",
       "           2008,  6749,  2011,  7327,  9153, 15222,  2271,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 10093, 14545, 16194,  7194,  2012,  1052, 18871,  1010,  4372,\n",
       "          15549,  6072,  1997, 25397,  7175, 22784,  1012, 25397, 14623,  2000,\n",
       "           2032,  2035,  2008,  2002,  4282,  2030,  2038,  2657,  1997,  1996,\n",
       "          24665,  4402, 14483,  2015,  2144,  2037,  6712,  2013,  1996,  6859,\n",
       "           1997,  9553,  1010,  2021,  2025,  2108,  2583,  2000,  2507,  2032,\n",
       "           2151, 23045,  4070,  1997, 22784,  1010,  5218,  2032,  2000,  2273,\n",
       "          10581,  2271,  1012,  2012,  3944, 27383,  8046,  2015, 10093, 14545,\n",
       "          16194,  1010,  2021,  9418,  2841,  1999,  2183,  1012, 25397, 18502,\n",
       "           2000,  1996,  7804,  1010,  1998,  1996, 19487,  3012,  3092,  1010,\n",
       "          10093, 14545, 16194,  4520,  5743,  2005, 21251,  1999,  2028,  1997,\n",
       "          25397,  1005,  1055, 23507,  2015,  1010,  1998,  5642,  2011, 25397,\n",
       "           1005,  1055,  2365,  1010, 14255,  6190,  6494,  5809,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 10093, 14545, 16194, 18365,  2229,  7327,  2213, 29667,  2271,\n",
       "           2000,  1996,  2103,  2000, 12367, 18641,  1997,  2010,  3647,  2709,\n",
       "           2013,  1052, 18871,  1025,  2076,  2010,  6438,  1010, 22784,  3084,\n",
       "           2370,  2124,  2000,  2010,  2365,  1012,  1996,  4848,  5668,  1010,\n",
       "           2383,  3427,  2005, 10093, 14545, 16194,  1999, 15784,  1010,  7180,\n",
       "           2153,  2012, 27939,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 10093, 14545, 16194,  5651,  2000,  1996,  2103,  1010,  1998,\n",
       "          14623,  2000,  2010,  2388,  1996,  4054, 13768,  1997,  2010,  8774,\n",
       "           1025, 22784,  1010,  4146,  2011,  7327,  2213, 29667,  2271,  1010,\n",
       "           8480,  2045,  2036,  1010,  1998,  8039,  2426,  1996,  4848,  5668,\n",
       "           1010,  2383,  2042,  2124,  2069,  2011,  2010,  2214,  3899, 25294,\n",
       "           1010,  2040,  8289,  2012,  2010,  2519,  1012,  1996, 10628,  1997,\n",
       "          18641,  2108,  7568,  2011,  1996,  4070,  2029,  7327,  2213, 29667,\n",
       "           2271,  3957,  2014,  1997, 22784,  1010,  2016,  4449,  2032,  3202,\n",
       "           2046,  2014,  3739,  1010,  2021, 22784,  2695, 29513,  2015,  1996,\n",
       "           4357,  6229,  3944,  1010,  2043,  1996,  4848,  5668,  2383,  2187,\n",
       "           1996,  4186,  1010,  2045,  4618,  2022,  2053,  5473,  1997, 24191,\n",
       "           1012,  7327,  2213, 29667,  2271,  5651,  2000,  2010,  9151,  1012,\n",
       "            102])},\n",
       " {'input_ids': tensor([  101, 10093, 14545, 16194,  1010,  4748,  8202, 13295,  2011, 27383,\n",
       "           1010,  3138,  2681,  1997,  2273, 10581,  2271,  1010,  2021,  9413,\n",
       "           2063,  2002, 17553,  1010,  2003, 16222, 14122,  2098,  2011, 14833,\n",
       "          20464, 25219, 15460,  1010,  1037, 12168,  1997, 12098, 12333,  1010,\n",
       "           3183,  2012,  2010, 17300,  5227,  2002,  3138,  2006,  2604,  1012,\n",
       "           1999,  1996, 12507,  7327,  2213, 29667,  2271, 14623,  2000, 22784,\n",
       "           1996,  2965,  2011,  2029,  2002,  2234,  2000, 27939,  1012, 10093,\n",
       "          14545, 16194,  7194,  2045,  1010,  3957,  4449,  2005,  1996,  2709,\n",
       "           1997,  2010, 11286,  2000,  1996,  2103,  1010,  1998, 10315,  2370,\n",
       "           2000,  7327,  2213, 29667,  2271,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 10093, 14545, 16194,  1010,  2007, 14255,  6190,  6494,  5809,\n",
       "           1010,  8480,  2012,  1996,  4186,  1997,  2273, 10581,  2271,  1010,\n",
       "           2013,  3183,  2002,  8267,  2070,  4840,  2592,  7175,  1996,  2709,\n",
       "           1997,  1996, 24665,  4402, 14483,  2015,  1010,  1998,  2003,  1999,\n",
       "           3327,  2409,  2006,  1996,  3691,  1997,  4013,  2618,  2271,  1010,\n",
       "           2008,  2010,  2269,  2003, 14620,  2011, 10250, 22571,  6499,  1012,\n",
       "           1996,  4848,  5668,  1010, 20699,  2114,  1996,  2166,  1997, 10093,\n",
       "          14545, 16194,  1010,  4682,  1999,  3524,  2000, 19115,  2032,  1999,\n",
       "           2010,  2709,  2000, 27939,  1012, 18641,  2108,  6727,  1997,  2010,\n",
       "           6712,  1010,  1998,  1997,  2037,  5617,  2000, 22889,  4710,  2032,\n",
       "           1010,  4150,  4297,  5644,  6030,  3468,  1010,  2021,  2003,  7653,\n",
       "           2011,  1037,  3959,  2741,  2000,  2014,  2013, 27383,  1012,   102])},\n",
       " {'input_ids': tensor([  101,  1996,  6887, 29667,  6305,  7066, 23363,  2006,  1996,  3395,\n",
       "           1997, 22784,  1012,  7547,  2003,  2081,  2005,  2010,  6712,  1012,\n",
       "           3424, 18674, 20432,  2015,  2068,  2012,  2010,  2795,  1012,  2399,\n",
       "           3582,  1996,  4024,  1012,  9703,  3527,  7874,  1996, 22759, 10955,\n",
       "           1010,  2034,  1996,  7459,  1997,  7733,  1998, 11691,  1010,  2059,\n",
       "           1996,  4955,  1997,  1996,  4799,  3586,  2046,  9553,  1012, 22784,\n",
       "           1010,  2172,  5360,  2011,  2010,  2299,  1010,  2003,  8781,  2011,\n",
       "           2632, 21081,  2271,  1010,  2043,  3401,  1010,  1998,  2040,  2002,\n",
       "           2003,  1010,  1998,  2054,  2003,  1996,  3426,  1997,  2010, 14038,\n",
       "           1012,   102])},\n",
       " {'input_ids': tensor([  101,  1996, 11379,  1998,  1044, 22571, 10222,  3370,  1999,  1996,\n",
       "           2434,  2024, 20316,  1010,  1998,  2031,  2025,  2042,  2904,  1012,\n",
       "           1037,  2261,  5793,  5939,  6873, 14773,  2389, 10697,  2031,  2042,\n",
       "          13371,  1010,  2004,  3205,  2012,  1996,  2203,  1997,  1996,  3802,\n",
       "          10288,  2102,  1012,  1065,   102])},\n",
       " {'input_ids': tensor([  101,  1996, 11379,  1998,  1044, 22571, 10222,  3370,  1999,  1996,\n",
       "           2434,  2024, 20316,  1010,  1998,  2031,  2025,  2042,  2904,  1012,\n",
       "           1037,  2261,  5793,  5939,  6873, 14773,  2389, 10697,  2031,  2042,\n",
       "          13371,  1010,  2004,  3205,  2917,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 22784,  1998, 10093, 14545, 16194,  6366,  1996,  2608,  2013,\n",
       "           1996,  2534,  2000,  2019,  3356,  1011,  4574,  1012,  1996,  5394,\n",
       "           2059,  9530, 24396,  2007, 18641,  1010,  2000,  3183,  2002,  3957,\n",
       "           1037, 23577,  7984,  1997,  2010,  7357,  1012,  7327,  2854, 14321,\n",
       "           2050,  1010,  2096, 17573, 22784,  1010,  9418,  2032,  2011,  1037,\n",
       "          11228,  2006,  2010,  6181,  1010,  2021,  2002, 16263,  2014,  4807,\n",
       "           1997,  2008,  5456,  2000, 18641,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 22784,  9418,  2370,  2000,  1996,  6887, 29667,  6305,  7066,\n",
       "           1010,  1998,  4269,  1996,  2381,  1997,  2010,  7357,  1012,  2002,\n",
       "          20735,  2003,  7849,  2271,  1010,  2103,  1997,  1996, 25022,  8663,\n",
       "           7066,  1025,  8480,  2426,  1996,  2843,  7361,  3270,  5856,  1025,\n",
       "           1998,  5728,  2012,  1996,  2455,  1997,  1996, 22330, 28659,  1012,\n",
       "           2002,  2003,  8580,  2011, 26572,  8458, 21382,  1999,  2010,  5430,\n",
       "           1010,  2040, 16475, 22957,  2416,  1997,  2010, 11946,  1025,  2046,\n",
       "           9048, 16280,  2015,  1996,  6071,  2007,  4511,  1010, 28279,  2032,\n",
       "           2096,  2002, 25126,  1010,  1998, 12976,  2013,  2032,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 22784, 14623,  2000,  2632, 21081,  2271,  2010,  8774,  2000,\n",
       "           1996,  1999,  7512, 12032,  4655,  1010,  2010,  3034,  2045,  2007,\n",
       "           1996, 12168, 13310,  7951,  7175,  2010,  2709,  2000, 27939,  1010,\n",
       "           1998,  3957,  2032,  2019,  4070,  1997,  1996,  7348,  1010, 18869,\n",
       "           2015,  1010,  1998,  2500,  3183,  2002,  2387,  2045,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 22784,  2007,  2070,  7669,  1010, 19480, 18641,  1997,  2010,\n",
       "           4767,  1010,  2040,  2012,  3091,  1010,  9462,  2011,  2486,  1997,\n",
       "           3350,  1010,  8267,  2032,  2000,  2014,  2608,  2007,  3665,  1012,\n",
       "           2002, 20432,  2015,  2014,  2007,  1037, 25521,  1997,  2010,  7357,\n",
       "           1010,  1998,  1999,  2010, 21283,  1996,  4054,  2824,  1997,  1996,\n",
       "           5961,  2024, 28667,  9331,  4183,  8898,  1012,  1999,  1996,  2851,\n",
       "           1010, 22784,  1010, 10093, 14545, 16194,  1010,  1996, 28822,  2386,\n",
       "           1998,  1996, 25430,  3170,  1011, 14906, 18280,  2046,  1996,  2406,\n",
       "           1012,   102])},\n",
       " {'input_ids': tensor([  101, 22784,  1010,  4797,  2075,  3251,  2002,  4618,  6033,  2030,\n",
       "           2025,  1996,  2308,  8858,  2040, 10797, 24992,  2094,  2791,  2007,\n",
       "           1996,  4848,  5668,  1010, 10663,  2015,  2012,  3091,  2000,  8622,\n",
       "           2068,  2005,  1996,  2556,  1012,  2002,  5176,  2019, 18168,  2368,\n",
       "           2013, 13035,  1010,  1998,  2008,  2002,  2052,  3946,  2032,  2036,\n",
       "           2000,  2963,  2070, 17678, 25090,  3560,  2616,  2013,  1996,  2970,\n",
       "           1997,  2028,  1999,  1996,  2155,  1012,  2010, 24320,  2024,  2119,\n",
       "           4660,  1012,  7547,  2003,  2081,  2005,  1996,  9831,  1012,  5819,\n",
       "           1996,  4848,  5668,  4133,  2012,  2795,  1010, 14412,  8523, 15488,\n",
       "           7616,  2068,  2007,  1037,  7570, 18752,  2094, 21517,  1012, 14833,\n",
       "          20464, 25219, 10182,  1010, 14158,  1996,  4326,  3896,  1997,  2009,\n",
       "           1010, 17678, 15689,  3111,  2037,  6215,  1010,  1998,  2027,  4315,\n",
       "           5178,  2010, 14951,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 22784,  1010,  2383,  2736,  2010,  7984,  1010,  1998,  2363,\n",
       "           3176,  7534,  2013,  1996,  6887, 29667,  6305,  7066,  1010, 28866,\n",
       "           2015,  1025,  2002,  2003, 21527,  1999,  2010,  3637,  2000, 27939,\n",
       "           1010,  1998,  1999,  2010,  3637,  2003,  5565,  2006,  2008,  2479,\n",
       "           1012,  1996,  2911,  2008,  3344,  2032,  2003,  1999,  2014,  2709,\n",
       "           8590,  2011, 21167,  2000,  1037,  2600,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 22784,  1010, 11828,  2010,  7984,  1010, 14623,  2010,  2709,\n",
       "           2013,  1996, 13178,  2000, 25022, 19170,  1005,  1055,  2479,  1010,\n",
       "           1996, 29361,  2445,  2032,  2011,  2008,  7804,  1010,  2010,  4019,\n",
       "           2013,  1996, 20675,  1010,  1998,  2013,  8040,  8516,  2721,  1998,\n",
       "          25869,  2100,  2497, 10521,  1025,  2010,  5508,  1999, 12071,  1010,\n",
       "           2073,  2010, 11946,  1010,  2383, 19668,  1998,  8828,  1996, 23060,\n",
       "           2368,  1997,  1996,  3103,  1010,  2024,  9707,  2911, 13088, 11012,\n",
       "           2098,  1998,  2439,  1025,  1998, 14730,  1996,  2878,  2007,  2019,\n",
       "           4070,  1997,  2010,  5508,  1010,  2894,  1010,  2006,  1996, 15429,\n",
       "           1997,  2010,  6258,  1010,  2012,  1996,  2479,  1997, 10250, 22571,\n",
       "           6499,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 22784,  1010,  2007,  2070,  2210,  5375,  2013, 10093, 14545,\n",
       "          16194,  1010,  7327,  2213, 29667,  2271,  1998,  6316, 29674, 22638,\n",
       "           1010, 22889, 22916,  2035,  1996,  4848,  5668,  1010,  1998,  4376,\n",
       "           1997,  1996,  2931,  8858,  2040,  2018,  3039,  3209,  2019, 25049,\n",
       "          23198,  2007,  2068,  1010,  2024, 17818,  1012, 11463,  4630,  4048,\n",
       "           2271,  2036,  2003, 14248,  2007, 13736, 14163, 26065,  3508,  1012,\n",
       "            102])},\n",
       " {'input_ids': tensor([  101, 23923,  1012,  8418,  2595,  1012,  1048,  1012,  6390,  2475,\n",
       "           1006,  7570,  2213,  1012,  8418,  2595,  1012,  1048,  1012,  5401,\n",
       "           2509,  1007,  1012,  1011,  1011,  1996,  2773,  1170, 29723, 29727,\n",
       "          29723, 29726, 29723, 14608, 19579,  1010,  2005,  2029, 11190,  4842,\n",
       "           3957,  2004,  1037, 11498,  8458, 23797,  1000, 19547,  1010, 25413,\n",
       "           2007,  1037,  3614,  1010,  1000,  6974,  2965, 19589,  1010,  1998,\n",
       "          11276,  2061,  2000,  2022,  5421,  2182,  1012,  2005,  2144, 11190,\n",
       "           4842,  1005,  1055,  2154,  2019, 12946,  1011,  2132,  1997,  1996,\n",
       "           2026, 27524, 29667,  2319,  2558,  2038,  2042,  3603,  2007,  1996,\n",
       "           6085, 16276,  2061,  2004,  2000,  2433,  1037,  4920,  2083,  2029,\n",
       "           2019,  8612,  2071,  3413,  1012,  1006,  2156, 24529, 21723,  3022,\n",
       "           1998, 24951,  4779,  1010,  1996,  2026, 27524, 29667,  2319,  2287,\n",
       "           1012,  1007, 19589,  1997,  2023,  2828,  2020,   102])},\n",
       " {'input_ids': tensor([  101,  2002, 11323, 11178,  1996,  6827,  3785,  1997,  1996,  8658,\n",
       "           3818,  1024,  1996, 19589,  2442,  2031,  2042,  2275,  2039,  1010,\n",
       "           2028,  2369,  1996,  2060,  1010,  1999,  1996,  2126,  2002,  4081,\n",
       "           2005,  2010,  3614,  2098,  7533,  1012,   102])},\n",
       " {'input_ids': tensor([  101, 23923,  1012, 22038,  6137,  1012,  1048,  1012, 16621,  1011,\n",
       "          17832,  1006,  7570,  2213,  1012, 22038,  6137,  1012,  1048,  1012,\n",
       "          14010,  1011, 16065,  1007,  1012,  1011,  1011,  2129, 11463,  4630,\n",
       "           4048,  2271,  2288,  2041,  1997,  1996,  2534,  3464,  1037, 11989,\n",
       "           1012, 11190,  4842, 15980,  1037,  2117, 13082,  2078,  1010,  2021,\n",
       "           2045,  2003,  2053,  3350,  2005,  2023,  1010,  1998,  1048,  1012,\n",
       "          16621, 21461,  1012,  1006,  1048,  1012, 14010, 21461,  1012,  1999,\n",
       "           1996,  3306,  1007,  6592,  2738,  6118,  2008,  2045,  2001,  2069,\n",
       "           2028,  1012,  6854,  1010,  1996, 10232,  2773,  1171, 29739, 29721,\n",
       "          29723, 19579,  2029,  5158,  1999,  1996,  2240,  7851, 11463,  4630,\n",
       "           4048,  2271,  1005,  6164,  2003,  2025,  2179,  6974,  1012,  1000,\n",
       "           2002,  2253,  2039,  1010,  1000,  1996,  4802,  2758,  1010,  1000,\n",
       "           2083,  1996,  1171, 29739, 29721, 29723, 19579,   102])},\n",
       " {'input_ids': tensor([  101,  1000, 12831,  6083,  2008,  1000,  2002, 13501,  2039,  2000,\n",
       "           1996,  7077, 19990,  2008,  2020, 16276,  1999,  1996,  2813,  1012,\n",
       "           1000,  2500,  6814,  2008,  2045,  2001,  1037, 10535,  2012,  1996,\n",
       "           5110,  2203,  1997,  1996,  2534,  2877,  2000,  1996,  3356,  2466,\n",
       "           1010,  1998,  2006,  2083, 13768,  2000,  1996, 12371,  2100,  1012,\n",
       "            102])},\n",
       " {'input_ids': tensor([  101, 10093, 14545, 16194,  2383, 19596,  2019,  3320,  1997,  1996,\n",
       "          24665,  4402, 14483,  2015,  1010,  7271,  4455,  2006,  1996,  4848,\n",
       "           5668,  2000,  2128,  4115, 15549,  4095,  1996,  2160,  1997, 22784,\n",
       "           1012,  2076,  1996,  9530,  7629, 26620,  1997,  1996,  2473,  2002,\n",
       "           2038,  2172,  2000,  9015,  2013,  1996,  9004,  7068,  5897,  1997,\n",
       "           1996,  4848,  5668,  1010,  2013,  3183,  1010,  2383,  6727,  2068,\n",
       "           1997,  2010,  2640,  2000, 16617,  1037,  8774,  1999,  3246,  2000,\n",
       "           6855,  2739,  1997, 22784,  1010,  2002,  5176,  1037,  2911,  1010,\n",
       "           2007,  2035,  2477,  4072,  2005,  1996,  3800,  1012,  2002,  2003,\n",
       "           4188,  1010,  2021,  2003,  5728, 19851,  2007,  2054,  2002,  4122,\n",
       "           2011, 27383,  1010,  1999,  1996,  2433,  1997, 10779,  1012,  2002,\n",
       "          28866,  2015,  1999,  1996,  3944,  2302,  1996, 26927, 24872,  1997,\n",
       "           2010,  2388,  1010,  1998,  1996,  7804, 17553,   102])},\n",
       " {'input_ids': tensor([  101,  1996, 11693,  6843, 20868,  2271,  8480,  2012,  1996,  4186,\n",
       "           1025,  1037,  4337,  3138,  2173,  2090,  2032,  1998, 22784,  1010,\n",
       "           1999,  2029, 20868,  2271,  2003,  2011,  2028,  6271,  3158, 15549,\n",
       "          14740,  1012, 18641,  3544,  2000,  1996,  4848,  5668,  1010,  1998,\n",
       "           2383,  6966,  2068,  1997,  1996,  7534,  2029,  2016,  2018,  1037,\n",
       "           2157,  2000,  5987,  2013,  2068,  1010,  8267,  1037,  5592,  2013,\n",
       "           2169,  1012,  7327,  2854, 22911,  9825,  1010, 19157,  2011,  1037,\n",
       "           4613,  1997, 22784,  1010, 27655,  2015,  1037,  3329,  1011, 14708,\n",
       "           2012,  2032,  1010,  2029, 21145,  2091,  1996,  2452,  1011, 20905,\n",
       "           1025,  1037,  2236, 10722, 12274,  7096,  2003,  1996,  9509,  1010,\n",
       "           2029,  4247,  1010,  6229,  2011,  1996,  6040,  1997, 10093, 14545,\n",
       "          16194,  1010,  2117,  2098,  2011, 23713, 10606,  5358,  2271,  1010,\n",
       "           1996,  4848,  5668, 11036,  2000,  2037,  7972,   102])},\n",
       " {'input_ids': tensor([  101, 22784,  1010,  1999,  8463,  1997,  2010,  7984,  1010, 14623,\n",
       "           2010,  5508,  2012,  1996,  2479,  1997,  1097,  4747,  2271,  1010,\n",
       "           2010,  6712, 23166,  1010,  1998,  1996, 12511,  6686,  1997,  2010,\n",
       "           2709, 16215,  8939,  2121,  1012,  1996, 11590,  1997,  1996,  7266,\n",
       "          19776,  2229,  2032,  2012,  2197,  2007,  2172,  2004,  4842,  3012,\n",
       "           1012,  2002,  2279,  4136,  1997,  2010,  5508,  2426,  1996,  1048,\n",
       "          29667,  3367,  2854,  7446,  7066,  1010,  2011,  3183,  2010,  2878,\n",
       "           4170,  1010,  2362,  2007,  2037, 10604,  1010,  2024,  3908,  1010,\n",
       "           2010,  2219,  2911,  1998,  3626,  3272,  2098,  1012, 23166,  2002,\n",
       "           2003,  5533,  2000,  1996,  2479,  1997, 25022, 19170,  1012,  2011,\n",
       "           2014,  1996,  2431,  1997,  2010,  2111,  2024,  8590,  2046, 25430,\n",
       "           3170,  1012,  7197,  2011,  8714,  1010,  2002,  9507,  2015,  2014,\n",
       "           4372, 14856, 21181,  2015,  2370,  1010,  1998,   102])},\n",
       " {'input_ids': tensor([  101,  1999,  9509,  1997, 25022, 19170,  1005,  1055,  8128,  1010,\n",
       "           2044,  2383,  2985,  1037,  3143,  2095,  1999,  2014,  4186,  1010,\n",
       "           2002, 20776,  2005,  1037,  8774,  2000,  1996,  1999,  7512, 12032,\n",
       "           4655,  1012,   102])}]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(128, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=128, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TrainingArguments(output_dir='./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainingArguments' object has no attribute 'should_log'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-ef5732dcd317>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_log\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TrainingArguments' object has no attribute 'should_log'"
     ]
    }
   ],
   "source": [
    "a.should_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIANT\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiant.proj.simple import runscript as run\n",
    "import jiant.scripts.download_data.runscript as downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_DIR = \"/path/to/exp\"\n",
    "\n",
    "# Download the Data\n",
    "downloader.download_data([\"mrpc\"], f\"{EXP_DIR}/tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from start\n",
      "  jiant_task_container_config_path: ./test_experiment/model/tasks\\run_configs\\simple_config.json\n",
      "  output_dir: ./test_experiment/model/tasks\\runs\\simple\n",
      "  hf_pretrained_model_name_or_path: ./test_experiment/model\n",
      "  model_path: ./test_experiment/model\n",
      "  model_config_path: ./test_experiment/model/tasks\\models\\bert\\model\\config.json\n",
      "  model_load_mode: partial\n",
      "  do_train: True\n",
      "  do_val: True\n",
      "  do_save: False\n",
      "  do_save_last: False\n",
      "  do_save_best: False\n",
      "  write_val_preds: False\n",
      "  write_test_preds: False\n",
      "  eval_every_steps: 0\n",
      "  save_every_steps: 0\n",
      "  save_checkpoint_every_steps: 0\n",
      "  no_improvements_for_n_evals: 0\n",
      "  keep_checkpoint_when_done: False\n",
      "  force_overwrite: True\n",
      "  seed: 3\n",
      "  learning_rate: 1e-05\n",
      "  adam_epsilon: 1e-08\n",
      "  max_grad_norm: 1.0\n",
      "  optimizer_type: adam\n",
      "  no_cuda: False\n",
      "  fp16: False\n",
      "  fp16_opt_level: O1\n",
      "  local_rank: -1\n",
      "  server_ip: \n",
      "  server_port: \n",
      "device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Using seed: 3\n",
      "{\n",
      "  \"jiant_task_container_config_path\": \"./test_experiment/model/tasks\\\\run_configs\\\\simple_config.json\",\n",
      "  \"output_dir\": \"./test_experiment/model/tasks\\\\runs\\\\simple\",\n",
      "  \"hf_pretrained_model_name_or_path\": \"./test_experiment/model\",\n",
      "  \"model_path\": \"./test_experiment/model\",\n",
      "  \"model_config_path\": \"./test_experiment/model/tasks\\\\models\\\\bert\\\\model\\\\config.json\",\n",
      "  \"model_load_mode\": \"partial\",\n",
      "  \"do_train\": true,\n",
      "  \"do_val\": true,\n",
      "  \"do_save\": false,\n",
      "  \"do_save_last\": false,\n",
      "  \"do_save_best\": false,\n",
      "  \"write_val_preds\": false,\n",
      "  \"write_test_preds\": false,\n",
      "  \"eval_every_steps\": 0,\n",
      "  \"save_every_steps\": 0,\n",
      "  \"save_checkpoint_every_steps\": 0,\n",
      "  \"no_improvements_for_n_evals\": 0,\n",
      "  \"keep_checkpoint_when_done\": false,\n",
      "  \"force_overwrite\": true,\n",
      "  \"seed\": 3,\n",
      "  \"learning_rate\": 1e-05,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"optimizer_type\": \"adam\",\n",
      "  \"no_cuda\": false,\n",
      "  \"fp16\": false,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"local_rank\": -1,\n",
      "  \"server_ip\": \"\",\n",
      "  \"server_port\": \"\"\n",
      "}\n",
      "1\n",
      "Creating Tasks:\n",
      "    mrpc (MrpcTask): /path/to/exp/tasks\\configs\\mrpc_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./test_experiment/model were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./test_experiment/model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './test_experiment/model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-831508dd895e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Run!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_simple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\jiant-2.2.0-py3.8.egg\\jiant\\proj\\simple\\runscript.py\u001b[0m in \u001b[0;36mrun_simple\u001b[1;34m(args, with_continue)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m     \u001b[0mrunscript\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m     \u001b[0mpy_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_output_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"simple_run_config.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\jiant-2.2.0-py3.8.egg\\jiant\\proj\\main\\runscript.py\u001b[0m in \u001b[0;36mrun_loop\u001b[1;34m(args, checkpoint)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mjiant_task_container_config_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjiant_task_container_config_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         )\n\u001b[1;32m--> 140\u001b[1;33m         runner = setup_runner(\n\u001b[0m\u001b[0;32m    141\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mjiant_task_container\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjiant_task_container\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\jiant-2.2.0-py3.8.egg\\jiant\\proj\\main\\runscript.py\u001b[0m in \u001b[0;36msetup_runner\u001b[1;34m(args, jiant_task_container, quick_init_out, verbose)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mtaskmodels_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjiant_task_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtaskmodels_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         )\n\u001b[1;32m---> 92\u001b[1;33m         jiant_model_setup.delegate_load_from_path(\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[0mjiant_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_load_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\jiant-2.2.0-py3.8.egg\\jiant\\proj\\main\\modeling\\model_setup.py\u001b[0m in \u001b[0;36mdelegate_load_from_path\u001b[1;34m(jiant_model, weights_path, load_mode)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mweights_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdelegate_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './test_experiment/model'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set up the arguments for the Simple API\n",
    "args = run.RunConfiguration(\n",
    "    run_name=\"simple\",\n",
    "    exp_dir='./test_experiment/model/tasks',\n",
    "    data_dir=f\"{EXP_DIR}/tasks\",\n",
    "    hf_pretrained_model_name_or_path=\"./test_experiment/model\",\n",
    "    model_weights_path=\"./test_experiment/model\",\n",
    "    tasks=\"mrpc\",\n",
    "    train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    force_overwrite=True,\n",
    "    seed = 3\n",
    "#    seed=3\n",
    ")\n",
    "\n",
    "# Run!\n",
    "run.run_simple(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "high is out of bounds for int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-0342df44e5e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m \u001b[1;36m32\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int32\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: high is out of bounds for int32"
     ]
    }
   ],
   "source": [
    "np.random.randint(0, 2** 32 -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.20.1'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiant.proj.main.export_model as export_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "export_model.export_model(\n",
    "    hf_pretrained_model_name_or_path=\"bert-base-uncased\",\n",
    "    output_base_path=\"./jiant/bert-base-uncased\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir './logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array([[101, 1996, 2622, 9535, 11029, 26885, 1997], \n",
    "         [101,2198, 9535, 11029, 1010, 2011, 8965, 3854, 22033, 9050, 3064, 102],\n",
    "         [101, 2102, 2023, 26885, 2003, 2005, 1996, 2224, 1997, 3087, 5973, 2012, 102]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset import StrategizedTokenizerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**custom_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a += ['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLUE and SentEval benchmarking\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"cola\"\n",
    "model_checkpoint = \"test_experiment/model2/\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\s145733\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'label': 1,\n",
       " 'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\"}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1824</td>\n",
       "      <td>unacceptable</td>\n",
       "      <td>I acknowledged that my father, he was tight as an owl.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>409</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>For him to do that would be a mistake.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4506</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>Mary sang a song, but Lee never did.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4012</td>\n",
       "      <td>unacceptable</td>\n",
       "      <td>John made Mary cooking Korean food.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3657</td>\n",
       "      <td>unacceptable</td>\n",
       "      <td>John sounded in the park.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2286</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>Clouds cleared from the sky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1679</td>\n",
       "      <td>unacceptable</td>\n",
       "      <td>It is this hat that that he was wearing is certain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1424</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>Who are you gawking at?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6912</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>Captain Oates died in order to save his comrades.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>520</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>The tree dropped fruit to the ground.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matthews_correlation': 0.14462158210542375}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_metrics = {\n",
    "    \"cola\": 'matthews_correlation',\n",
    "    \"mnli\": \"accuracy\",\n",
    "    \"mnli-mm\": \"accuracy\",\n",
    "    \"mrpc\": ['accuracy', 'f1'],\n",
    "    \"qnli\": \"accuracy\",\n",
    "    \"qqp\": ['accuracy', 'f1'],\n",
    "    \"rte\": 'accuracy',\n",
    "    \"sst2\": 'accuracy',\n",
    "    \"stsb\": ['pearson', 'spearmanr'],\n",
    "    \"wnli\": 'accuracy',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Our friends won't buy this analysis, let alone the next one we propose.\n"
     ]
    }
   ],
   "source": [
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47f2b263d9d4fd59de13e7b4e58cccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623d50f1b440460390dc9cfec73b0110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c3122615824705870f79d13c514bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at test_experiment/model2/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at test_experiment/model2/ and are newly initialized: ['classifier.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = os.path.join(model_checkpoint, 'cola'),\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    max_steps=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:34, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.6951454480489095, metrics={'train_runtime': 55.7825, 'train_samples_per_second': 0.054, 'total_flos': 22483142784.0, 'epoch': 0.01, 'init_mem_cpu_alloc_delta': -21917696, 'init_mem_gpu_alloc_delta': 17349632, 'init_mem_cpu_peaked_delta': 21946368, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': -79511552, 'train_mem_gpu_alloc_delta': 52054528, 'train_mem_cpu_peaked_delta': 79540224, 'train_mem_gpu_peaked_delta': 15810048})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6929337382316589,\n",
       " 'eval_matthews_correlation': 0.053976421365281024,\n",
       " 'eval_runtime': 507.7157,\n",
       " 'eval_samples_per_second': 2.054,\n",
       " 'epoch': 0.01,\n",
       " 'eval_mem_cpu_alloc_delta': -3543040,\n",
       " 'eval_mem_gpu_alloc_delta': 0,\n",
       " 'eval_mem_cpu_peaked_delta': 3612672,\n",
       " 'eval_mem_gpu_peaked_delta': 3193344}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(model_checkpoint, 'cola'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter search\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at test_experiment/model2/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at test_experiment/model2/ and are newly initialized: ['classifier.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.hyperparameter_search(n_trials=1, direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9993847012519836},\n",
       " {'label': 'POSITIVE', 'score': 0.9993847012519836},\n",
       " {'label': 'POSITIVE', 'score': 0.9993847012519836}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:15:04.979391\n",
      "16:15:31.633785\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().strftime(\"%H:%M:%S.%f\"))\n",
    "predictions = nlp(text_list)\n",
    "print(datetime.now().strftime(\"%H:%M:%S.%f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
