{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing code\n",
    "from cleaner_utils import super_cleaner\n",
    "from preprocessing_utils import whole_word_MO_tokenization_and_masking\n",
    "from pretraining_data_utils import book_properties, make_df_book_properties\n",
    "\n",
    "from gutenberg.acquire import load_etext\n",
    "\n",
    "#Training code\n",
    "from transformers import BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read scraped metadata from the gutenberg metadata database \n",
    "#(Original data was scraped by using https://github.com/c-w/gutenberg)\n",
    "#The data is then further preprocessed by https://github.com/hugovk/gutenberg-metadata so it is actually usable.\n",
    "\n",
    "f = open('gutenberg-metadata.json', 'r')\n",
    "metadata = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13142"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve how many english books there are that we can use\n",
    "english_book_keys = [key for key in metadata.keys() if metadata[key]['language'] == ['en']]\n",
    "len(english_book_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Randomly select 20 books that we can query\n",
    "np.random.seed(42)\n",
    "rand_20_books = [x for x in np.random.choice(english_book_keys, size=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16968 ['Browne, Porter Emerson', 'Towne, Charles Hanson'] ['The Bad Man: A Novel']\n",
      "1741 ['Packard, Frank L. (Frank Lucius)'] ['The White Moll']\n",
      "14575 ['Cable, George Washington'] ['Bylow Hill']\n",
      "14334 ['Bower, B. M.'] ['The Range Dwellers']\n",
      "22924 ['Douglas, Alan, Captain'] ['Pathfinder; or, The Missing Tenderfoot']\n"
     ]
    }
   ],
   "source": [
    "# Titles and authors for the first 5 books\n",
    "for book_id in rand_20_books[:5]:\n",
    "    print(book_id, metadata[book_id]['author'], metadata[book_id]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-0358ce9648a3>\", line 6, in <module>\n",
      "    super_cleaner(load_etext(14575), -1, verify_deletions=True)\n",
      "  File \"C:\\Users\\s145733\\Anaconda3\\lib\\site-packages\\gutenberg\\acquire\\text.py\", line 152, in load_etext\n",
      "    text = cache.read().decode('utf-8')\n",
      "  File \"C:\\Users\\s145733\\Anaconda3\\lib\\gzip.py\", line 292, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"C:\\Users\\s145733\\Anaconda3\\lib\\gzip.py\", line 470, in read\n",
      "    self._read_eof()\n",
      "  File \"C:\\Users\\s145733\\Anaconda3\\lib\\gzip.py\", line 516, in _read_eof\n",
      "    raise BadGzipFile(\"CRC check failed %s != %s\" % (hex(crc32),\n",
      "gzip.BadGzipFile: CRC check failed 0x0 != 0xd0c5998f\n"
     ]
    }
   ],
   "source": [
    "# The third book cant be retrieved because of faults in retrieval. This happens sometimes.\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    super_cleaner(load_etext(14575), -1, verify_deletions=True)\n",
    "except Exception as e:\n",
    "    try:\n",
    "        exc_info = sys.exc_info()\n",
    "    finally:\n",
    "        # Display the *original* exception\n",
    "        traceback.print_exception(*exc_info)\n",
    "        del exc_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of John Gutenberg, by Franz von Dingelstedt\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org/license\\r\\n\\r\\n\\r\\nTitle: John Gutenberg\\r\\n       First Master Printer, His Acts and Most Remarkable\\r\\n       Discourses and his Death\\r\\n\\r\\nAuthor: Franz von Dingelstedt\\r\\n\\r\\nRelease Da'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original unprocessed text\n",
    "text = load_etext(50000)[:500]\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of John Gutenberg, by Franz von Dingelstedt\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.org/license\r\n",
      "\r\n",
      "\r\n",
      "Title: John Gutenberg\r\n",
      "       First Master Printer, His Acts and Most Remarkable\r\n",
      "       Discourses and his Death\r\n",
      "\r\n",
      "Author: Franz von Dingelstedt\r\n",
      "\r\n",
      "Release Da\n"
     ]
    }
   ],
   "source": [
    "#Text with formatting\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the cleaner to retrieve cleaned text from the first book of the random selection\n",
    "sentences = super_cleaner(load_etext(16968), -1, verify_deletions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Looking back now, after so many months of struggle and foreboding, he wondered how he had ever had the high courage to come to this strange country. Had he been a few years older he would not have started forth--he was sure of that now. But the flame of youth was in him, the sure sense that he could conquer where others had miserably failed; and, like all virile young Americans, he had love of adventure, and zest for the unknown was in his blood. The glamour of Arizona lured him; the color of these great hills and mountains he had come to love captivated him from the first. It was as if a siren beckoned, and he had to follow.',\n",
       " 'For days he had been worried almost to the breaking point. Things had not shaped themselves as he had planned. Event piled upon event, and now disaster--definite disaster--threatened to descend upon him.',\n",
       " 'All morning, despite the intense heat, he had been about the ranch, appraising this and that, mentally; pottering in the shed; looking at his horses--the few that were left!--smiling at the thought of his wheezing Ford, wondering just when he would clear out altogether.',\n",
       " \"Not that young Gilbert Jones was a pessimist. And yet he wasn't one of those damnable Pollyanna optimists he so abominated--the kind who went about saying continually that God was in His heaven and all was right with the world. No, indeed! He was just a normal, regular fellow, ready to face a difficult situation when it came about as the natural result of a series of events. He saw the impending catastrophe as the logical finale of many happenings--for some of which he was not in any way responsible.\",\n",
       " 'Who could have foreseen the Great War, for instance? Surely that was not his fault! A pitiful archduke was murdered in a European city. He remembered reading about it, and then instantly dismissing it from his mind as of no consequence. He never connected himself with so remote an event. Yet a few years later he, with many others, was fighting in France--a lieutenant in the United States Army--just because a shot had been fired at a man he had never heard of!',\n",
       " 'A strange world, he pondered, as he looked out over the blue hills, heavy with heat, and meandering away to God knows where.',\n",
       " 'Then, surely it was no fault of his if the Government under which he lived made no strenuous effort to stop the Mexican massacres of American citizens all along the border. One firm word, one splendid gesture, and daring raids would have ceased; and there would have been no menace of bandits hereabouts. It would have been a country fit to live in. There would have developed a feeling of permanence and peace, and a young chap could have made his plans for the future with some sense of security and high optimism. Surely they were entitled to protection--these brave boys and stalwart sons of America who fearlessly took up claims, staked all, and strove to make homes in this thrilling section along the borderland. They were not mere adventurers; they were pioneers. They were of the best stuff that America contained--clean-cut, clear-eyed, with level heads and high hearts. Yet their own Government did not think enough of them to offer them the sure protection they were entitled to.',\n",
       " 'Gilbert looked back on that distant day when he had gone up to Bisbee and purchased four head of cattle, and brought them himself to this ranch he had purchased, happy as only a fool is happy. Within a week they had mysteriously disappeared.',\n",
       " 'Rumors of Mexican thieves and assassins had come to him, as they had come to all the young land-owners along the line. He recalled how, after one raid, in which a good citizen had been foully murdered in his bed, he had called a meeting of the ranchers in their section, and with one voice they agreed to send a protest to Washington.',\n",
       " 'They did so. Nothing happened. An aching silence followed. They wrote again; and then one day a pale acknowledgment of their communication came in one of those long and important-looking unstamped envelopes. It seemed very official, very impressive. But mere looks never helped any cause. They were not naÃ¯ve enough to expect the Secretary of State to come down in person and see to the mending of things. But a platoon of soldiers--a handful of troops--would have worked wonders. Jones always contended that not a shot would have to be fired; no more deaths on either side would be necessary. The mere presence of a few men in uniform would have the desired effect. The bandits, now prowling about, would slink over the invisible border to their own territory, and never be heard of again. Of that he felt confident.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text is now a list of paragraphs\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"No.\"',\n",
       " '\"No.\"',\n",
       " '\"No.\"',\n",
       " '\"Yes.\"',\n",
       " '\"Why?\"',\n",
       " '\"Yes.\"',\n",
       " '\"Gun?\"',\n",
       " '\"Pells?\"',\n",
       " '\"Really?\"',\n",
       " '\"A what?\"',\n",
       " '\"Joking?\"',\n",
       " '\"I have?\"',\n",
       " '\"I ain\\'t!\"',\n",
       " '\"Kiss me!\"',\n",
       " '\"Uh--huh!\"',\n",
       " '\"Yes; why?\"',\n",
       " 'She nodded.',\n",
       " '\"In a way.\"',\n",
       " '\"What for?\"',\n",
       " '\"Yes, sir!\"']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with some short sentences\n",
    "sorted(sentences, key=len)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2041, 5, 1532, 75308, 353]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find some properties about the book\n",
    "book_properties(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setting to ignore warnings about sequences being longer than BERT can handle\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "np.random.seed(42)\n",
    "rand_100_books = [x for x in np.random.choice(english_book_keys, size=100)]\n",
    "df_books = make_df_book_properties(rand_100_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Shortest sentence (char)</th>\n",
       "      <th>Longest sentence (char)</th>\n",
       "      <th>Total tokens</th>\n",
       "      <th>Longest sequence (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>15920</td>\n",
       "      <td>2790</td>\n",
       "      <td>4</td>\n",
       "      <td>1228</td>\n",
       "      <td>105158</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>16733</td>\n",
       "      <td>2314</td>\n",
       "      <td>5</td>\n",
       "      <td>1848</td>\n",
       "      <td>125803</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>14578</td>\n",
       "      <td>1256</td>\n",
       "      <td>5</td>\n",
       "      <td>2301</td>\n",
       "      <td>119420</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>21268</td>\n",
       "      <td>1971</td>\n",
       "      <td>5</td>\n",
       "      <td>1763</td>\n",
       "      <td>78698</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2166</td>\n",
       "      <td>1517</td>\n",
       "      <td>5</td>\n",
       "      <td>2974</td>\n",
       "      <td>104315</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>12298</td>\n",
       "      <td>44</td>\n",
       "      <td>320</td>\n",
       "      <td>2303</td>\n",
       "      <td>9823</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14575</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10188</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>20039</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>22950</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id num_sentences Shortest sentence (char) Longest sentence (char)  \\\n",
       "98   15920          2790                        4                    1228   \n",
       "65   16733          2314                        5                    1848   \n",
       "51   14578          1256                        5                    2301   \n",
       "54   21268          1971                        5                    1763   \n",
       "44    2166          1517                        5                    2974   \n",
       "..     ...           ...                      ...                     ...   \n",
       "47   12298            44                      320                    2303   \n",
       "2    14575        Failed                   Failed                  Failed   \n",
       "15   10188        Failed                   Failed                  Failed   \n",
       "62   20039        Failed                   Failed                  Failed   \n",
       "94   22950        Failed                   Failed                  Failed   \n",
       "\n",
       "   Total tokens Longest sequence (tokens)  \n",
       "98       105158                       282  \n",
       "65       125803                       460  \n",
       "51       119420                       565  \n",
       "54        78698                       374  \n",
       "44       104315                       670  \n",
       "..          ...                       ...  \n",
       "47         9823                       526  \n",
       "2        Failed                    Failed  \n",
       "15       Failed                    Failed  \n",
       "62       Failed                    Failed  \n",
       "94       Failed                    Failed  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sort df and account for the fact that the column has both text and numbers\n",
    "df_books.loc[pd.to_numeric(df_books['Shortest sentence (char)'], errors='coerce').sort_values().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"No.\"',\n",
       " '\"No.\"',\n",
       " '\"Ay.\"',\n",
       " '\"No.\"',\n",
       " '\"Go!\"',\n",
       " '\"Wot?\"',\n",
       " '\"Yep.\"',\n",
       " '\"Nor--\"',\n",
       " '\"Sure?\"',\n",
       " '\"Sure.\"',\n",
       " '\"What?\"',\n",
       " '\"Time!\"',\n",
       " '\"I had.\"',\n",
       " '\"Where?\"',\n",
       " ' Contents:',\n",
       " '\"And now?\"',\n",
       " '\"No, suh.\"',\n",
       " '\"And you?\"',\n",
       " '\"One inch.\"',\n",
       " '\"Think so?\"',\n",
       " '\"\\'Ow close?\"',\n",
       " 'Bill nodded.',\n",
       " '\"But now! now!\"',\n",
       " '\"I know, but--\"',\n",
       " '\"Listen, Joy--\"',\n",
       " '\"And if I win?\"',\n",
       " '\"Good society?\"',\n",
       " '\"The gold-dust.\"',\n",
       " '\"Hang who?  Me?\"',\n",
       " '\"Streak of fat?\"']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookoi = 1655\n",
    "sorted(super_cleaner(load_etext(bookoi), -1, verify_deletions=False), key=len)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "has_plus_sign = 0\n",
    "no_plus_sign = 0\n",
    "for i, book_id in enumerate(english_book_keys):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    try:\n",
    "        text = load_etext(int(book_id))\n",
    "        if '+' in text:\n",
    "            has_plus_sign += 1\n",
    "        else:\n",
    "            no_plus_sign += 1\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "print('{} with plus sign'.format(has_plus_sign))\n",
    "print('{} without plus sign'.format(no_plus_sign))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: 11:46:24.958870\n"
     ]
    }
   ],
   "source": [
    "inputs_sent0 = whole_word_MO_tokenization_and_masking(tokenizer, nlp, sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: 11:46:25.067581\n"
     ]
    }
   ],
   "source": [
    "inputs_sent1 = whole_word_MO_tokenization_and_masking(tokenizer, nlp, sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "        [ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "        [ 101, 2559,  103,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "        [ 101, 2298, 2067,  ...,    0,    0,    0],\n",
       "        [ 101, 2559, 2067,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "        [ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "        [ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "        [ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "        [ 101, 2559, 2067,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_sent0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "         [ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "         [ 101, 2559,  103,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 2005, 2420,  ...,    0,    0,    0],\n",
       "         [ 101, 2005, 2154,  ...,    0,    0,    0],\n",
       "         [ 101, 2005, 2420,  ...,    0,    0,    0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "         [ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "         [ 101, 2559, 2067,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 2005, 2420,  ...,    0,    0,    0],\n",
       "         [ 101, 2005, 2420,  ...,    0,    0,    0],\n",
       "         [ 101, 2005, 2420,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Future work: combine sentences for more efficient batchings\n",
    "concat_inputs = {k: torch.cat((v, inputs_sent1[k]), 0) for k,v in inputs_sent0.items()}\n",
    "concat_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MODataset at 0x19515c4cfd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = {key: val for key, val in encodings.items() if key != 'labels'}\n",
    "        self.labels = encodings['labels']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MODataset(concat_inputs)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny_config = {\"hidden_size\": 128, \n",
    "                    \"hidden_act\": \"gelu\", \n",
    "                    \"initializer_range\": 0.02, \n",
    "                    \"vocab_size\": 30522, \n",
    "                    \"hidden_dropout_prob\": 0.1, \n",
    "                    \"num_attention_heads\": 2, \n",
    "                    \"type_vocab_size\": 2, \n",
    "                    \"max_position_embeddings\": 512, \n",
    "                    \"num_hidden_layers\": 2, \n",
    "                    \"intermediate_size\": 512, \n",
    "                    \"attention_probs_dropout_prob\": 0.1}\n",
    "\n",
    "model = BertForMaskedLM(config=BertConfig(**bert_tiny_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    #per_device_eval_batch_size=256,   # batch size for evaluation\n",
    "    learning_rate=1e-5,     \n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataloader,         # training dataset\n",
    "    eval_dataset=None            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bada4b61657f4d51b8a29eb438cece0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce214c1acccb4725bd34f7b90e5ffda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=29.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model_path, trial)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m             \u001b[0mepoch_pbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Iteration\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
