{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MODataset' from 'dataset.dataset' (C:\\Users\\s145733\\Google Drive\\Data science in Engineering\\Jaar 2\\Q3\\Thesis\\LessIsMore\\dataset\\dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-74eb95339c21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgutenberg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_etext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStrategizedTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMODataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#Training code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MODataset' from 'dataset.dataset' (C:\\Users\\s145733\\Google Drive\\Data science in Engineering\\Jaar 2\\Q3\\Thesis\\LessIsMore\\dataset\\dataset.py)"
     ]
    }
   ],
   "source": [
    "#Custom code in this library\n",
    "from cleaner_utils import super_cleaner\n",
    "from pretraining_data_utils import make_book_token_frequency, token_freq_df_to_dict, \\\n",
    "                                    all_available_tokens_from_df, optimize_book_subset, optimize_book_subset_ratio\n",
    "from pretraining_data_utils import book_properties, make_df_book_properties\n",
    "from gutenberg.acquire import load_etext\n",
    "\n",
    "\n",
    "from tokenizer.tokenizer import StrategizedTokenizer\n",
    "from dataset.dataset import StrategizedTokenizerDataset\n",
    "from dataset.dataset import DefaultTokenizerDataset\n",
    "\n",
    "#Training code\n",
    "from transformers import BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from more_itertools import take\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cached_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read scraped metadata from the gutenberg metadata database \n",
    "#(Original data was scraped by using https://github.com/c-w/gutenberg)\n",
    "#The data is then further preprocessed by https://github.com/hugovk/gutenberg-metadata so it is actually usable.\n",
    "\n",
    "f = open(cache_dir + 'gutenberg-metadata.json', 'r')\n",
    "metadata = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve how many english books there in english\n",
    "english_book_keys = [key for key in metadata.keys() if metadata[key]['language'] == ['en']]\n",
    "len(english_book_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third book cant be retrieved because of faults in retrieval. This happens sometimes.\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    super_cleaner(load_etext(14575), -1, verify_deletions=True)\n",
    "except Exception as e:\n",
    "    try:\n",
    "        exc_info = sys.exc_info()\n",
    "    finally:\n",
    "        # Display the *original* exception\n",
    "        traceback.print_exception(*exc_info)\n",
    "        del exc_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#retrieve how many english books there are actually loadable\n",
    "#If books arent cached this may take a while because it needs to scrape the books from gutenberg.org\n",
    "#Therefore i provide a pre-processed file\n",
    "if os.path.isfile(cache_dir + 'loadable_english_book_keys.pkl'):\n",
    "    with open(cache_dir + 'loadable_english_book_keys.pkl', 'rb') as f:\n",
    "        loadable_english_book_keys = pickle.load(f)              \n",
    "else:\n",
    "    loadable_english_book_keys = []\n",
    "    i = 0\n",
    "    for key in english_book_keys:\n",
    "        if i % 1000 == 0:\n",
    "            print(i, datetime.now())\n",
    "        i += 1\n",
    "        try:\n",
    "            load_etext(int(key))\n",
    "            loadable_english_book_keys.append(key)\n",
    "        except:\n",
    "            continue\n",
    "    with open(cache_dir + 'loadable_english_book_keys.pkl', 'wb') as f:\n",
    "        pickle.dump(loadable_english_book_keys, f)\n",
    "            \n",
    "len(loadable_english_book_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly select 20 books that we can query\n",
    "np.random.seed(42)\n",
    "rand_10_books = [x for x in np.random.choice(loadable_english_book_keys, size=10)]\n",
    "print(rand_10_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titles and authors for the first 5 books\n",
    "# 1 book isnt actually loadable, see below.\n",
    "for book_id in rand_10_books[:5]:\n",
    "    print(book_id, metadata[book_id]['author'], metadata[book_id]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing\n",
    "==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original unprocessed text\n",
    "text = load_etext(50000)[:500]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text with formatting\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cleaner to retrieve cleaned text from the first book of the random selection.\n",
    "The _super_cleaner_ strips a headers/disclaimers/tables that are not required for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sentences = super_cleaner(load_etext(16968), -1, verify_deletions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Text is now a list of paragraphs\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#with some short sentences\n",
    "sorted(sentences, key=len)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find some properties about the book\n",
    "book_properties(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization in practice\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initialize custom tokenizer\n",
    "ST_tokenizer = StrategizedTokenizer(padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ST_tokenizer.tokenize(\"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masks are at different places\n",
    "for masked_line in ST_tokenizer.convert_ids_to_tokens(inputs['input_ids']):\n",
    "    print(masked_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gutenberg book-selection\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setting to ignore warnings about sequences being longer than BERT can handle\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "df_books_10 = make_df_book_properties(rand_10_books)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort df and account for the fact that the column has both text and numbers\n",
    "df_books_10.sort_values(by='Shortest sentence (char)')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some books have very few tokens.\n",
    "df_books_10.sort_values(by='Total tokens').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books_10.sort_values(by='Total tokens').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Retrieve token occurences per book in a dataframe and another dataframe with total number of tokens\n",
    "print(datetime.now())\n",
    "df_book_token_freq_10, df_10_total_tokens = make_book_token_frequency(rand_10_books)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DataFrame is obviously very sparse\n",
    "df_book_token_freq_10[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#total number of tokens per book\n",
    "df_10_total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of tokens in our small set\n",
    "df_10_total_tokens.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All tokens which are present in our subsample of 20 books\n",
    "all_present_tokens_10 = all_available_tokens_from_df(df_book_token_freq_10)\n",
    "all_present_tokens_10, len(all_present_tokens_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show first 3 entries\n",
    "tokens_per_book_10 = token_freq_df_to_dict(df_book_token_freq_10, df_10_total_tokens)\n",
    "take(3, tokens_per_book_10.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_book_subset(all_present_tokens_10, tokens_per_book_10, threshold = 1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimize_book_subset_ratio(all_present_tokens_10, tokens_per_book_10, threshold = 1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[PAD]</th>\n",
       "      <th>[unused0]</th>\n",
       "      <th>[unused1]</th>\n",
       "      <th>[unused2]</th>\n",
       "      <th>[unused3]</th>\n",
       "      <th>[unused4]</th>\n",
       "      <th>[unused5]</th>\n",
       "      <th>[unused6]</th>\n",
       "      <th>[unused7]</th>\n",
       "      <th>[unused8]</th>\n",
       "      <th>...</th>\n",
       "      <th>##ÔºÅ</th>\n",
       "      <th>##Ôºà</th>\n",
       "      <th>##Ôºâ</th>\n",
       "      <th>##Ôºå</th>\n",
       "      <th>##Ôºç</th>\n",
       "      <th>##Ôºé</th>\n",
       "      <th>##Ôºè</th>\n",
       "      <th>##Ôºö</th>\n",
       "      <th>##Ôºü</th>\n",
       "      <th>##ÔΩû</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 30522 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      [PAD]  [unused0]  [unused1]  [unused2]  [unused3]  [unused4]  [unused5]  \\\n",
       "1         0          0          0          0          0          0          0   \n",
       "10        0          0          0          0          0          0          0   \n",
       "11        0          0          0          0          0          0          0   \n",
       "12        0          0          0          0          0          0          0   \n",
       "13        0          0          0          0          0          0          0   \n",
       "...     ...        ...        ...        ...        ...        ...        ...   \n",
       "1880      0          0          0          0          0          0          0   \n",
       "1881      0          0          0          0          0          0          0   \n",
       "1882      0          0          0          0          0          0          0   \n",
       "1883      0          0          0          0          0          0          0   \n",
       "1884      0          0          0          0          0          0          0   \n",
       "\n",
       "      [unused6]  [unused7]  [unused8]  ...  ##ÔºÅ  ##Ôºà  ##Ôºâ  ##Ôºå  ##Ôºç  ##Ôºé  ##Ôºè  \\\n",
       "1             0          0          0  ...    0    0    0    0    0    0    0   \n",
       "10            0          0          0  ...    0    0    0    0    0    0    0   \n",
       "11            0          0          0  ...    0    0    0    0    0    0    0   \n",
       "12            0          0          0  ...    0    0    0    0    0    0    0   \n",
       "13            0          0          0  ...    0    0    0    0    0    0    0   \n",
       "...         ...        ...        ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1880          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "1881          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "1882          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "1883          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "1884          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "\n",
       "      ##Ôºö  ##Ôºü  ##ÔΩû  \n",
       "1       0    0    0  \n",
       "10      0    0    0  \n",
       "11      0    0    0  \n",
       "12      0    0    0  \n",
       "13      0    0    0  \n",
       "...   ...  ...  ...  \n",
       "1880    0    0    0  \n",
       "1881    0    0    0  \n",
       "1882    0    0    0  \n",
       "1883    0    0    0  \n",
       "1884    0    0    0  \n",
       "\n",
       "[1000 rows x 30522 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_book_token_freq = pd.read_csv(cache_dir + 'df_book_token_freq.csv', index_col=0)\n",
    "df_book_token_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         7642.0\n",
       "10      901309.0\n",
       "11       36117.0\n",
       "12       40833.0\n",
       "13        6778.0\n",
       "          ...   \n",
       "1880    227995.0\n",
       "1881     98655.0\n",
       "1882     65838.0\n",
       "1883    100735.0\n",
       "1884     15394.0\n",
       "Name: 0, Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total_tokens = pd.read_csv(cache_dir +'df_total_tokens.csv', index_col=0).squeeze()\n",
    "df_total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_book = token_freq_df_to_dict(df_book_token_freq, df_total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 239,  117,  156,  232,  127,  129,  212,  114,  116, 1525, 1016,\n",
       "             115, 1546, 1593, 1065, 1544,  104, 1545, 1341,  235],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total_tokens.sort_values()[:20].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset_booklist ['1061', '1034', '114']\n",
      "subset_total_tokens 99978.0\n",
      "subset_present_tokens [  101.   102.   999. ... 29604. 29606. 29611.]\n",
      "subset_unique_tokens 9320\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_100K.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_100K.pkl', 'rb') as f:\n",
    "        subset_100K = pickle.load(f)\n",
    "        \n",
    "for k,v in subset_100K.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset_booklist ['117', '127', '1525', '116', '1546', '1321', '232', '104', '1567', '235', '1016', '1137', '1359', '1064', '1593', '1757', '1336', '1475', '1861', '115', '1331', '1171', '212', '114']\n",
      "subset_total_tokens 99181.0\n",
      "subset_present_tokens [  101.   102.   999. ... 29731. 29735. 29739.]\n",
      "subset_unique_tokens 10892\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_100K.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_100K.pkl', 'rb') as f:\n",
    "        subset_ratio_100K = pickle.load(f)\n",
    "for k,v in subset_ratio_100K.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset_booklist ['22', '1662', '1558', '1034', '235']\n",
      "subset_total_tokens 999936.0\n",
      "subset_present_tokens [  101.   102.   999. ... 29604. 29607. 29667.]\n",
      "subset_unique_tokens 20841\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_1M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_1M.pkl', 'rb') as f:\n",
    "        subset_1M = pickle.load(f)\n",
    "for k,v in subset_1M.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset_booklist ['117', '127', '1525', '116', '1546', '1321', '232', '104', '1567', '235', '1016', '1137', '1359', '1064', '1593', '1757', '1336', '1475', '1861', '115', '1331', '1042', '180', '1034', '1870', '1092', '1462', '1663', '1222', '1219', '1031', '101', '212', '1566', '112', '1746', '1221', '1817', '1855', '162', '114', '1179', '1063', '1805', '1615', '1645', '1280', '118', '1215', '1753', '1080', '1189']\n",
      "subset_total_tokens 999805.0\n",
      "subset_present_tokens [  100.   101.   102. ... 29736. 29737. 29739.]\n",
      "subset_unique_tokens 21996\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_1M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_1M.pkl', 'rb') as f:\n",
    "        subset_ratio_1M = pickle.load(f)\n",
    "for k,v in subset_ratio_1M.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset_booklist ['200', '22', '1662', '101', '1449', '162', '135', '14', '1365', '1156', '118', '1340', '1444', '100', '1615', '1226', '1725', '1391', '1351', '1694', '1399', '1162', '1320', '1452', '152', '1166', '1218', '1198', '1479', '112', '1304', '180', '1804', '1039', '1210', '1470', '1558', '1060', '235', '116', '114', '115', '1065', '1593']\n",
      "subset_total_tokens 9997943.0\n",
      "subset_present_tokens [  100.   101.   102. ... 29736. 29737. 29739.]\n",
      "subset_unique_tokens 26396\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_10M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_10M.pkl', 'rb') as f:\n",
    "        subset_10M = pickle.load(f)\n",
    "for k,v in subset_10M.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset_booklist ['117', '127', '1525', '116', '1546', '1321', '232', '104', '1567', '235', '1016', '1137', '1359', '1064', '1593', '1757', '1336', '1475', '1861', '115', '1331', '1042', '180', '1034', '1870', '1092', '1462', '1663', '1222', '1219', '1031', '101', '212', '1566', '112', '1746', '1221', '1817', '1855', '162', '114', '1179', '1063', '1805', '1615', '1280', '118', '1324', '1753', '1708', '1317', '1425', '1645', '1479', '1444', '14', '1418', '1351', '1551', '1873', '1270', '1864', '22', '1818', '1199', '1060', '1446', '1166', '152', '1367', '1210', '1065', '1804', '1638', '1149', '1742', '1847', '1451', '1491', '1234', '1187', '1477', '1662', '1808', '1220', '1758', '1054', '1278', '1764', '1253', '1233', '1478', '1694', '1395', '1470', '1761', '1763', '200', '1670', '1186', '132', '1240', '1816', '1669', '1830', '1020', '1387', '240', '1273', '1050', '1206', '1820', '1045', '1156', '1195', '1594', '1391', '1859', '1824', '1827', '1734', '1169', '1731', '1554', '1160', '1165', '1229', '1110', '230', '1520', '1571', '1602', '1599', '1725', '1276', '1841', '1413', '1452', '1289', '1162', '1547', '1409', '1326', '1464', '1246', '1398', '1674', '1574', '1087', '1046', '1689', '1815', '138', '1408', '1416', '1449', '1231', '1335', '1656', '1424', '1172', '1616', '1044', '172', '1875', '1082', '1049', '1733', '1455', '1320', '1086', '1161', '149', '223', '1225', '1383']\n",
      "subset_total_tokens 9993212.0\n",
      "subset_present_tokens [  100.   101.   102. ... 29736. 29737. 29739.]\n",
      "subset_unique_tokens 26631\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_10M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_10M.pkl', 'rb') as f:\n",
    "        subset_ratio_10M = pickle.load(f)\n",
    "for k,v in subset_ratio_10M.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k,v in tokenizer.vocab.items():\n",
    "    if not v in subset_10M['subset_present_tokens'] and not k.startswith('[unused'):\n",
    "        print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer.tokenizer import StrategizedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<tokenizer.tokenizer.StrategizedTokenizer object at 0x000001FDF9CD6B20>'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = StrategizedTokenizer()\n",
    "tok.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<transformers.tokenization_bert.BertTokenizer object at 0x000001FD97F5F070>'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "default_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_tokenizer = StrategizedTokenizer(padding=True)\n",
    "inputs = ST_tokenizer.tokenize(text)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ST_tokenizer.convert_ids_to_tokens(inputs['input_ids']):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MODataset()\n",
    "train_dataset.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny_config = {\"hidden_size\": 128, \n",
    "                    \"hidden_act\": \"gelu\", \n",
    "                    \"initializer_range\": 0.02, \n",
    "                    \"vocab_size\": 30522, \n",
    "                    \"hidden_dropout_prob\": 0.1, \n",
    "                    \"num_attention_heads\": 2, \n",
    "                    \"type_vocab_size\": 2, \n",
    "                    \"max_position_embeddings\": 512, \n",
    "                    \"num_hidden_layers\": 2, \n",
    "                    \"intermediate_size\": 512, \n",
    "                    \"attention_probs_dropout_prob\": 0.1}\n",
    "\n",
    "model = BertForMaskedLM(config=BertConfig(**bert_tiny_config))\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=2,  # batch size per device during training\n",
    "    #per_device_eval_batch_size=256,   # batch size for evaluation\n",
    "    learning_rate=1e-5,     \n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=None            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
