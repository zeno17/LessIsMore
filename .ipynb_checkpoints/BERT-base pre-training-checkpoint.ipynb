{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "# ner_list = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\",\n",
    " 'These facts seemed to me to throw some light on the origin of species--that mystery of mysteries, as it has been called by one of our greatest philosophers.',\n",
    " 'On my return home, it occurred to me, in 1837, that something might perhaps be made out on this question by patiently accumulating and reflecting on all sorts of facts which could possibly have any bearing on it.',\n",
    " \"After five years' work I allowed myself to speculate on the subject, and drew up some short notes; these I enlarged in 1844 into a sketch of the conclusions, which then seemed to me probable: from that period to the present day I have steadily pursued the same object.\",\n",
    " 'I hope that I may be excused for entering on these personal details, as I give them to show that I have not been hasty in coming to a decision.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_MO_tokenization_and_masking(tokenizer, nlp_model, sequence: str):\n",
    "        \"\"\"\n",
    "        posoi: Part-Of-Speech of interest\n",
    "        \n",
    "        Performs whole-word-masking based on selected posoi.\n",
    "        \n",
    "        POS possibilities:['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "                             \n",
    "        TODO: What if no tokens are masked?\n",
    "        \n",
    "        \"\"\"\n",
    "        print('loading:', datetime.now().time())\n",
    "        spacy_sentence = nlp_model(sequence, disable=[\"parser\"])\n",
    "        \n",
    "        POS_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "        NER_list = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', \n",
    "                    'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "        NER_pairs = ['']\n",
    "        \n",
    "        input_ids = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        print(sequence)\n",
    "        print(input_tokens)\n",
    "        sequence_pos_list = [token.pos_ for token in spacy_sentence]\n",
    "        sequence_pos_frequency = {pos: sequence_pos_list.count(pos) for pos in sequence_pos_list}\n",
    "        \n",
    "        modified_input_list = []\n",
    "        \n",
    "        #POS-masking\n",
    "        print('pos-start:', datetime.now().time())\n",
    "        for posoi in sequence_pos_frequency.keys():\n",
    "            posoi_vocab = [token.text.lower() for token in spacy_sentence if token.pos_ == posoi]\n",
    "            \n",
    "            mask_indices = []\n",
    "            composite_word_indices = []\n",
    "            composite_word_tokens = []\n",
    "            for (i, token) in enumerate(input_tokens):\n",
    "                if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                    continue\n",
    "                elif token.startswith(\"##\"):\n",
    "                    composite_word_indices.append(i)\n",
    "                    composite_word_tokens.append(token)\n",
    "                    print(\"\".join([x.strip(\"##\") for x in composite_word_tokens]))\n",
    "                    if \"\".join([x.strip(\"##\") for x in composite_word_tokens]) in posoi_vocab:\n",
    "                        mask_indices = mask_indices + composite_word_indices\n",
    "\n",
    "                elif token in posoi_vocab:\n",
    "                    mask_indices.append(i)\n",
    "                else:\n",
    "                    composite_word_indices = [i]\n",
    "                    composite_word_tokens = [token]\n",
    "\n",
    "            mask_labels = [1 if i in mask_indices else 0 for i in range(len(input_tokens))]\n",
    "            masked_tokens = [x if mask_labels[i] == 0 else 103 for i,x in enumerate(input_ids)]\n",
    "            masked_input = tokenizer.decode(masked_tokens)         \n",
    "            modified_input_list.append(masked_input)\n",
    "\n",
    "        #POS-based lemmatization\n",
    "        replacement_tuples = [(token.text, token.lemma_) for token in spacy_sentence if token.text.lower() != token.lemma_]\n",
    "        #print(replacement_tuples)\n",
    "        pos_replaced_sentence = sequence\n",
    "        for replacement in replacement_tuples:\n",
    "            pos_replaced_sentence = re.sub(r'\\b' + replacement[0] + r'\\b', replacement[1], pos_replaced_sentence)\n",
    "\n",
    "        pos_replaced_sentence = pos_replaced_sentence.replace(\"  \", \" \")\n",
    "        print('Lemma', pos_replaced_sentence)\n",
    "        modified_input_list.append(pos_replaced_sentence)\n",
    "        \n",
    "        #NER-based swapping of time-place (if present)\n",
    "        print('ner-start:', datetime.now().time())\n",
    "        ner_swapped_sentence = spacy_sentence.text\n",
    "        for ent in spacy_sentence.ents:\n",
    "            if ent.label_ == 'TIME':\n",
    "                time_substring = ner_swapped_sentence[ent.start_char:ent.end_char].split(\" \")\n",
    "                time_substring.reverse()\n",
    "                ner_swapped_sentence = ner_swapped_sentence.replace(ner_swapped_sentence[ent.start_char:ent.end_char], \" \".join(time_substring))\n",
    "        print('NER', ner_swapped_sentence)\n",
    "        modified_input_list.append(ner_swapped_sentence)\n",
    "        \n",
    "        \n",
    "        #TODO future ideas\n",
    "        #\n",
    "        #\n",
    "        \n",
    "    \n",
    "        #actually tokenize input\n",
    "        inputs = tokenizer(modified_input_list, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        inputs['labels'] = tokenizer([sequence for i in range(0,inputs['input_ids'].shape[0])], \n",
    "                                     return_attention_mask=False, \n",
    "                                     return_token_type_ids=False,\n",
    "                                     return_tensors='pt', padding=True)['input_ids']\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:49:09.696654\n",
      "loading: 12:49:09.696654\n",
      "Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\n",
      "['anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'me', '.']\n",
      "pos-start: 12:49:09.704615\n",
      "['anne', 'albert', 'heijn']\n",
      "heij\n",
      "heijn\n",
      "PROPN [MASK] went to the [MASK] [MASK] [MASK] [MASK] at 5 o'clock to buy some milk for me.\n",
      "['went', 'buy']\n",
      "heij\n",
      "heijn\n",
      "VERB anne [MASK] to the albert heijn at 5 o'clock to [MASK] some milk for me.\n",
      "['to', 'at', 'for']\n",
      "heij\n",
      "heijn\n",
      "ADP anne went [MASK] the albert heijn [MASK] 5 o'clock [MASK] buy some milk [MASK] me.\n",
      "['the', 'some']\n",
      "heij\n",
      "heijn\n",
      "DET anne went to [MASK] albert heijn at 5 o'clock to buy [MASK] milk for me.\n",
      "['5']\n",
      "heij\n",
      "heijn\n",
      "NUM anne went to the albert heijn at [MASK] o'clock to buy some milk for me.\n",
      "[\"o'clock\", 'milk']\n",
      "heij\n",
      "heijn\n",
      "NOUN anne went to the albert heijn at 5 o'clock to buy some [MASK] for me.\n",
      "['to']\n",
      "heij\n",
      "heijn\n",
      "PART anne went [MASK] the albert heijn at 5 o'clock [MASK] buy some milk for me.\n",
      "['me']\n",
      "heij\n",
      "heijn\n",
      "PRON anne went to the albert heijn at 5 o'clock to buy some milk for [MASK].\n",
      "['.']\n",
      "heij\n",
      "heijn\n",
      "PUNCT anne went to the albert heijn at 5 o'clock to buy some milk for me [MASK]\n",
      "lemma-start: 12:49:09.708602\n",
      "Lemma Anne go to the Albert Heijn at 5 o'clock to buy some milk for I.\n",
      "ner-start: 12:49:09.708602\n",
      "NER Anne went to the Albert Heijn at o'clock 5 to buy some milk for me.\n",
      "12:49:09.715611\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().time())\n",
    "test_sentence = \"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\"\n",
    "example_sentence_inputs = whole_word_MO_tokenization_and_masking(tokenizer=tokenizer, nlp_model=nlp, sequence=test_sentence)\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anne PROPN\n",
      "went VERB\n",
      "to ADP\n",
      "the DET\n",
      "Albert PROPN\n",
      "Heijn PROPN\n",
      "at ADP\n",
      "5 NUM\n",
      "o'clock NOUN\n",
      "to PART\n",
      "buy VERB\n",
      "some DET\n",
      "milk NOUN\n",
      "for ADP\n",
      "me PRON\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "spacy_sentence = nlp(\"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\")\n",
    "\n",
    "for token in spacy_sentence:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,   103,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,   103,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,   103,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,   103,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,   103,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,   103,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "           103,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,   103,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,   103,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "           103,   102],\n",
       "        [  101,  4776,  2175,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  1045,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1051,  1005,  5119,  1019,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,   103,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,   103,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,   103,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,   103,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,   103,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,   103,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "           103,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,   103,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,   103,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "           103,   102],\n",
       "        [  101,  4776,  2175,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  1045,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1051,  1005,  5119,  1019,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= 'On their very first meeting, Gilbert had not been pleasantly impressed with Hardy. But he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of Mexico\\'s casual visitors, and was ready to protect not only his own interests but those of any newcomers. He seemed to have the spirit of fair-mindedness; and he believed firmly in the possibilities of this magic land, particularly for young men. \"It\\'s God\\'s country,\" he told Gilbert on more than one occasion. \"Get into the soil all you can. Dig--and dig deep.\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: 18:18:05.687520\n",
      "On their very first meeting, Gilbert had not been pleasantly impressed with Hardy. But he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of Mexico's casual visitors, and was ready to protect not only his own interests but those of any newcomers. He seemed to have the spirit of fair-mindedness; and he believed firmly in the possibilities of this magic land, particularly for young men. \"It's God's country,\" he told Gilbert on more than one occasion. \"Get into the soil all you can. Dig--and dig deep.\"\n",
      "['on', 'their', 'very', 'first', 'meeting', ',', 'gilbert', 'had', 'not', 'been', 'pleasantly', 'impressed', 'with', 'hardy', '.', 'but', 'he', 'soon', 'saw', 'that', 'the', 'man', 'had', 'a', 'certain', 'rugged', 'strength', ',', 'and', 'there', 'was', 'no', 'doubt', 'he', 'had', 'suffered', 'from', 'the', 'de', '##pre', '##dation', '##s', 'of', 'mexico', \"'\", 's', 'casual', 'visitors', ',', 'and', 'was', 'ready', 'to', 'protect', 'not', 'only', 'his', 'own', 'interests', 'but', 'those', 'of', 'any', 'newcomers', '.', 'he', 'seemed', 'to', 'have', 'the', 'spirit', 'of', 'fair', '-', 'minded', '##ness', ';', 'and', 'he', 'believed', 'firmly', 'in', 'the', 'possibilities', 'of', 'this', 'magic', 'land', ',', 'particularly', 'for', 'young', 'men', '.', '\"', 'it', \"'\", 's', 'god', \"'\", 's', 'country', ',', '\"', 'he', 'told', 'gilbert', 'on', 'more', 'than', 'one', 'occasion', '.', '\"', 'get', 'into', 'the', 'soil', 'all', 'you', 'can', '.', 'dig', '-', '-', 'and', 'dig', 'deep', '.', '\"']\n",
      "pos-start: 18:18:05.713481\n",
      "ADP [MASK] their very first meeting, gilbert had not been pleasantly impressed [MASK] hardy. but he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered [MASK] the depredations [MASK] mexico's casual visitors, and was ready to protect not only his own interests but those [MASK] any newcomers. he seemed to have the spirit [MASK] fair - mindedness ; and he believed firmly [MASK] the possibilities [MASK] this magic land, particularly [MASK] young men. \" it's god's country, \" he told gilbert [MASK] more than one occasion. \" get [MASK] the soil all you can. dig - - and dig deep. \"\n",
      "PRON on [MASK] very first meeting, gilbert had not been pleasantly impressed with hardy. but [MASK] soon saw that the man had a certain rugged strength, and [MASK] was no doubt [MASK] had suffered from the depredations of mexico's casual visitors, and was ready to protect not only [MASK] own interests but those of any newcomers. [MASK] seemed to have the spirit of fair - mindedness ; and [MASK] believed firmly in the possibilities of this magic land, particularly for young men. \" it's god's country, \" [MASK] told gilbert on more than one occasion. \" get into the soil all [MASK] can. dig - - and dig deep. \"\n",
      "ADV on their [MASK] first meeting, gilbert had not been [MASK] impressed with hardy. but he [MASK] saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of mexico's casual visitors, and was ready to protect not [MASK] his own interests but those of any newcomers. he seemed to have the spirit of fair - mindedness ; and he believed [MASK] in the possibilities of this magic land, [MASK] for young men. \" it's god's country, \" he told gilbert on more than one occasion. \" get into the soil all you can. dig - - and dig deep. \"\n",
      "ADJ on their very [MASK] meeting, gilbert had not been pleasantly [MASK] with hardy. but he soon saw that the man had a [MASK] [MASK] strength, and there was no doubt he had suffered from the depredations of mexico's [MASK] visitors, and was [MASK] to protect not only his [MASK] interests but those of any newcomers. he seemed to have the spirit of fair - mindedness ; and he believed firmly in the possibilities of this [MASK] land, particularly for [MASK] men. \" it's god's country, \" he told gilbert on [MASK] than one occasion. \" get into the soil all you can. dig - - and dig [MASK]. \"\n",
      "NOUN on their very first [MASK], gilbert had not been pleasantly impressed with hardy. but he soon saw that the [MASK] had a certain rugged [MASK], and there was no [MASK] he had suffered from the [MASK] [MASK] [MASK] [MASK] of mexico's casual [MASK], and was ready to protect not only his own [MASK] but those of any [MASK]. he seemed to have the [MASK] of [MASK] - [MASK] [MASK] ; and he believed firmly in the [MASK] of this magic [MASK], particularly for young [MASK]. \" it's god's [MASK], \" he told gilbert on more than one [MASK]. \" get into the [MASK] all you can. dig - - and dig deep. \"\n",
      "PUNCT on their very first meeting [MASK] gilbert had not been pleasantly impressed with hardy [MASK] but he soon saw that the man had a certain rugged strength [MASK] and there was no doubt he had suffered from the depredations of mexico's casual visitors [MASK] and was ready to protect not only his own interests but those of any newcomers [MASK] he seemed to have the spirit of fair [MASK] mindedness [MASK] and he believed firmly in the possibilities of this magic land [MASK] particularly for young men [MASK] [MASK] it's god's country [MASK] [MASK] he told gilbert on more than one occasion [MASK] [MASK] get into the soil all you can [MASK] dig [MASK] [MASK] and dig deep [MASK] [MASK]\n",
      "PROPN on their very first meeting, gilbert had not been pleasantly impressed with hardy. but he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of mexico's casual visitors, and was ready to protect not only his own interests but those of any newcomers. he seemed to have the spirit of fair - mindedness ; and he believed firmly in the possibilities of this magic land, particularly for young men. \" it's god's country, \" he told gilbert on more than one occasion. \" get into the soil all you can. dig - - and dig deep. \"\n",
      "VERB on their very first meeting, gilbert [MASK] not [MASK] pleasantly impressed with hardy. but he soon [MASK] that the man [MASK] a certain rugged strength, and there [MASK] no doubt he [MASK] [MASK] from the depredations of mexico's casual visitors, and [MASK] ready to [MASK] not only his own interests but those of any newcomers. he [MASK] to [MASK] the spirit of fair - mindedness ; and he [MASK] firmly in the possibilities of this magic land, particularly for young men. \" it's god's country, \" he [MASK] gilbert on more than one occasion. \" get into the soil all you can. [MASK] - - and [MASK] deep. \"\n",
      "PART on their very first meeting, gilbert had [MASK] been pleasantly impressed with hardy. but he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of mexico's casual visitors, and was ready [MASK] protect [MASK] only his own interests but those of any newcomers. he seemed [MASK] have the spirit of fair - mindedness ; and he believed firmly in the possibilities of this magic land, particularly for young men. \" it's god's country, \" he told gilbert on more than one occasion. \" get into the soil all you can. dig - - and dig deep. \"\n",
      "CCONJ on their very first meeting, gilbert had not been pleasantly impressed with hardy. [MASK] he soon saw that the man had a certain rugged strength, [MASK] there was no doubt he had suffered from the depredations of mexico's casual visitors, [MASK] was ready to protect not only his own interests [MASK] those of any newcomers. he seemed to have the spirit of fair - mindedness ; [MASK] he believed firmly in the possibilities of this magic land, particularly for young men. \" it's god's country, \" he told gilbert on more than one occasion. \" get into the soil all you can. dig - - [MASK] dig deep. \"\n",
      "SCONJ on their very first meeting, gilbert had not been pleasantly impressed with hardy. but he soon saw [MASK] the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of mexico's casual visitors, and was ready to protect not only his own interests but those of any newcomers. he seemed to have the spirit of fair - mindedness ; and he believed firmly in the possibilities of this magic land, particularly for young men. \" it's god's country, \" he told gilbert on more [MASK] one occasion. \" get into the soil all you can. dig - - and dig deep. \"\n",
      "DET on their very first meeting, gilbert had not been pleasantly impressed with hardy. but he soon saw that [MASK] man had [MASK] certain rugged strength, and there was [MASK] doubt he had suffered from [MASK] depredations of mexico's casual visitors, and was ready to protect not only his own interests but [MASK] of [MASK] newcomers. he seemed to have [MASK] spirit of fair - mindedness ; and he believed firmly in [MASK] possibilities of [MASK] magic land, particularly for young men. \" it's god's country, \" he told gilbert on more than one occasion. \" get into [MASK] soil [MASK] you can. dig - - and dig deep. \"\n",
      "NUM on their very first meeting, gilbert had not been pleasantly impressed with hardy. but he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of mexico's casual visitors, and was ready to protect not only his own interests but those of any newcomers. he seemed to have the spirit of fair - mindedness ; and he believed firmly in the possibilities of this magic land, particularly for young men. \" it's god's country, \" he told gilbert on more than [MASK] occasion. \" get into the soil all you can. dig - - and dig deep. \"\n",
      "AUX on their very first meeting, gilbert had not been pleasantly impressed with hardy. but he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of mexico's casual visitors, and was ready to protect not only his own interests but those of any newcomers. he seemed to have the spirit of fair - mindedness ; and he believed firmly in the possibilities of this magic land, particularly for young men. \" it's god's country, \" he told gilbert on more than one occasion. \" get into the soil all you [MASK]. dig - - and dig deep. \"\n",
      "lemma-start: 18:18:05.717469\n",
      "Lemma On their very first meeting, Gilbert have not be pleasantly impressed with Hardy. But he soon see that the man have a certain rugged strength, and there be no doubt he have suffer from the depredation of Mexicobe casual visitor, and be ready to protect not only his own interest but those of any newcomer. He seem to have the spirit of fair-mindedness; and he believe firmly in the possibility of this magic land, particularly for young man. \"Itbe Godbe country,\" he tell Gilbert on more than one occasion. \"Get into the soil all you can. Dig--and dig deep.\"\n",
      "ner-start: 18:18:05.718467\n",
      "NER On their very first meeting, Gilbert had not been pleasantly impressed with Hardy. But he soon saw that the man had a certain rugged strength, and there was no doubt he had suffered from the depredations of Mexico's casual visitors, and was ready to protect not only his own interests but those of any newcomers. He seemed to have the spirit of fair-mindedness; and he believed firmly in the possibilities of this magic land, particularly for young men. \"It's God's country,\" he told Gilbert on more than one occasion. \"Get into the soil all you can. Dig--and dig deep.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  103, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006,  103,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        ...,\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ...,    0,    0,    0],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        ...,\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102],\n",
       "        [ 101, 2006, 2037,  ..., 1012, 1000,  102]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_word_MO_tokenization_and_masking(tokenizer=tokenizer, nlp_model=nlp, sequence=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: 18:18:05.795231\n",
      "\"Sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border,\" Pell was telling her. \"Some important men want to talk things over with me at Bisbee.I want to get started in a day or two.Don't take your maid.It's a rough country, but you'll be all right.Just old clothes.You can ride a lot, so bring your habit.I'll be busy most of the time; but I think you'll like the trip.Never been down that way, have you?\"\n",
      "['\"', 'stu', '##rg', '##is', 'telegraph', '##ed', 'me', 'that', 'there', 'was', 'a', 'big', 'possibility', 'of', 'a', 'new', 'vein', 'of', 'oil', 'down', 'on', 'the', 'border', ',', '\"', 'pe', '##ll', 'was', 'telling', 'her', '.', '\"', 'some', 'important', 'men', 'want', 'to', 'talk', 'things', 'over', 'with', 'me', 'at', 'bis', '##bee', '.', 'i', 'want', 'to', 'get', 'started', 'in', 'a', 'day', 'or', 'two', '.', 'don', \"'\", 't', 'take', 'your', 'maid', '.', 'it', \"'\", 's', 'a', 'rough', 'country', ',', 'but', 'you', \"'\", 'll', 'be', 'all', 'right', '.', 'just', 'old', 'clothes', '.', 'you', 'can', 'ride', 'a', 'lot', ',', 'so', 'bring', 'your', 'habit', '.', 'i', \"'\", 'll', 'be', 'busy', 'most', 'of', 'the', 'time', ';', 'but', 'i', 'think', 'you', \"'\", 'll', 'like', 'the', 'trip', '.', 'never', 'been', 'down', 'that', 'way', ',', 'have', 'you', '?', '\"']\n",
      "pos-start: 18:18:05.819167\n",
      "PUNCT [MASK] sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border [MASK] [MASK] pell was telling her [MASK] [MASK] some important men want to talk things over with me at bisbee [MASK] i want to get started in a day or two [MASK] don't take your maid [MASK] it's a rough country [MASK] but you'll be all right [MASK] just old clothes [MASK] you can ride a lot [MASK] so bring your habit [MASK] i'll be busy most of the time [MASK] but i think you'll like the trip [MASK] never been down that way [MASK] have you [MASK] [MASK]\n",
      "PROPN \" sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border, \" pell was telling her. \" some important men want to talk things over with me at bisbee. i want to get started in a day or two. don't take your maid. it's a rough country, but you'll be all right. just old clothes. you can ride a lot, so bring your habit. i'll be busy most of the time ; but i think you'll like the trip. never been down that way, have you? \"\n",
      "VERB \" sturgis [MASK] [MASK] me that there [MASK] a big possibility of a new vein of oil down on the border, \" pell [MASK] [MASK] her. \" some important men [MASK] to [MASK] things over with me at bisbee. i [MASK] to [MASK] [MASK] in a day or two. don't [MASK] your maid. it's a rough country, but you'll [MASK] all right. just old clothes. you can [MASK] a lot, so [MASK] your habit. i'll [MASK] busy most of the time ; but i [MASK] you'll [MASK] the trip. never [MASK] down that way, [MASK] you? \"\n",
      "PRON \" sturgis telegraphed [MASK] that [MASK] was a big possibility of a new vein of oil down on the border, \" pell was telling [MASK]. \" some important men want to talk things over with [MASK] at bisbee. i want to get started in a day or two. don't take [MASK] maid. it's a rough country, but [MASK]'ll be all right. just old clothes. [MASK] can ride a lot, so bring [MASK] habit. i'll be busy most of the time ; but i think [MASK]'ll like the trip. never been down that way, have [MASK]? \"\n",
      "SCONJ \" sturgis telegraphed me [MASK] there was a big possibility of a new vein of oil down on the border, \" pell was telling her. \" some important men want to talk things over with me at bisbee. i want to get started in a day or two. don't take your maid. it's a rough country, but you'll be all right. just old clothes. you can ride a lot, so bring your habit. i'll be busy most of the time ; but i think you'll like the trip. never been down [MASK] way, have you? \"\n",
      "DET \" sturgis telegraphed me [MASK] there was [MASK] big possibility of [MASK] new vein of oil down on [MASK] border, \" pell was telling her. \" some important men want to talk things over with me at bisbee. i want to get started in [MASK] day or two. don't take your maid. it's [MASK] rough country, but you'll be all right. just old clothes. you can ride [MASK] lot, so bring your habit. i'll be busy most of [MASK] time ; but i think you'll like [MASK] trip. never been down [MASK] way, have you? \"\n",
      "ADJ \" sturgis telegraphed me that there was a [MASK] possibility of a [MASK] vein of oil down on the border, \" pell was telling her. \" some [MASK] men want to talk things over with me at bisbee. i want to get started in a day or two. don't take your maid. it's a [MASK] country, but you'll be all [MASK]. just [MASK] clothes. you can ride a lot, so bring your habit. i'll be [MASK] [MASK] of the time ; but i think you'll like the trip. never been down that way, have you? \"\n",
      "NOUN \" sturgis telegraphed me that there was a big [MASK] of a new [MASK] of [MASK] down on the [MASK], \" pell was telling her. \" some important [MASK] want to talk [MASK] over with me at bisbee. i want to get started in a [MASK] or two. don't take your [MASK]. it's a rough [MASK], but you'll be all right. just old [MASK]. you can ride a [MASK], so bring your [MASK]. i'll be busy most of the [MASK] ; but i think you'll like the [MASK]. never been down that [MASK], have you? \"\n",
      "ADP \" sturgis telegraphed me that there was a big possibility [MASK] a new vein [MASK] oil [MASK] [MASK] the border, \" pell was telling her. \" some important men want to talk things [MASK] [MASK] me [MASK] bisbee. i want to get started [MASK] a day or two. don't take your maid. it's a rough country, but you'll be all right. just old clothes. you can ride a lot, so bring your habit. i'll be busy most [MASK] the time ; but i think you'll like the trip. never been [MASK] that way, have you? \"\n",
      "PART \" sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border, \" pell was telling her. \" some important men want [MASK] talk things over with me at bisbee. i want [MASK] get started in a day or two. don't take your maid. it's a rough country, but you'll be all right. just old clothes. you can ride a lot, so bring your habit. i'll be busy most of the time ; but i think you'll like the trip. never been down that way, have you? \"\n",
      "CCONJ \" sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border, \" pell was telling her. \" some important men want to talk things over with me at bisbee. i want to get started in a day [MASK] two. don't take your maid. it's a rough country, [MASK] you'll be all right. just old clothes. you can ride a lot, so bring your habit. i'll be busy most of the time ; [MASK] i think you'll like the trip. never been down that way, have you? \"\n",
      "NUM \" sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border, \" pell was telling her. \" some important men want to talk things over with me at bisbee. i want to get started in a day or [MASK]. don't take your maid. it's a rough country, but you'll be all right. just old clothes. you can ride a lot, so bring your habit. i'll be busy most of the time ; but i think you'll like the trip. never been down that way, have you? \"\n",
      "INTJ \" sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border, \" pell was telling her. \" some important men want to talk things over with me at bisbee. i want to get started in a day or two. don't take your maid. it's a rough country, but you'll be all right. just old clothes. you can ride a lot, so bring your habit. i'll be busy most of the time ; but i think you'll like the trip. never been down that way, have you? \"\n",
      "AUX \" sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border, \" pell was telling her. \" some important men want to talk things over with me at bisbee. i want to get started in a day or two. don't take your maid. it's a rough country, but you'll be all right. just old clothes. you [MASK] ride a lot, so bring your habit. i'll be busy most of the time ; but i think you'll like the trip. never been down that way, have you? \"\n",
      "ADV \" sturgis telegraphed me that there was a big possibility of a new vein of oil [MASK] on the border, \" pell was telling her. \" some important men want to talk things over with me at bisbee. i want to get started in a day or two. don't take your maid. it's a rough country, but you'll be [MASK] right. just old clothes. you can ride a lot, [MASK] bring your habit. i'll be busy most of the time ; but i think you'll like the trip. never been [MASK] that way, have you? \"\n",
      "lemma-start: 18:18:05.825152\n",
      "Lemma \"Sturgis telegraph I that there be a big possibility of a new vein of oil down on the border,\" Pell be tell she. \"Some important man want to talk thing over with I at Bisbee.I want to get start in a day or two.Don't take your maid.Itbe a rough country, but you'll be all right.Just old clothe.You can ride a lot, so bring your habit.I'll be busy most of the time; but I think you'll like the trip.Never be down that way, have you?\"\n",
      "ner-start: 18:18:05.826149\n",
      "NER \"Sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border,\" Pell was telling her. \"Some important men want to talk things over with me at Bisbee.I want to get started in a day or two.Don't take your maid.It's a rough country, but you'll be all right.Just old clothes.You can ride a lot, so bring your habit.I'll be busy most of the time; but I think you'll like the trip.Never been down that way, have you?\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103, 24646,  ...,   103,   103,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        ...,\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1000,   102,     0],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        ...,\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102],\n",
       "        [  101,  1000, 24646,  ...,  1029,  1000,   102]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = '\"Sturgis telegraphed me that there was a big possibility of a new vein of oil down on the border,\" Pell was telling her. \"Some important men want to talk things over with me at Bisbee.I want to get started in a day or two.Don\\'t take your maid.It\\'s a rough country, but you\\'ll be all right.Just old clothes.You can ride a lot, so bring your habit.I\\'ll be busy most of the time; but I think you\\'ll like the trip.Never been down that way, have you?\"'\n",
    "whole_word_MO_tokenization_and_masking(tokenizer=tokenizer, nlp_model=nlp, sequence=text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny_config = {\"hidden_size\": 128, \n",
    "                    \"hidden_act\": \"gelu\", \n",
    "                    \"initializer_range\": 0.02, \n",
    "                    \"vocab_size\": 30522, \n",
    "                    \"hidden_dropout_prob\": 0.1, \n",
    "                    \"num_attention_heads\": 2, \n",
    "                    \"type_vocab_size\": 2, \n",
    "                    \"max_position_embeddings\": 512, \n",
    "                    \"num_hidden_layers\": 2, \n",
    "                    \"intermediate_size\": 512, \n",
    "                    \"attention_probs_dropout_prob\": 0.1}\n",
    "\n",
    "\n",
    "model = BertForMaskedLM(config=BertConfig(**bert_tiny_config))\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**example_sentence_inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(10.3635, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.4080,  0.2334, -0.1340,  ..., -0.0911,  0.3205, -0.1181],\n",
       "         [ 0.1753,  0.1798, -0.0103,  ...,  0.0868,  0.2404, -0.0679],\n",
       "         [ 0.3360, -0.2163,  0.2287,  ...,  0.1449,  0.2034, -0.3322],\n",
       "         ...,\n",
       "         [ 0.2280, -0.2229, -0.1290,  ..., -0.0234, -0.0660, -0.0227],\n",
       "         [ 0.5470, -0.1972,  0.1615,  ...,  0.0868, -0.0703,  0.2246],\n",
       "         [ 0.3038,  0.0603, -0.1612,  ..., -0.1186,  0.2610,  0.3539]],\n",
       "\n",
       "        [[ 0.3188,  0.1850, -0.0963,  ..., -0.0993,  0.2571, -0.3354],\n",
       "         [ 0.2630,  0.1995,  0.1957,  ...,  0.0708,  0.2714,  0.0073],\n",
       "         [ 0.2004, -0.1573,  0.3025,  ...,  0.1859,  0.2579, -0.1575],\n",
       "         ...,\n",
       "         [ 0.3260, -0.3047, -0.0999,  ...,  0.0763,  0.0275, -0.0704],\n",
       "         [ 0.5253, -0.3192,  0.1256,  ...,  0.0444, -0.0655,  0.4057],\n",
       "         [ 0.3130, -0.0074, -0.1666,  ...,  0.0190,  0.3194,  0.3130]],\n",
       "\n",
       "        [[ 0.3328,  0.2426, -0.1400,  ..., -0.1458,  0.2980, -0.2120],\n",
       "         [ 0.1907,  0.1023,  0.0388,  ...,  0.1762,  0.3204, -0.0094],\n",
       "         [ 0.3141, -0.2662,  0.1801,  ..., -0.2736,  0.0145, -0.3376],\n",
       "         ...,\n",
       "         [ 0.3765, -0.1680, -0.1541,  ...,  0.0371,  0.0975, -0.1008],\n",
       "         [ 0.4559, -0.2811,  0.2282,  ..., -0.0399, -0.0869,  0.3479],\n",
       "         [ 0.4043,  0.0591, -0.2207,  ..., -0.0494,  0.2238,  0.3412]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2342,  0.1639, -0.1579,  ..., -0.0545,  0.2320, -0.2114],\n",
       "         [ 0.0245,  0.0599,  0.1438,  ...,  0.0194,  0.2432,  0.0155],\n",
       "         [ 0.2544, -0.1755,  0.1939,  ..., -0.0592,  0.1919, -0.3171],\n",
       "         ...,\n",
       "         [ 0.2050, -0.2293, -0.1400,  ...,  0.0269,  0.0490, -0.0445],\n",
       "         [ 0.5308, -0.0585,  0.1669,  ...,  0.1082, -0.1933,  0.0451],\n",
       "         [ 0.2421,  0.1235, -0.1517,  ..., -0.1687,  0.1765,  0.1982]],\n",
       "\n",
       "        [[ 0.4122,  0.2150, -0.1297,  ..., -0.1760,  0.1165, -0.1324],\n",
       "         [ 0.2128,  0.1847,  0.1462,  ...,  0.2215,  0.3560, -0.0062],\n",
       "         [ 0.3762,  0.0097,  0.1396,  ...,  0.0419,  0.2547, -0.2638],\n",
       "         ...,\n",
       "         [ 0.1400,  0.0473,  0.1803,  ...,  0.1809,  0.1227, -0.3260],\n",
       "         [ 0.5229, -0.3176,  0.2405,  ...,  0.0403, -0.1125,  0.3473],\n",
       "         [ 0.3048,  0.0121, -0.1973,  ..., -0.0512,  0.3790,  0.4213]],\n",
       "\n",
       "        [[ 0.4146,  0.2256, -0.0494,  ..., -0.0865,  0.2301, -0.1716],\n",
       "         [ 0.2199,  0.1764, -0.0018,  ...,  0.1429,  0.2856,  0.0030],\n",
       "         [ 0.4046, -0.2568,  0.0220,  ..., -0.0649,  0.2504, -0.2205],\n",
       "         ...,\n",
       "         [ 0.2458, -0.2358, -0.0765,  ...,  0.0339,  0.0196, -0.0680],\n",
       "         [ 0.4625, -0.2134,  0.2998,  ...,  0.0720,  0.0228,  0.2265],\n",
       "         [ 0.3201,  0.0502, -0.0923,  ..., -0.1320,  0.3438,  0.4545]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(10.3671, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.2816,  0.2122, -0.0551,  ..., -0.2080,  0.1901, -0.2705],\n",
       "         [-0.0213,  0.0675,  0.1182,  ...,  0.0587,  0.4813,  0.0793],\n",
       "         [ 0.3258, -0.1267,  0.1164,  ..., -0.1771,  0.3251, -0.2187],\n",
       "         ...,\n",
       "         [ 0.2462, -0.1275, -0.2061,  ...,  0.0215,  0.0755, -0.1286],\n",
       "         [ 0.5200, -0.2259,  0.2210,  ..., -0.0152, -0.0605,  0.2946],\n",
       "         [ 0.3501, -0.0105, -0.1718,  ..., -0.0210,  0.2615,  0.3812]],\n",
       "\n",
       "        [[ 0.2275,  0.1725, -0.0553,  ..., -0.1587,  0.2556, -0.1112],\n",
       "         [ 0.1502,  0.0833,  0.0923,  ...,  0.1065,  0.3873,  0.0937],\n",
       "         [ 0.2265, -0.1045,  0.4209,  ..., -0.0535,  0.1077, -0.3624],\n",
       "         ...,\n",
       "         [ 0.3259, -0.1775, -0.1235,  ..., -0.1407,  0.0281, -0.0293],\n",
       "         [ 0.3989, -0.2696,  0.3612,  ..., -0.0359, -0.1130,  0.1344],\n",
       "         [ 0.3135,  0.1012, -0.0567,  ..., -0.2063,  0.1898,  0.3347]],\n",
       "\n",
       "        [[ 0.4600,  0.2001, -0.1444,  ..., -0.0630,  0.2134, -0.2831],\n",
       "         [ 0.1443,  0.1785,  0.1251,  ...,  0.1313,  0.2628, -0.0070],\n",
       "         [ 0.3480, -0.2529,  0.2161,  ..., -0.0476,  0.0933, -0.2561],\n",
       "         ...,\n",
       "         [ 0.3594, -0.1510, -0.0927,  ...,  0.0556,  0.0759, -0.0126],\n",
       "         [ 0.4560, -0.2729,  0.4380,  ..., -0.0316, -0.0910,  0.0859],\n",
       "         [ 0.2657,  0.0853, -0.1495,  ..., -0.1960,  0.1884,  0.1716]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4066,  0.3265, -0.1718,  ..., -0.1383,  0.2710, -0.1947],\n",
       "         [ 0.1619,  0.1569,  0.1255,  ...,  0.0737,  0.2232, -0.1639],\n",
       "         [ 0.2516, -0.2060,  0.0753,  ..., -0.1899,  0.3505, -0.1606],\n",
       "         ...,\n",
       "         [ 0.3563, -0.2261,  0.0580,  ...,  0.1974, -0.0423, -0.0620],\n",
       "         [ 0.3868, -0.0905,  0.1130,  ...,  0.2249, -0.0126,  0.1889],\n",
       "         [ 0.3091,  0.0272, -0.2514,  ..., -0.1250,  0.2928,  0.2478]],\n",
       "\n",
       "        [[ 0.2843,  0.1545, -0.1877,  ..., -0.3571,  0.2111, -0.2885],\n",
       "         [ 0.2940,  0.0729,  0.1109,  ...,  0.1499,  0.3473,  0.0296],\n",
       "         [ 0.4392, -0.1212,  0.1567,  ..., -0.0101,  0.1292, -0.1343],\n",
       "         ...,\n",
       "         [ 0.1106, -0.0064,  0.2608,  ...,  0.2644,  0.1413, -0.1852],\n",
       "         [ 0.5408, -0.1474,  0.3615,  ..., -0.1125, -0.0660,  0.3307],\n",
       "         [ 0.2371,  0.1462, -0.2115,  ..., -0.0931,  0.3663,  0.4074]],\n",
       "\n",
       "        [[ 0.3778,  0.2204, -0.1780,  ..., -0.0677,  0.3275, -0.1848],\n",
       "         [ 0.1432,  0.1848, -0.0086,  ...,  0.0392,  0.2615,  0.0910],\n",
       "         [ 0.2999, -0.2431,  0.0979,  ..., -0.2364,  0.2583, -0.1904],\n",
       "         ...,\n",
       "         [ 0.4049, -0.1668, -0.1279,  ...,  0.0686,  0.1414, -0.0027],\n",
       "         [ 0.4753, -0.2474,  0.3202,  ..., -0.0497, -0.0528,  0.2128],\n",
       "         [ 0.3502,  0.0458, -0.0928,  ..., -0.0666,  0.3003,  0.3849]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**example_sentence_inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MODataset at 0x220e4b25fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = {key: val for key, val in encodings.items() if key != 'labels'}\n",
    "        self.labels = encodings['labels']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MODataset(example_sentence_inputs)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=256,  # batch size per device during training\n",
    "    per_device_eval_batch_size=256,   # batch size for evaluation\n",
    "    learning_rate=1e-5,     \n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=None            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d974faea4849e5858f2b115ffe313d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581a12771642460784998c2bacaa1641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f765e5c77aa84679bce29b5b9a7407c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08fda4492c24b8ab7e38735fd1e6e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=10.356541315714518)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom tokenizer\n",
    "====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategizedTokenizer(object):\n",
    "    def __init__(self, pos_based_mask=True, lemmatize=True, ner_based_swap=True):\n",
    "        \"\"\"\n",
    "        Constructs the strategized Tokenizer.\n",
    "        Loads the required spacy model\n",
    "        \n",
    "        Processes the sentence based on desired properties\n",
    "        \n",
    "        ==Not guaranteed to work on cased vocabularies==\n",
    "        \"\"\"\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.pos_based_mask = pos_based_mask\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ner_based_swap = ner_based_swap\n",
    "        \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        spacy_sentence = nlp(text, disable=['parser'])\n",
    "        \n",
    "        processed_text_list = []\n",
    "        if self.pos_based_mask:\n",
    "            processed_text_list += self.mask_text_pos_based(text, spacy_sentence)\n",
    "        if self.lemmatize:\n",
    "            processed_text_list += self.lemmatize_text(text, spacy_sentence)         \n",
    "        if self.ner_based_swap:\n",
    "            processed_text_list += self.ner_swap_text(text, spacy_sentence)\n",
    "        #TODO add more?\n",
    "        \n",
    "        for x in processed_text_list:\n",
    "            print(x)\n",
    "        inputs = self.tokenizer(processed_text_list,\n",
    "                                return_token_type_ids=False #Dont need this because we dont use NSP\n",
    "                                return_tensors=\"pt\", \n",
    "                                padding=True)\n",
    "        inputs['labels'] = self.tokenizer([text for i in range(0,len(processed_text_list))], \n",
    "                                          return_attention_mask=False, \n",
    "                                          return_token_type_ids=False,\n",
    "                                          return_tensors='pt', \n",
    "                                          padding=True)['input_ids']\n",
    "        return inputs\n",
    "    \n",
    "    def mask_text_pos_based(self, text, spacy_sentence) -> list:\n",
    "        input_ids = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        input_tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        pos_masks = []\n",
    "        \n",
    "        text_pos_list = [token.pos_ for token in spacy_sentence]\n",
    "        text_pos_frequency = {pos: text_pos_list.count(pos) for pos in text_pos_list}\n",
    "        \n",
    "        for posoi in text_pos_frequency.keys():\n",
    "            posoi_vocab = [token.text.lower() for token in spacy_sentence if token.pos_ == posoi]\n",
    "            mask_indices = []\n",
    "            \n",
    "            composite_word_indices = []\n",
    "            composite_word_tokens = []\n",
    "            for (i, token) in enumerate(input_tokens):\n",
    "                if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                    continue\n",
    "                elif token.startswith(\"##\"):\n",
    "                    composite_word_indices.append(i)\n",
    "                    composite_word_tokens.append(token)\n",
    "                    if \"\".join([x.strip(\"##\") for x in composite_word_tokens]) in posoi_vocab:\n",
    "                        mask_indices = mask_indices + composite_word_indices\n",
    "\n",
    "                elif token in posoi_vocab:\n",
    "                    mask_indices.append(i)\n",
    "                else:\n",
    "                    composite_word_indices = [i]\n",
    "                    composite_word_tokens = [token]\n",
    "\n",
    "            mask_labels = [1 if i in mask_indices else 0 for i in range(len(input_tokens))]\n",
    "            masked_tokens = [x if mask_labels[i] == 0 else 103 for i,x in enumerate(input_ids)]\n",
    "            masked_input = self.tokenizer.decode(masked_tokens)         \n",
    "            pos_masks.append(masked_input)\n",
    "            \n",
    "        return pos_masks\n",
    "    \n",
    "    def lemmatize_text(self, text, spacy_sentence) -> list:\n",
    "        replacement_tuples = [(token.text, token.lemma_) for token in spacy_sentence if token.text.lower() != token.lemma_]\n",
    "        lemmatized_text = text\n",
    "        for replacement in replacement_tuples:\n",
    "            lemmatized_text = re.sub(r'\\b' + replacement[0] + r'\\b', replacement[1], lemmatized_text)\n",
    "\n",
    "        lemmatized_text = lemmatized_text.replace(\"  \", \" \")\n",
    "        return [lemmatized_text]\n",
    "    \n",
    "    def ner_swap_text(self, text, spacy_sentence) -> list:\n",
    "        ner_swapped_text = spacy_sentence.text\n",
    "        for ent in spacy_sentence.ents:\n",
    "            if ent.label_ == 'TIME':\n",
    "                time_substring = ner_swapped_text[ent.start_char:ent.end_char].split(\" \")\n",
    "                time_substring.reverse()\n",
    "                ner_swapped_text = ner_swapped_text.replace(ner_swapped_text[ent.start_char:ent.end_char], \" \".join(time_substring))\n",
    "            #TODO add other possible ideas\n",
    "        return [ner_swapped_text]\n",
    "        \n",
    "    def convert_ids_to_tokens(self, input_ids):\n",
    "        return [self.tokenizer.convert_ids_to_tokens(row) for row in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_tokenizer = StrategizedTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK] went to the [MASK] [MASK] [MASK] [MASK] at 5 o'clock to buy some milk for me.\n",
      "anne [MASK] to the albert heijn at 5 o'clock to [MASK] some milk for me.\n",
      "anne went [MASK] the albert heijn [MASK] 5 o'clock [MASK] buy some milk [MASK] me.\n",
      "anne went to [MASK] albert heijn at 5 o'clock to buy [MASK] milk for me.\n",
      "anne went to the albert heijn at [MASK] o'clock to buy some milk for me.\n",
      "anne went to the albert heijn at 5 o'clock to buy some [MASK] for me.\n",
      "anne went [MASK] the albert heijn at 5 o'clock [MASK] buy some milk for me.\n",
      "anne went to the albert heijn at 5 o'clock to buy some milk for [MASK].\n",
      "anne went to the albert heijn at 5 o'clock to buy some milk for me [MASK]\n",
      "Anne go to the Albert Heijn at 5 o'clock to buy some milk for I.\n",
      "Anne went to the Albert Heijn at o'clock 5 to buy some milk for me.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,  2253,  2000,  1996,   103,   103,   103,   103,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,   103,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,   103,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,   103,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,   103,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,   103,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,   103,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "           103,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,   103,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,   103,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "           103,   102],\n",
       "        [  101,  4776,  2175,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  1045,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1051,  1005,  5119,  1019,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]])}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = ST_tokenizer.tokenize(\"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[] + [1,2,3] + [4] + [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing some classification model\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# From paper:\n",
    "# lr: 1e-4\n",
    "# Beta1 = 0.9 (default)\n",
    "# Beta2 = 0.999 (default)\n",
    "# L2 weight decay = 0.01\n",
    "\n",
    "# Longer sequences are disproportionately expensive\n",
    "# because attention is quadratic to the sequence\n",
    "# length. To speed up pretraing in our experiments,\n",
    "# we pre-train the model with sequence length of\n",
    "# 128 for 90% of the steps. Then, we train the rest\n",
    "# 10% of the steps of sequence of 512 to learn the\n",
    "# positional embeddings.\n",
    "\n",
    "\n",
    "\n",
    "#Batch size 256 for 1e6 steps\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When ADV when\n",
      "on ADP on\n",
      "board NOUN board\n",
      "H.M.S. PROPN H.M.S.\n",
      "' PUNCT '\n",
      "Beagle PROPN Beagle\n",
      ", PUNCT ,\n",
      "' PUNCT '\n",
      "as ADP as\n",
      "naturalist ADJ naturalist\n",
      ", PUNCT ,\n",
      "I PRON I\n",
      "was AUX be\n",
      "much ADV much\n",
      "struck VERB strike\n",
      "with ADP with\n",
      "certain ADJ certain\n",
      "facts NOUN fact\n",
      "in ADP in\n",
      "the DET the\n",
      "distribution NOUN distribution\n",
      "of ADP of\n",
      "the DET the\n",
      "inhabitants NOUN inhabitant\n",
      "of ADP of\n",
      "South PROPN South\n",
      "America PROPN America\n",
      ", PUNCT ,\n",
      "and CCONJ and\n",
      "in ADP in\n",
      "the DET the\n",
      "geological ADJ geological\n",
      "relations NOUN relation\n",
      "of ADP of\n",
      "the DET the\n",
      "present NOUN present\n",
      "to ADP to\n",
      "the DET the\n",
      "past ADJ past\n",
      "inhabitants NOUN inhabitant\n",
      "of ADP of\n",
      "that DET that\n",
      "continent NOUN continent\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "#doc = nlp(\"That's a lot better. He was finally walking to the beaches. There he had a meeting with his father. Afterwards, he read a book. The fishing rod that he used was really old\")\n",
    "doc = nlp(\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San PROPN San\n",
      "Francisco PROPN Francisco\n",
      "is AUX be\n",
      "a DET a\n",
      "long ADJ long\n",
      "drive NOUN drive\n",
      "away ADV away\n",
      "from ADP from\n",
      "here ADV here\n",
      ". PUNCT .\n",
      "Ah INTJ ah\n",
      ", PUNCT ,\n",
      "I PRON I\n",
      "forgot VERB forget\n",
      "what PRON what\n",
      "I PRON I\n",
      "was AUX be\n",
      "doing VERB do\n",
      ". PUNCT .\n",
      "He PRON he\n",
      "had VERB have\n",
      "to PART to\n",
      "get VERB get\n",
      "a DET a\n",
      "new ADJ new\n",
      "pair NOUN pair\n",
      "of ADP of\n",
      "shoes NOUN shoe\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"San Francisco is a long drive away from here. Ah, I forgot what I was doing. He had to get a new pair of shoes.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
