{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "ner_list = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\",\n",
    " 'These facts seemed to me to throw some light on the origin of species--that mystery of mysteries, as it has been called by one of our greatest philosophers.',\n",
    " 'On my return home, it occurred to me, in 1837, that something might perhaps be made out on this question by patiently accumulating and reflecting on all sorts of facts which could possibly have any bearing on it.',\n",
    " \"After five years' work I allowed myself to speculate on the subject, and drew up some short notes; these I enlarged in 1844 into a sketch of the conclusions, which then seemed to me probable: from that period to the present day I have steadily pursued the same object.\",\n",
    " 'I hope that I may be excused for entering on these personal details, as I give them to show that I have not been hasty in coming to a decision.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_pos_tokenization_and_masking(sequence: str, nlp_model=None, posoi=\"VERB\"):\n",
    "        \"\"\"\n",
    "        posoi: Part-Of-Speech of interest\n",
    "        \n",
    "        Performs whole-word-masking based on selected posoi.\n",
    "        \n",
    "        POS possibilities:['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "                             \n",
    "        TODO: What if no tokens are masked?\n",
    "        \n",
    "        \"\"\"\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        spacy_sentence = nlp(sequence)\n",
    "        posoi_vocab = [token.text for token in spacy_sentence if token.pos_ == posoi]\n",
    "        input_ids = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        mask_indices = []\n",
    "        composite_word_indices = []\n",
    "        composite_word_tokens = []\n",
    "        for (i, token) in enumerate(input_tokens):\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                continue\n",
    "            elif token.startswith(\"##\"):\n",
    "                composite_word_indices.append(i)\n",
    "                composite_word_tokens.append(token)\n",
    "                if \"\".join([x.strip(\"##\") for x in composite_word_tokens]) in posoi_vocab:\n",
    "                    mask_indices = mask_indices + composite_word_indices\n",
    "                    \n",
    "            elif token in posoi_vocab:\n",
    "                mask_indices.append(i)\n",
    "            else:\n",
    "                composite_word_indices = [i]\n",
    "                composite_word_tokens = [token]\n",
    "                \n",
    "        mask_labels = [1 if i in mask_indices else 0 for i in range(len(input_tokens))]\n",
    "        masked_tokens = [x if mask_labels[i] == 0 else 103 for i,x in enumerate(input_ids)]\n",
    "        masked_input = tokenizer.decode(masked_tokens)\n",
    "        print(sequence)\n",
    "        print(masked_input)\n",
    "        \n",
    "        inputs = tokenizer(masked_input, return_tensors=\"pt\")\n",
    "        inputs['labels'] = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\n",
      "when on board h. m. s.'beagle,'as naturalist, i was much [MASK] with certain facts in the distribution of the inhabitants of south america, and in the geological relations of the present to the past inhabitants of that continent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2043,  2006,  2604,  1044,  1012,  1049,  1012,  1055,  1012,\n",
       "          1005, 26892,  9354,  1010,  1005,  2004, 19176,  1010,  1045,  2001,\n",
       "          2172,   103,  2007,  3056,  8866,  1999,  1996,  4353,  1997,  1996,\n",
       "          4864,  1997,  2148,  2637,  1010,  1998,  1999,  1996,  9843,  4262,\n",
       "          1997,  1996,  2556,  2000,  1996,  2627,  4864,  1997,  2008,  9983,\n",
       "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]]), 'labels': tensor([[  101,  2043,  2006,  2604,  1044,  1012,  1049,  1012,  1055,  1012,\n",
       "          1005, 26892,  9354,  1010,  1005,  2004, 19176,  1010,  1045,  2001,\n",
       "          2172,  4930,  2007,  3056,  8866,  1999,  1996,  4353,  1997,  1996,\n",
       "          4864,  1997,  2148,  2637,  1010,  1998,  1999,  1996,  9843,  4262,\n",
       "          1997,  1996,  2556,  2000,  1996,  2627,  4864,  1997,  2008,  9983,\n",
       "          1012,   102]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_inputs = whole_word_pos_tokenization_and_masking(\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\")\n",
    "example_sentence_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(config=BertConfig())\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**example_sentence_inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(10.4737, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.1085, -0.5772,  0.1293,  ...,  1.0368,  0.5574,  0.9810],\n",
       "         [-0.2183,  0.2934, -1.4217,  ...,  1.0051,  0.2181,  0.2821],\n",
       "         [ 0.1343, -0.0310, -0.2285,  ...,  0.5672,  0.5362,  0.3733],\n",
       "         ...,\n",
       "         [-0.0724,  0.0114, -0.5355,  ...,  1.0788,  0.3760,  1.0045],\n",
       "         [ 0.4110, -0.6894, -0.2067,  ...,  0.6501,  0.1130, -0.1410],\n",
       "         [ 0.1350,  0.8632, -0.0531,  ...,  0.8801, -0.3061,  0.2938]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(10.0098, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.3021,  0.2153,  0.3544,  ...,  0.4808,  0.4257,  0.3227],\n",
       "         [ 0.3361,  0.2841, -1.6514,  ...,  0.7961,  0.1002,  0.5828],\n",
       "         [-0.6459, -0.2280, -0.3719,  ...,  0.2865,  0.4029,  0.3031],\n",
       "         ...,\n",
       "         [ 0.2780,  0.7134, -0.7532,  ...,  1.0657,  0.1708,  0.6355],\n",
       "         [-0.3745,  0.1551, -0.5077,  ...,  0.7912,  0.3046, -0.2200],\n",
       "         [-0.8340, -0.1227, -0.5316,  ...,  0.2119,  0.2569,  0.1668]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**example_sentence_inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MODataset at 0x1b3cf5284f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = {key: val for key, val in encodings.items() if key != 'labels'}\n",
    "        self.labels = encodings['labels']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MODataset(example_sentence_inputs)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=256,  # batch size per device during training\n",
    "    per_device_eval_batch_size=256,   # batch size for evaluation\n",
    "    learning_rate=1e-5,     \n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=None            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb045c02ef44833a0f937057b1eb892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059f372fa0b74dc0b93bde42e69b6aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6078ea5fe1df405b809e90b62993a5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476489798bdd4ea0a5cb966422ec7b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=9.672528584798178)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_MO_tokenization_and_masking(tokenizer, nlp_model, sequence: str):\n",
    "        \"\"\"\n",
    "        posoi: Part-Of-Speech of interest\n",
    "        \n",
    "        Performs whole-word-masking based on selected posoi.\n",
    "        \n",
    "        POS possibilities:['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "                             \n",
    "        TODO: What if no tokens are masked?\n",
    "        \n",
    "        \"\"\"\n",
    "        print('loading:', datetime.now().time())\n",
    "        spacy_sentence = nlp_model(sequence, disable=[\"parser\"])\n",
    "        \n",
    "        POS_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', \n",
    "                            'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "        NER_list = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', \n",
    "                    'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "        NER_pairs = ['']\n",
    "        \n",
    "        input_ids = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        print(input_tokens)\n",
    "        sequence_pos_list = [token.pos_ for token in spacy_sentence]\n",
    "        sequence_pos_frequency = {pos: sequence_pos_list.count(pos) for pos in sequence_pos_list}\n",
    "        \n",
    "        modified_input_list = []\n",
    "        \n",
    "        #POS-masking\n",
    "        print('pos-start:', datetime.now().time())\n",
    "        for posoi in sequence_pos_frequency.keys():\n",
    "            posoi_vocab = [token.text for token in spacy_sentence if token.pos_ == posoi]\n",
    "            \n",
    "            mask_indices = []\n",
    "            composite_word_indices = []\n",
    "            composite_word_tokens = []\n",
    "            for (i, token) in enumerate(input_tokens):\n",
    "                if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                    continue\n",
    "                elif token.startswith(\"##\"):\n",
    "                    composite_word_indices.append(i)\n",
    "                    composite_word_tokens.append(token)\n",
    "                    if \"\".join([x.strip(\"##\") for x in composite_word_tokens]) in posoi_vocab:\n",
    "                        mask_indices = mask_indices + composite_word_indices\n",
    "\n",
    "                elif token in posoi_vocab:\n",
    "                    mask_indices.append(i)\n",
    "                else:\n",
    "                    composite_word_indices = [i]\n",
    "                    composite_word_tokens = [token]\n",
    "\n",
    "            mask_labels = [1 if i in mask_indices else 0 for i in range(len(input_tokens))]\n",
    "            masked_tokens = [x if mask_labels[i] == 0 else 103 for i,x in enumerate(input_ids)]\n",
    "            masked_input = tokenizer.decode(masked_tokens)\n",
    "        \n",
    "            modified_input_list.append((posoi, masked_input))\n",
    "        \n",
    "        print('lemma-start:', datetime.now().time())\n",
    "        #POS-based lemmatization\n",
    "        replacement_tuples = [(token.text, token.lemma_) for token in spacy_sentence if token.text.lower() != token.lemma_]\n",
    "        pos_replaced_sentence = sequence\n",
    "        for replacement in replacement_tuples:\n",
    "            pos_replaced_sentence = re.sub(r'\\b' + replacement[0] + r'\\b', replacement[1], pos_replaced_sentence)\n",
    "\n",
    "        pos_replaced_sentence = pos_replaced_sentence.replace(\"  \", \" \")\n",
    "        modified_input_list.append(('Lemma', pos_replaced_sentence))\n",
    "        \n",
    "        #NER-based swapping of time-place (if present)\n",
    "        print('ner-start:', datetime.now().time())\n",
    "        ner_swapped_sentence = spacy_sentence.text\n",
    "        for ent in spacy_sentence.ents:\n",
    "            if ent.label_ == 'TIME':\n",
    "                time_substring = ner_swapped_sentence[ent.start_char:ent.end_char].split(\" \")\n",
    "                time_substring.reverse()\n",
    "                ner_swapped_sentence = ner_swapped_sentence.replace(ner_swapped_sentence[ent.start_char:ent.end_char], \" \".join(time_substring))\n",
    "                \n",
    "        modified_input_list.append(('NER', ner_swapped_sentence))\n",
    "        \n",
    "        \n",
    "        #TODO future ideas\n",
    "        \n",
    "        #Show all resulting sequences    \n",
    "        for mask in modified_input_list:\n",
    "            print(mask)            \n",
    "        print(sequence)\n",
    "        inputs = tokenizer(modified_input_list, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        \n",
    "        inputs['labels'] = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:01:38.732978\n",
      "loading: 12:01:38.732978\n",
      "['Anne', 'went', 'to', 'the', 'Albert', 'He', '##i', '##jn', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', '.', 'After', 'that', ',', 'me', 'and', 'my', 'buddy', 'went', 'home', '.']\n",
      "pos-start: 12:01:38.761900\n",
      "lemma-start: 12:01:38.762898\n",
      "ner-start: 12:01:38.762898\n",
      "('PROPN', \"[MASK] went to the [MASK] [MASK] [MASK] [MASK] at 5 o'clock to buy some milk. After that, me and my buddy went home.\")\n",
      "('VERB', \"Anne [MASK] to the Albert Heijn at 5 o'clock to [MASK] some milk. After that, me and my buddy [MASK] home.\")\n",
      "('ADP', \"Anne went [MASK] the Albert Heijn [MASK] 5 o'clock [MASK] buy some milk. [MASK] that, me and my buddy went home.\")\n",
      "('DET', \"Anne went to [MASK] Albert Heijn at 5 o'clock to buy [MASK] milk. After [MASK], me and my buddy went home.\")\n",
      "('NUM', \"Anne went to the Albert Heijn at [MASK] o'clock to buy some milk. After that, me and my buddy went home.\")\n",
      "('NOUN', \"Anne went to the Albert Heijn at 5 o'clock to buy some [MASK]. After that, me and my [MASK] went home.\")\n",
      "('PART', \"Anne went [MASK] the Albert Heijn at 5 o'clock [MASK] buy some milk. After that, me and my buddy went home.\")\n",
      "('PUNCT', \"Anne went to the Albert Heijn at 5 o'clock to buy some milk [MASK] After that [MASK] me and my buddy went home [MASK]\")\n",
      "('PRON', \"Anne went to the Albert Heijn at 5 o'clock to buy some milk. After that, [MASK] and [MASK] buddy went home.\")\n",
      "('CCONJ', \"Anne went to the Albert Heijn at 5 o'clock to buy some milk. After that, me [MASK] my buddy went home.\")\n",
      "('ADV', \"Anne went to the Albert Heijn at 5 o'clock to buy some milk. After that, me and my buddy went [MASK].\")\n",
      "('Lemma', \"Anne go to the Albert Heijn at 5 o'clock to buy some milk. After that, I and my buddy go home.\")\n",
      "('NER', \"Anne went to the Albert Heijn at o'clock 5 to buy some milk. After that, me and my buddy went home.\")\n",
      "Anne went to the Albert Heijn at 5 o'clock to buy some milk. After that, me and my buddy went home.\n",
      "12:01:38.777857\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().time())\n",
    "test_sentence = \"Anne went to the Albert Heijn at 5 o'clock to buy some milk. After that, me and my buddy went home.\"\n",
    "whole_word_MO_tokenization_and_masking(tokenizer=tokenizer, nlp_model=nlp, sequence=test_sentence)\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'primary', 'objective', 'of', 'the', 'German', 'forces', 'was', 'to', 'com', '##pel', 'Britain', 'to', 'agree', 'to', 'a', 'negotiated', 'peace', 'settlement', '.', 'In', 'July', '1940', ',', 'the', 'air', 'and', 'sea', 'blockade', 'began', ',', 'with', 'the', 'Luftwaffe', 'mainly', 'targeting', 'coastal', '-', 'shipping', 'convoys', ',', 'as', 'well', 'as', 'ports', 'and', 'shipping', 'centres', 'such', 'as', 'Portsmouth', '.', 'On', '1', 'August', ',', 'the', 'Luftwaffe', 'was', 'directed', 'to', 'achieve', 'air', 'superiority', 'over', 'the', 'RAF', ',', 'with', 'the', 'aim', 'of', 'in', '##cap', '##ac', '##itating', 'RAF', 'Fighter', 'Command', ';', '12', 'days', 'later', ',', 'it', 'shifted', 'the', 'attacks', 'to', 'RAF', 'airfield', '##s', 'and', 'infrastructure', '.']\n",
      "('DET', '[MASK] primary objective of [MASK] German forces was to compel Britain to agree to [MASK] negotiated peace settlement. In July 1940, [MASK] air and sea blockade began, with [MASK] Luftwaffe mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, [MASK] Luftwaffe was directed to achieve air superiority over [MASK] RAF, with [MASK] aim of incapacitating RAF Fighter Command ; 12 days later, it shifted [MASK] attacks to RAF airfields and infrastructure.')\n",
      "('ADJ', 'The [MASK] objective of the [MASK] forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting [MASK] - shipping convoys, as well as ports and shipping centres [MASK] as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days later, it shifted the attacks to RAF airfields and infrastructure.')\n",
      "('NOUN', 'The primary [MASK] of the German [MASK] was to compel Britain to agree to a negotiated [MASK] [MASK]. In July 1940, the [MASK] and [MASK] [MASK] began, with the Luftwaffe mainly targeting coastal - [MASK] [MASK], as well as [MASK] and [MASK] [MASK] such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve [MASK] [MASK] over the RAF, with the [MASK] of incapacitating RAF Fighter Command ; 12 [MASK] later, it shifted the [MASK] to RAF [MASK] [MASK] and [MASK].')\n",
      "('ADP', 'The primary objective [MASK] the German forces was [MASK] compel Britain [MASK] agree [MASK] a negotiated peace settlement. [MASK] July 1940, the air and sea blockade began, [MASK] the Luftwaffe mainly targeting coastal - shipping convoys, [MASK] well [MASK] ports and shipping centres such [MASK] Portsmouth. [MASK] 1 August, the Luftwaffe was directed [MASK] achieve air superiority [MASK] the RAF, [MASK] the aim [MASK] incapacitating RAF Fighter Command ; 12 days later, it shifted the attacks [MASK] RAF airfields and infrastructure.')\n",
      "('VERB', 'The primary objective of the German forces [MASK] to [MASK] [MASK] Britain to [MASK] to a [MASK] peace settlement. In July 1940, the air and sea blockade [MASK], with the Luftwaffe mainly [MASK] coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe [MASK] [MASK] to [MASK] air superiority over the RAF, with the aim of [MASK] [MASK] [MASK] [MASK] RAF Fighter Command ; 12 days later, it [MASK] the attacks to RAF airfields and infrastructure.')\n",
      "('PART', 'The primary objective of the German forces was [MASK] compel Britain [MASK] agree [MASK] a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed [MASK] achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days later, it shifted the attacks [MASK] RAF airfields and infrastructure.')\n",
      "('PROPN', 'The primary objective of the German forces was to compel [MASK] to agree to a negotiated peace settlement. In [MASK] 1940, the air and sea blockade began, with the [MASK] mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as [MASK]. On 1 [MASK], the [MASK] was directed to achieve air superiority over the [MASK], with the aim of incapacitating [MASK] [MASK] [MASK] ; 12 days later, it shifted the attacks to [MASK] airfields and infrastructure.')\n",
      "('PUNCT', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement [MASK] In July 1940 [MASK] the air and sea blockade began [MASK] with the Luftwaffe mainly targeting coastal [MASK] shipping convoys [MASK] as well as ports and shipping centres such as Portsmouth [MASK] On 1 August [MASK] the Luftwaffe was directed to achieve air superiority over the RAF [MASK] with the aim of incapacitating RAF Fighter Command [MASK] 12 days later [MASK] it shifted the attacks to RAF airfields and infrastructure [MASK]')\n",
      "('NUM', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July [MASK], the air and sea blockade began, with the Luftwaffe mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On [MASK] August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; [MASK] days later, it shifted the attacks to RAF airfields and infrastructure.')\n",
      "('CCONJ', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air [MASK] sea blockade began, with the Luftwaffe mainly targeting coastal - shipping convoys, as well as ports [MASK] shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days later, it shifted the attacks to RAF airfields [MASK] infrastructure.')\n",
      "('ADV', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe [MASK] targeting coastal - shipping convoys, [MASK] [MASK] [MASK] ports and shipping centres such [MASK] Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days [MASK], it shifted the attacks to RAF airfields and infrastructure.')\n",
      "('PRON', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal - shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command ; 12 days later, [MASK] shifted the attacks to RAF airfields and infrastructure.')\n",
      "('Lemma', 'The primary objective of the German force be to compel Britain to agree to a negotiate peace settlement. In July 1940, the air and sea blockade begin, with the Luftwaffe mainly target coastal-shipping convoy, as well as port and shipping centre such as Portsmouth. On 1 August, the Luftwaffe be direct to achieve air superiority over the RAF, with the aim of incapacitate RAF Fighter Command; 12 day later, it shift the attack to RAF airfield and infrastructure.')\n",
      "('NER', 'The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal-shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command; 12 days later, it shifted the attacks to RAF airfields and infrastructure.')\n",
      "The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal-shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command; 12 days later, it shifted the attacks to RAF airfields and infrastructure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 18581,  1942,  ...,   119,   102,     0],\n",
       "        [  101,  5844,  4538,  ...,   119,   102,     0],\n",
       "        [  101, 24819, 27370,  ...,   119,   102,     0],\n",
       "        ...,\n",
       "        [  101, 11629, 11414,  ...,   119,   102,     0],\n",
       "        [  101,  3180, 12917,  ...,   102,     0,     0],\n",
       "        [  101, 26546,  2069,  ...,   119,   102,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 0],\n",
       "        [0, 0, 0,  ..., 1, 1, 0],\n",
       "        [0, 0, 0,  ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 0],\n",
       "        [0, 0, 0,  ..., 1, 0, 0],\n",
       "        [0, 0, 0,  ..., 1, 1, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 0]]), 'labels': tensor([[  101,  1109,  2425,  7649,  1104,  1103,  1528,  2088,  1108,  1106,\n",
       "          3254, 10522,  2855,  1106,  5340,  1106,   170, 14071,  3519,  3433,\n",
       "           119,  1130,  1351,  3020,   117,  1103,  1586,  1105,  2343, 17567,\n",
       "          1310,   117,  1114,  1103, 19027,  2871, 15141,  5869,   118,  8629,\n",
       "         26626,   117,  1112,  1218,  1112,  9267,  1105,  8629,  9335,  1216,\n",
       "          1112, 10867,   119,  1212,   122,  1360,   117,  1103, 19027,  1108,\n",
       "          2002,  1106,  5515,  1586, 21378,  1166,  1103,  6733,   117,  1114,\n",
       "          1103,  6457,  1104,  1107, 25265,  7409, 20563,  6733,  7388,  5059,\n",
       "           132,  1367,  1552,  1224,   117,  1122,  4707,  1103,  3690,  1106,\n",
       "          6733, 11897,  1116,  1105,  6557,   119,   102]])}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_word_MO_tokenization_and_masking(\"The primary objective of the German forces was to compel Britain to agree to a negotiated peace settlement. In July 1940, the air and sea blockade began, with the Luftwaffe mainly targeting coastal-shipping convoys, as well as ports and shipping centres such as Portsmouth. On 1 August, the Luftwaffe was directed to achieve air superiority over the RAF, with the aim of incapacitating RAF Fighter Command; 12 days later, it shifted the attacks to RAF airfields and infrastructure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy testing\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anne PROPN Anne anne\n",
      "went VERB go went\n",
      "to ADP to to\n",
      "the DET the the\n",
      "Albert PROPN Albert albert\n",
      "Heijn PROPN Heijn heijn\n",
      "at ADP at at\n",
      "5 NUM 5 5\n",
      "o'clock NOUN o'clock o'clock\n",
      "to PART to to\n",
      "buy VERB buy buy\n",
      "some DET some some\n",
      "milk NOUN milk milk\n",
      ". PUNCT . .\n",
      "After ADP after after\n",
      "that DET that that\n",
      ", PUNCT , ,\n",
      "me PRON I me\n",
      "and CCONJ and and\n",
      "my PRON my my\n",
      "buddy NOUN buddy buddy\n",
      "went VERB go went\n",
      "home ADV home home\n",
      ". PUNCT . .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#en_core_web_trf doesnt work in spacy 3.0\n",
    "#spacy_sentence = nlp(\"Apple is looking at aggresively buying U.K. startup for $1.2 billion. They walked 5 km\")\n",
    "spacy_sentence = nlp(\"Anne went to the Albert Heijn at 5 o'clock to buy some milk. After that, me and my buddy went home.\")\n",
    "test_pos_list = []\n",
    "for token in spacy_sentence:\n",
    "    test_pos_list.append(token.pos_)\n",
    "    print(token, token.pos_, token.lemma_, token.text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PROPN': 3,\n",
       " 'VERB': 3,\n",
       " 'ADP': 3,\n",
       " 'DET': 3,\n",
       " 'NUM': 1,\n",
       " 'NOUN': 3,\n",
       " 'PART': 1,\n",
       " 'PUNCT': 3,\n",
       " 'PRON': 2,\n",
       " 'CCONJ': 1,\n",
       " 'ADV': 1}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_pos_list = [token.pos_ for token in spacy_sentence]\n",
    "sequence_pos_frequency = {pos: sequence_pos_list.count(pos) for pos in sequence_pos_list}\n",
    "sequence_pos_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anne 0 4 PERSON\n",
      "the Albert Heijn 13 29 ORG\n",
      "5 o'clock 33 42 TIME\n"
     ]
    }
   ],
   "source": [
    "for ent in spacy_sentence.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anne went to the Albert Heijn at o'clock 5 to buy some milk. After that, me and my buddy went home.\n"
     ]
    }
   ],
   "source": [
    "test_string = spacy_sentence.text\n",
    "for ent in spacy_sentence.ents:\n",
    "    if ent.label_ == 'TIME':\n",
    "        time_substring = test_string[ent.start_char:ent.end_char].split(\" \")\n",
    "        time_substring.reverse()\n",
    "        test_string = test_string.replace(test_string[ent.start_char:ent.end_char], \" \".join(time_substring))\n",
    "        \n",
    "        \n",
    "print(test_string)\n",
    "        \n",
    "        \n",
    "#print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"5 o'clock\""
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string[ent.start_char:ent.end_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m some buy o'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = spacy_sentence.text[44:56].split(\" \")\n",
    "time.reverse()\n",
    "' '.join(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentence.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1b3cc0d8d10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1b40b7e5a90>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1b3cbd48d00>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1b3cbc0be20>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1b414456ac0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1b513751ec0>)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing some classification model\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# From paper:\n",
    "# lr: 1e-4\n",
    "# Beta1 = 0.9 (default)\n",
    "# Beta2 = 0.999 (default)\n",
    "# L2 weight decay = 0.01\n",
    "\n",
    "# Longer sequences are disproportionately expensive\n",
    "# because attention is quadratic to the sequence\n",
    "# length. To speed up pretraing in our experiments,\n",
    "# we pre-train the model with sequence length of\n",
    "# 128 for 90% of the steps. Then, we train the rest\n",
    "# 10% of the steps of sequence of 512 to learn the\n",
    "# positional embeddings.\n",
    "\n",
    "\n",
    "\n",
    "#Batch size 256 for 1e6 steps\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\""
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When ADV when\n",
      "on ADP on\n",
      "board NOUN board\n",
      "H.M.S. PROPN H.M.S.\n",
      "' PUNCT '\n",
      "Beagle PROPN Beagle\n",
      ", PUNCT ,\n",
      "' PUNCT '\n",
      "as ADP as\n",
      "naturalist ADJ naturalist\n",
      ", PUNCT ,\n",
      "I PRON I\n",
      "was AUX be\n",
      "much ADV much\n",
      "struck VERB strike\n",
      "with ADP with\n",
      "certain ADJ certain\n",
      "facts NOUN fact\n",
      "in ADP in\n",
      "the DET the\n",
      "distribution NOUN distribution\n",
      "of ADP of\n",
      "the DET the\n",
      "inhabitants NOUN inhabitant\n",
      "of ADP of\n",
      "South PROPN South\n",
      "America PROPN America\n",
      ", PUNCT ,\n",
      "and CCONJ and\n",
      "in ADP in\n",
      "the DET the\n",
      "geological ADJ geological\n",
      "relations NOUN relation\n",
      "of ADP of\n",
      "the DET the\n",
      "present NOUN present\n",
      "to ADP to\n",
      "the DET the\n",
      "past ADJ past\n",
      "inhabitants NOUN inhabitant\n",
      "of ADP of\n",
      "that DET that\n",
      "continent NOUN continent\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "#doc = nlp(\"That's a lot better. He was finally walking to the beaches. There he had a meeting with his father. Afterwards, he read a book. The fishing rod that he used was really old\")\n",
    "doc = nlp(\"When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the inhabitants of South America, and in the geological relations of the present to the past inhabitants of that continent.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San PROPN San\n",
      "Francisco PROPN Francisco\n",
      "is AUX be\n",
      "a DET a\n",
      "long ADJ long\n",
      "drive NOUN drive\n",
      "away ADV away\n",
      "from ADP from\n",
      "here ADV here\n",
      ". PUNCT .\n",
      "Ah INTJ ah\n",
      ", PUNCT ,\n",
      "I PRON I\n",
      "forgot VERB forget\n",
      "what PRON what\n",
      "I PRON I\n",
      "was AUX be\n",
      "doing VERB do\n",
      ". PUNCT .\n",
      "He PRON he\n",
      "had VERB have\n",
      "to PART to\n",
      "get VERB get\n",
      "a DET a\n",
      "new ADJ new\n",
      "pair NOUN pair\n",
      "of ADP of\n",
      "shoes NOUN shoe\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"San Francisco is a long drive away from here. Ah, I forgot what I was doing. He had to get a new pair of shoes.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
