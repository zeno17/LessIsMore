{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries involved in cleaning\n",
    "from cleaner_utils import super_cleaner\n",
    "from pretraining_data_utils import make_book_token_frequency, token_freq_df_to_dict, \\\n",
    "                                    all_available_tokens_from_df, optimize_book_subset_ratio\n",
    "from pretraining_data_utils import book_properties, make_df_book_properties\n",
    "from pretraining_data_utils import SentenceChunker, SentenceWriter\n",
    "from gutenberg.acquire import load_etext\n",
    "\n",
    "\n",
    "#Library utilities\n",
    "from tokenizer.tokenizer import StrategizedTokenizer\n",
    "from dataset.dataset import StrategizedTokenizerDataset\n",
    "from dataset.dataset import DefaultTokenizerDataset\n",
    "\n",
    "#Training code\n",
    "from transformers import BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from more_itertools import take\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cached_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read scraped metadata from the gutenberg metadata database \n",
    "#(Original data was scraped by using https://github.com/c-w/gutenberg)\n",
    "#The data is then further preprocessed by https://github.com/hugovk/gutenberg-metadata so it is actually usable.\n",
    "\n",
    "f = open(cache_dir + 'gutenberg-metadata.json', 'r')\n",
    "metadata = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13142"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve how many english books there in english\n",
    "english_book_keys = [key for key in metadata.keys() if metadata[key]['language'] == ['en']]\n",
    "len(english_book_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-0358ce9648a3>\", line 6, in <module>\n",
      "    super_cleaner(load_etext(14575), -1, verify_deletions=True)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gutenberg\\acquire\\text.py\", line 78, in load_etext\n",
      "    text = cache.read().decode('utf-8')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\gzip.py\", line 292, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\gzip.py\", line 470, in read\n",
      "    self._read_eof()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\gzip.py\", line 516, in _read_eof\n",
      "    raise BadGzipFile(\"CRC check failed %s != %s\" % (hex(crc32),\n",
      "gzip.BadGzipFile: CRC check failed 0x0 != 0xd0c5998f\n"
     ]
    }
   ],
   "source": [
    "# The third book cant be retrieved because of faults in retrieval. This happens sometimes.\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    super_cleaner(load_etext(14575), -1, verify_deletions=True)\n",
    "except Exception as e:\n",
    "    try:\n",
    "        exc_info = sys.exc_info()\n",
    "    finally:\n",
    "        # Display the *original* exception\n",
    "        traceback.print_exception(*exc_info)\n",
    "        del exc_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12640"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve how many english books there are actually loadable\n",
    "#If books arent cached this may take a while because it needs to scrape the books from gutenberg.org\n",
    "#Therefore i provide a pre-processed file\n",
    "if os.path.isfile(cache_dir + 'loadable_english_book_keys.pkl'):\n",
    "    with open(cache_dir + 'loadable_english_book_keys.pkl', 'rb') as f:\n",
    "        loadable_english_book_keys = pickle.load(f)              \n",
    "else:\n",
    "    loadable_english_book_keys = []\n",
    "    i = 0\n",
    "    for key in english_book_keys:\n",
    "        if i % 1000 == 0:\n",
    "            print(i, datetime.now())\n",
    "        i += 1\n",
    "        try:\n",
    "            load_etext(int(key))\n",
    "            loadable_english_book_keys.append(key)\n",
    "        except:\n",
    "            continue\n",
    "    with open(cache_dir + 'loadable_english_book_keys.pkl', 'wb') as f:\n",
    "        pickle.dump(loadable_english_book_keys, f)\n",
    "            \n",
    "len(loadable_english_book_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['17255', '1742', '14870', '14596', '23436', '22563', '15306', '15976', '1344', '13579']\n",
      "['15116', '23050', '22669', '22310', '18782', '10343', '1650', '21698', '16831', '11194', '14752', '14429', '16170', '2078', '13766', '12310', '23892', '16144', '22293', '19224']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly select 10 books that we can query\n",
    "np.random.seed(42)\n",
    "rand_10_books = [x for x in np.random.choice(loadable_english_book_keys, size=10)]\n",
    "rand_20_books = [x for x in np.random.choice(loadable_english_book_keys, size=20)]\n",
    "print(rand_10_books), print(rand_20_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17255 ['Alma-Tadema, Laurence'] ['The Wings of Icarus: Being the Life of one Emilia Fletcher']\n",
      "1742 ['Davis, Richard Harding'] ['Miss Civilization: A Comedy in One Act']\n",
      "14870 ['Hopkinson, Alfred, Sir'] ['Rebuilding Britain: A Survey of Problems of Reconstruction After the World War']\n",
      "14596 ['Inge, William Ralph'] ['Christian Mysticism']\n",
      "23436 ['Anonymous'] ['Aladdin or The Wonderful Lamp']\n"
     ]
    }
   ],
   "source": [
    "# Titles and authors for the first 5 books\n",
    "# 1 book isnt actually loadable, see below.\n",
    "for book_id in rand_10_books[:5]:\n",
    "    print(book_id, metadata[book_id]['author'], metadata[book_id]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing\n",
    "==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of John Gutenberg, by Franz von Dingelstedt\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org/license\\r\\n\\r\\n\\r\\nTitle: John Gutenberg\\r\\n       First Master Printer, His Acts and Most Remarkable\\r\\n       Discourses and his Death\\r\\n\\r\\nAuthor: Franz von Dingelstedt\\r\\n\\r\\nRelease Da'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original unprocessed text\n",
    "text = load_etext(50000)[:500]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of John Gutenberg, by Franz von Dingelstedt\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.org/license\r\n",
      "\r\n",
      "\r\n",
      "Title: John Gutenberg\r\n",
      "       First Master Printer, His Acts and Most Remarkable\r\n",
      "       Discourses and his Death\r\n",
      "\r\n",
      "Author: Franz von Dingelstedt\r\n",
      "\r\n",
      "Release Da\n"
     ]
    }
   ],
   "source": [
    "#Text with formatting\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cleaner to retrieve cleaned text from the first book of the random selection.\n",
    "The _super_cleaner_ strips a headers/disclaimers/tables that are not required for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = super_cleaner(load_etext(16968), -1, verify_deletions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \"And now for business,\" Lopez said. \"And remember zat he what tells a lie shall be right away shotted.\" In his excitement he lost the little English he had.',\n",
       " ' \"Put all ze men outside,\" Lopez ordered. Venustiano and Pedro, his chief lieutenants, obeyed at once, forcing them to march ahead of them, and standing guard over them near a great cactus bush a few feet from the adobe. \"Leave ze women with me,\" the bandit continued. \"But first, Alvarada, you find ze cook. I am \\'ongry.\"',\n",
       " ' \"Red\" Giddings had been on the ranch with Gilbert since the very beginning. He came from the North with the young man, willing to stake all on this one venture. Like young Jones, he was not afraid. He was an efficient, well-set-up young fellow, with three consuming passions: Arizona, his harmonica, and Angela Hardy. The first saw a lot of \"Red\"; the second touched his lips frequently; but as for Angela--well, perhaps the poor boy kissed his harmonica so often in order to forget her lips. But if his own music charmed \"Red,\" it failed to have that effect upon others--particularly Uncle Henry, who went into a rage whenever he heard the detested instrument. \"Red\\'s\" music had no charms to soothe the savage breast of Henry Smith.',\n",
       " ' \"What\\'s coming off?\" Gilbert said, looking about him, and not a little surprised to find a Mexican and his adherents in his adobe.',\n",
       " ' A heavy silence fell upon the men who were left in the room. The bandit, unconcerned, puffed his cigarette. Hardy and Pell felt like rats in a trap. Only Uncle Henry was passive. In the tense stillness, the clock could be heard ticking on and on. Pell was beginning to crack beneath the strain. Suddenly he began to pace the floor, his hands behind his back. No tiger in a cage was ever more impatient in his captivity.',\n",
       " ' A story of the Werewolves, made wonderfully credible and told with great skill and feeling. This is far from being an ordinary detective novel. Mr. Biss is on brand new ground and will puzzle every reader till the mystery is at last solved by the right man--the mystery of the baffling murders on the Brighton road.',\n",
       " ' A well-known English critic said of The Untamed--\"There are in it passages of extraordinary power--the whole conception is very bold.\" And no less bold nor less powerful is its sequel The Night Horseman. Once again we ride in company with \"Whistlin\\' Dan,\" the fearless, silent, mysterious chap who shares the instincts of wild things, and once again we engage with him in his desperate adventures, hair-breadth escapes, and whirlwind triumphs. A novel thrilling in its reality, which will not be put down by lovers of exciting fiction.',\n",
       " ' Deeper and deeper grew the darkness. Outside, indeed, the first stars had begun to shine, and soon the heavens were a miraculous glory. But there was no moon. Every road was hushed, and the trees waved their long arms in the gloom. The little machine that took Angela and her father home, rolled down the quiet valley. Its chug-chug was the only sound for miles around. \"Red\" was happy in the cool night. He rode all the way out to the Hardy ranch. He and Angela sang an old song, and let Jasper Hardy sit at the wheel and whirl them to the lights of home.',\n",
       " \" Immediately after, Lucia came in. She saw the body of her husband, the legs drawn up a bit, the arms stretched out, the wounded head turned so that the blood flowing from the forehead could not be seen. Only a few moments before, this limp, pitiful object had been speaking to her--calling her by name. It seemed incredible that Pell was powerless now to harm her. Brute though he had been, he gained, in this awesome instant, a strange glory, as the dead always do. The splendor of that universal experience was suddenly his; and, even lying there like a discarded meal-sack, he took on something of the pomp of a cardinal who had died. Never, of course, had she respected him more; and though she could not bring herself to shed a tear, she looked down at the still body, huddled in a heap, and craved one more word with him. No matter what has happened between a man and a woman; no matter what tragic hours they have known, when the moment of separation comes, there is always that wish to have explained a little more, to have taken a different course in all one's previous actions. It was not that she blamed herself; she had nothing on her conscience. But there was an instinctive dread at meeting the certain pain of this crisis.\",\n",
       " ' It was high noon, two days later. Gilbert again had been about the ranch looking things over. He had his dreamy moments, but he was far too practical to let the poet in him rule his life. One sensed, by the most cursory glance, that here was a type of virile young American who could not only dream, but make his dreams come true. No idler he! And he had no use for idlers. He had dared to come to this far country, establish himself on a ranch, and seek to win out in the face of overwhelming odds.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text is now a list of paragraphs\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"No.\"',\n",
       " '\"Gun?\"',\n",
       " '\"Why?\"',\n",
       " '\"Yes.\"',\n",
       " '\"Pells?\"',\n",
       " '\"A what?\"',\n",
       " '\"I have?\"',\n",
       " '\"Joking?\"',\n",
       " '\"Really?\"',\n",
       " '\"I ain\\'t!\"',\n",
       " '\"Kiss me!\"',\n",
       " '\"Uh--huh!\"',\n",
       " '\"In a way.\"',\n",
       " '\"What for?\"',\n",
       " '\"Yes, sir!\"',\n",
       " '\"Yes; why?\"',\n",
       " 'She nodded.',\n",
       " '\"All those?\"',\n",
       " '\"You won\\'t?\"',\n",
       " 'She started.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with some short sentences\n",
    "sorted(sentences, key=len)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \"And now for business,\" Lopez said. \"And remember zat he what tells a lie shall be right away shotted.\" In his excitement he lost the little English he had.',\n",
       " ' \"Put all ze men outside,\" Lopez ordered. Venustiano and Pedro, his chief lieutenants, obeyed at once, forcing them to march ahead of them, and standing guard over them near a great cactus bush a few feet from the adobe. \"Leave ze women with me,\" the bandit continued. \"But first, Alvarada, you find ze cook. I am \\'ongry.\"',\n",
       " ' \"Red\" Giddings had been on the ranch with Gilbert since the very beginning. He came from the North with the young man, willing to stake all on this one venture. Like young Jones, he was not afraid. He was an efficient, well-set-up young fellow, with three consuming passions: Arizona, his harmonica, and Angela Hardy. The first saw a lot of \"Red\"; the second touched his lips frequently; but as for Angela--well, perhaps the poor boy kissed his harmonica so often in order to forget her lips. But if his own music charmed \"Red,\" it failed to have that effect upon others--particularly Uncle Henry, who went into a rage whenever he heard the detested instrument. \"Red\\'s\" music had no charms to soothe the savage breast of Henry Smith.',\n",
       " ' \"What\\'s coming off?\" Gilbert said, looking about him, and not a little surprised to find a Mexican and his adherents in his adobe.',\n",
       " ' A heavy silence fell upon the men who were left in the room. The bandit, unconcerned, puffed his cigarette. Hardy and Pell felt like rats in a trap. Only Uncle Henry was passive. In the tense stillness, the clock could be heard ticking on and on. Pell was beginning to crack beneath the strain. Suddenly he began to pace the floor, his hands behind his back. No tiger in a cage was ever more impatient in his captivity.',\n",
       " ' A story of the Werewolves, made wonderfully credible and told with great skill and feeling. This is far from being an ordinary detective novel. Mr. Biss is on brand new ground and will puzzle every reader till the mystery is at last solved by the right man--the mystery of the baffling murders on the Brighton road.',\n",
       " ' A well-known English critic said of The Untamed--\"There are in it passages of extraordinary power--the whole conception is very bold.\" And no less bold nor less powerful is its sequel The Night Horseman. Once again we ride in company with \"Whistlin\\' Dan,\" the fearless, silent, mysterious chap who shares the instincts of wild things, and once again we engage with him in his desperate adventures, hair-breadth escapes, and whirlwind triumphs. A novel thrilling in its reality, which will not be put down by lovers of exciting fiction.',\n",
       " ' Deeper and deeper grew the darkness. Outside, indeed, the first stars had begun to shine, and soon the heavens were a miraculous glory. But there was no moon. Every road was hushed, and the trees waved their long arms in the gloom. The little machine that took Angela and her father home, rolled down the quiet valley. Its chug-chug was the only sound for miles around. \"Red\" was happy in the cool night. He rode all the way out to the Hardy ranch. He and Angela sang an old song, and let Jasper Hardy sit at the wheel and whirl them to the lights of home.',\n",
       " \" Immediately after, Lucia came in. She saw the body of her husband, the legs drawn up a bit, the arms stretched out, the wounded head turned so that the blood flowing from the forehead could not be seen. Only a few moments before, this limp, pitiful object had been speaking to her--calling her by name. It seemed incredible that Pell was powerless now to harm her. Brute though he had been, he gained, in this awesome instant, a strange glory, as the dead always do. The splendor of that universal experience was suddenly his; and, even lying there like a discarded meal-sack, he took on something of the pomp of a cardinal who had died. Never, of course, had she respected him more; and though she could not bring herself to shed a tear, she looked down at the still body, huddled in a heap, and craved one more word with him. No matter what has happened between a man and a woman; no matter what tragic hours they have known, when the moment of separation comes, there is always that wish to have explained a little more, to have taken a different course in all one's previous actions. It was not that she blamed herself; she had nothing on her conscience. But there was an instinctive dread at meeting the certain pain of this crisis.\",\n",
       " ' It was high noon, two days later. Gilbert again had been about the ranch looking things over. He had his dreamy moments, but he was far too practical to let the poet in him rule his life. One sensed, by the most cursory glance, that here was a type of virile young American who could not only dream, but make his dreams come true. No idler he! And he had no use for idlers. He had dared to come to this far country, establish himself on a ranch, and seek to win out in the face of overwhelming odds.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sentences)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2041, 5, 1532, 75140, 353]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find some properties about the book\n",
    "book_properties(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization in practice\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize custom tokenizer\n",
    "ST_tokenizer = StrategizedTokenizer(padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,  2253,  2000,  1996,   103,   103,   103,   103,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,   103,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,   103,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,   103,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,   103,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,   103,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,   103,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "           103,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,   103,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,   103,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "           103,   102],\n",
       "        [  101,  4776,  2175,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  1045,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1051,  1005,  5119,  1019,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = ST_tokenizer.tokenize(\"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', 'went', 'to', 'the', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', '[MASK]', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', '[MASK]', 'some', 'milk', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', '[MASK]', 'the', 'albert', 'he', '##ij', '##n', '[MASK]', '5', 'o', \"'\", 'clock', '[MASK]', 'buy', 'some', 'milk', '[MASK]', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', '[MASK]', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', '[MASK]', 'milk', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '[MASK]', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', '[MASK]', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', '[MASK]', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', '[MASK]', 'buy', 'some', 'milk', 'for', 'me', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', '[MASK]', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'me', '[MASK]', '[SEP]']\n",
      "['[CLS]', 'anne', 'go', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', '5', 'o', \"'\", 'clock', 'to', 'buy', 'some', 'milk', 'for', 'i', '.', '[SEP]']\n",
      "['[CLS]', 'anne', 'went', 'to', 'the', 'albert', 'he', '##ij', '##n', 'at', 'o', \"'\", 'clock', '5', 'to', 'buy', 'some', 'milk', 'for', 'me', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#Masks are at different places\n",
    "for masked_line in ST_tokenizer.convert_ids_to_tokens(inputs['input_ids']):\n",
    "    print(masked_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load original tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [MASK] went to the [MASK] [MASK] [MASK] [MASK] at 5 o ' clock to buy some milk for me . [SEP]\n",
      "[CLS] anne [MASK] to the albert heijn at 5 o ' clock to [MASK] some milk for me . [SEP]\n",
      "[CLS] anne went [MASK] the albert heijn [MASK] 5 o ' clock [MASK] buy some milk [MASK] me . [SEP]\n",
      "[CLS] anne went to [MASK] albert heijn at 5 o ' clock to buy [MASK] milk for me . [SEP]\n",
      "[CLS] anne went to the albert heijn at [MASK] o ' clock to buy some milk for me . [SEP]\n",
      "[CLS] anne went to the albert heijn at 5 o ' clock to buy some [MASK] for me . [SEP]\n",
      "[CLS] anne went [MASK] the albert heijn at 5 o ' clock [MASK] buy some milk for me . [SEP]\n",
      "[CLS] anne went to the albert heijn at 5 o ' clock to buy some milk for [MASK] . [SEP]\n",
      "[CLS] anne went to the albert heijn at 5 o ' clock to buy some milk for me [MASK] [SEP]\n",
      "[CLS] anne go to the albert heijn at 5 o ' clock to buy some milk for i . [SEP]\n",
      "[CLS] anne went to the albert heijn at o ' clock 5 to buy some milk for me . [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in ST_tokenizer.convert_ids_to_tokens(inputs['input_ids']):\n",
    "    print(tokenizer.convert_tokens_to_string(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gutenberg book-selection\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-19 13:18:53.438601\n",
      "2021-05-19 13:19:21.767784\n"
     ]
    }
   ],
   "source": [
    "#Setting to ignore warnings about sequences being longer than BERT can handle\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "df_books_10 = make_df_book_properties(rand_10_books)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Shortest sentence (char)</th>\n",
       "      <th>Longest sentence (char)</th>\n",
       "      <th>Total tokens</th>\n",
       "      <th>Longest sequence (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17255</td>\n",
       "      <td>677</td>\n",
       "      <td>8</td>\n",
       "      <td>2083</td>\n",
       "      <td>41672</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1742</td>\n",
       "      <td>239</td>\n",
       "      <td>15</td>\n",
       "      <td>1051</td>\n",
       "      <td>8481</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14870</td>\n",
       "      <td>317</td>\n",
       "      <td>22</td>\n",
       "      <td>5189</td>\n",
       "      <td>62696</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14596</td>\n",
       "      <td>528</td>\n",
       "      <td>18</td>\n",
       "      <td>7685</td>\n",
       "      <td>115181</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23436</td>\n",
       "      <td>11</td>\n",
       "      <td>41</td>\n",
       "      <td>293</td>\n",
       "      <td>579</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22563</td>\n",
       "      <td>358</td>\n",
       "      <td>6</td>\n",
       "      <td>2344</td>\n",
       "      <td>16372</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15306</td>\n",
       "      <td>390</td>\n",
       "      <td>9</td>\n",
       "      <td>1672</td>\n",
       "      <td>40223</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15976</td>\n",
       "      <td>1419</td>\n",
       "      <td>10</td>\n",
       "      <td>1724</td>\n",
       "      <td>78126</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1344</td>\n",
       "      <td>263</td>\n",
       "      <td>6</td>\n",
       "      <td>6391</td>\n",
       "      <td>27092</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13579</td>\n",
       "      <td>870</td>\n",
       "      <td>13</td>\n",
       "      <td>1939</td>\n",
       "      <td>65500</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id num_sentences Shortest sentence (char) Longest sentence (char)  \\\n",
       "0   17255           677                        8                    2083   \n",
       "1    1742           239                       15                    1051   \n",
       "2   14870           317                       22                    5189   \n",
       "3   14596           528                       18                    7685   \n",
       "4   23436            11                       41                     293   \n",
       "5   22563           358                        6                    2344   \n",
       "6   15306           390                        9                    1672   \n",
       "7   15976          1419                       10                    1724   \n",
       "8    1344           263                        6                    6391   \n",
       "9   13579           870                       13                    1939   \n",
       "\n",
       "  Total tokens Longest sequence (tokens)  \n",
       "0        41672                       522  \n",
       "1         8481                       253  \n",
       "2        62696                      1022  \n",
       "3       115181                      1887  \n",
       "4          579                        71  \n",
       "5        16372                       530  \n",
       "6        40223                       383  \n",
       "7        78126                       453  \n",
       "8        27092                      1536  \n",
       "9        65500                       447  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Shortest sentence (char)</th>\n",
       "      <th>Longest sentence (char)</th>\n",
       "      <th>Total tokens</th>\n",
       "      <th>Longest sequence (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22563</td>\n",
       "      <td>358</td>\n",
       "      <td>6</td>\n",
       "      <td>2344</td>\n",
       "      <td>16372</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1344</td>\n",
       "      <td>263</td>\n",
       "      <td>6</td>\n",
       "      <td>6391</td>\n",
       "      <td>27092</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17255</td>\n",
       "      <td>677</td>\n",
       "      <td>8</td>\n",
       "      <td>2083</td>\n",
       "      <td>41672</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15306</td>\n",
       "      <td>390</td>\n",
       "      <td>9</td>\n",
       "      <td>1672</td>\n",
       "      <td>40223</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15976</td>\n",
       "      <td>1419</td>\n",
       "      <td>10</td>\n",
       "      <td>1724</td>\n",
       "      <td>78126</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13579</td>\n",
       "      <td>870</td>\n",
       "      <td>13</td>\n",
       "      <td>1939</td>\n",
       "      <td>65500</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1742</td>\n",
       "      <td>239</td>\n",
       "      <td>15</td>\n",
       "      <td>1051</td>\n",
       "      <td>8481</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14596</td>\n",
       "      <td>528</td>\n",
       "      <td>18</td>\n",
       "      <td>7685</td>\n",
       "      <td>115181</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14870</td>\n",
       "      <td>317</td>\n",
       "      <td>22</td>\n",
       "      <td>5189</td>\n",
       "      <td>62696</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23436</td>\n",
       "      <td>11</td>\n",
       "      <td>41</td>\n",
       "      <td>293</td>\n",
       "      <td>579</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id num_sentences Shortest sentence (char) Longest sentence (char)  \\\n",
       "5   22563           358                        6                    2344   \n",
       "8    1344           263                        6                    6391   \n",
       "0   17255           677                        8                    2083   \n",
       "6   15306           390                        9                    1672   \n",
       "7   15976          1419                       10                    1724   \n",
       "9   13579           870                       13                    1939   \n",
       "1    1742           239                       15                    1051   \n",
       "3   14596           528                       18                    7685   \n",
       "2   14870           317                       22                    5189   \n",
       "4   23436            11                       41                     293   \n",
       "\n",
       "  Total tokens Longest sequence (tokens)  \n",
       "5        16372                       530  \n",
       "8        27092                      1536  \n",
       "0        41672                       522  \n",
       "6        40223                       383  \n",
       "7        78126                       453  \n",
       "9        65500                       447  \n",
       "1         8481                       253  \n",
       "3       115181                      1887  \n",
       "2        62696                      1022  \n",
       "4          579                        71  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sort df and account for the fact that the column has both text and numbers\n",
    "df_books_10.sort_values(by='Shortest sentence (char)')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Shortest sentence (char)</th>\n",
       "      <th>Longest sentence (char)</th>\n",
       "      <th>Total tokens</th>\n",
       "      <th>Longest sequence (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14596</td>\n",
       "      <td>528</td>\n",
       "      <td>18</td>\n",
       "      <td>7685</td>\n",
       "      <td>115181</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15976</td>\n",
       "      <td>1419</td>\n",
       "      <td>10</td>\n",
       "      <td>1724</td>\n",
       "      <td>78126</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13579</td>\n",
       "      <td>870</td>\n",
       "      <td>13</td>\n",
       "      <td>1939</td>\n",
       "      <td>65500</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14870</td>\n",
       "      <td>317</td>\n",
       "      <td>22</td>\n",
       "      <td>5189</td>\n",
       "      <td>62696</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17255</td>\n",
       "      <td>677</td>\n",
       "      <td>8</td>\n",
       "      <td>2083</td>\n",
       "      <td>41672</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id num_sentences Shortest sentence (char) Longest sentence (char)  \\\n",
       "3   14596           528                       18                    7685   \n",
       "7   15976          1419                       10                    1724   \n",
       "9   13579           870                       13                    1939   \n",
       "2   14870           317                       22                    5189   \n",
       "0   17255           677                        8                    2083   \n",
       "\n",
       "  Total tokens Longest sequence (tokens)  \n",
       "3       115181                      1887  \n",
       "7        78126                       453  \n",
       "9        65500                       447  \n",
       "2        62696                      1022  \n",
       "0        41672                       522  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some books have very few tokens.\n",
    "df_books_10.sort_values(by='Total tokens', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Shortest sentence (char)</th>\n",
       "      <th>Longest sentence (char)</th>\n",
       "      <th>Total tokens</th>\n",
       "      <th>Longest sequence (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17255</td>\n",
       "      <td>677</td>\n",
       "      <td>8</td>\n",
       "      <td>2083</td>\n",
       "      <td>41672</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14870</td>\n",
       "      <td>317</td>\n",
       "      <td>22</td>\n",
       "      <td>5189</td>\n",
       "      <td>62696</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13579</td>\n",
       "      <td>870</td>\n",
       "      <td>13</td>\n",
       "      <td>1939</td>\n",
       "      <td>65500</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15976</td>\n",
       "      <td>1419</td>\n",
       "      <td>10</td>\n",
       "      <td>1724</td>\n",
       "      <td>78126</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14596</td>\n",
       "      <td>528</td>\n",
       "      <td>18</td>\n",
       "      <td>7685</td>\n",
       "      <td>115181</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id num_sentences Shortest sentence (char) Longest sentence (char)  \\\n",
       "0   17255           677                        8                    2083   \n",
       "2   14870           317                       22                    5189   \n",
       "9   13579           870                       13                    1939   \n",
       "7   15976          1419                       10                    1724   \n",
       "3   14596           528                       18                    7685   \n",
       "\n",
       "  Total tokens Longest sequence (tokens)  \n",
       "0        41672                       522  \n",
       "2        62696                      1022  \n",
       "9        65500                       447  \n",
       "7        78126                       453  \n",
       "3       115181                      1887  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books_10.sort_values(by='Total tokens').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-19 13:19:21.870087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:11<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-19 13:19:36.536246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Retrieve token occurences per book in a dataframe and another dataframe with total number of tokens\n",
    "print(datetime.now())\n",
    "df_book_token_freq_10, df_10_total_tokens = make_book_token_frequency(rand_10_books)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[PAD]</th>\n",
       "      <th>[unused0]</th>\n",
       "      <th>[unused1]</th>\n",
       "      <th>[unused2]</th>\n",
       "      <th>[unused3]</th>\n",
       "      <th>[unused4]</th>\n",
       "      <th>[unused5]</th>\n",
       "      <th>[unused6]</th>\n",
       "      <th>[unused7]</th>\n",
       "      <th>[unused8]</th>\n",
       "      <th>...</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17255</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14870</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23436</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22563</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15306</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15976</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13579</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  30522 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      [PAD] [unused0] [unused1] [unused2] [unused3] [unused4] [unused5]  \\\n",
       "17255     0         0         0         0         0         0         0   \n",
       "1742      0         0         0         0         0         0         0   \n",
       "14870     0         0         0         0         0         0         0   \n",
       "14596     0         0         0         0         0         0         0   \n",
       "23436     0         0         0         0         0         0         0   \n",
       "22563     0         0         0         0         0         0         0   \n",
       "15306     0         0         0         0         0         0         0   \n",
       "15976     0         0         0         0         0         0         0   \n",
       "1344      0         0         0         0         0         0         0   \n",
       "13579     0         0         0         0         0         0         0   \n",
       "\n",
       "      [unused6] [unused7] [unused8]  ... ## ## ## ## ## ## ## ## ##  \\\n",
       "17255         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "1742          0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "14870         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "14596         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "23436         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "22563         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "15306         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "15976         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "1344          0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "13579         0         0         0  ...   0   0   0   0   0   0   0   0   0   \n",
       "\n",
       "      ##  \n",
       "17255   0  \n",
       "1742    0  \n",
       "14870   0  \n",
       "14596   0  \n",
       "23436   0  \n",
       "22563   0  \n",
       "15306   0  \n",
       "15976   0  \n",
       "1344    0  \n",
       "13579   0  \n",
       "\n",
       "[10 rows x 30522 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame is obviously very sparse\n",
    "df_book_token_freq_10[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17255     41679.0\n",
       "1742       8491.0\n",
       "14870     62696.0\n",
       "14596    115181.0\n",
       "23436       579.0\n",
       "22563     16372.0\n",
       "15306     40223.0\n",
       "15976     78170.0\n",
       "1344      27092.0\n",
       "13579     65500.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of tokens per book\n",
    "df_10_total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455983.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total number of tokens in our small set\n",
    "df_10_total_tokens.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  999,  1000,  1002, ..., 29645, 29664, 29667], dtype=int64), 15198)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All tokens which are present in our subsample of 20 books\n",
    "all_present_tokens_10 = all_available_tokens_from_df(df_book_token_freq_10)\n",
    "all_present_tokens_10, len(all_present_tokens_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('17255',\n",
       "  {'tokens': array([  999,  1000,  1005, ..., 29591, 29602, 29667], dtype=int64),\n",
       "   'total_tokens': 41679.0}),\n",
       " ('1742',\n",
       "  {'tokens': array([  999,  1000,  1005, ..., 28838, 29122, 29586], dtype=int64),\n",
       "   'total_tokens': 8491.0}),\n",
       " ('14870',\n",
       "  {'tokens': array([  999,  1000,  1005, ..., 29598, 29602, 29609], dtype=int64),\n",
       "   'total_tokens': 62696.0})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show first 3 entries\n",
    "tokens_per_book_10 = token_freq_df_to_dict(df_book_token_freq_10, df_10_total_tokens)\n",
    "take(3, tokens_per_book_10.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book best:  22563 new tokens:  3650 book_total_tokens:  16372.0 ratio:  0.22294160762277057\n",
      "book best:  23436 new tokens:  125 book_total_tokens:  579.0 ratio:  0.2158894645941278\n",
      "book best:  1344 new tokens:  2211 book_total_tokens:  27092.0 ratio:  0.08161080761848516\n",
      "book best:  1742 new tokens:  368 book_total_tokens:  8491.0 ratio:  0.0433400070663055\n",
      "book best:  15306 new tokens:  2157 book_total_tokens:  40223.0 ratio:  0.053626034855679586\n",
      "{'subset_booklist': ['22563', '23436', '1344', '1742', '15306'], 'subset_total_tokens': 92757.0, 'subset_present_tokens': array([  999.,  1000.,  1005., ..., 29602., 29664., 29667.]), 'subset_unique_tokens': 8511}\n"
     ]
    }
   ],
   "source": [
    "print(optimize_book_subset_ratio(all_present_tokens_10, tokens_per_book_10, threshold = 1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[PAD]</th>\n",
       "      <th>[unused0]</th>\n",
       "      <th>[unused1]</th>\n",
       "      <th>[unused2]</th>\n",
       "      <th>[unused3]</th>\n",
       "      <th>[unused4]</th>\n",
       "      <th>[unused5]</th>\n",
       "      <th>[unused6]</th>\n",
       "      <th>[unused7]</th>\n",
       "      <th>[unused8]</th>\n",
       "      <th>...</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "      <th>##</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30522 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    [PAD]  [unused0]  [unused1]  [unused2]  [unused3]  [unused4]  [unused5]  \\\n",
       "1       0          0          0          0          0          0          0   \n",
       "10      0          0          0          0          0          0          0   \n",
       "11      0          0          0          0          0          0          0   \n",
       "12      0          0          0          0          0          0          0   \n",
       "13      0          0          0          0          0          0          0   \n",
       "\n",
       "    [unused6]  [unused7]  [unused8]  ...  ##  ##  ##  ##  ##  ##  ##  \\\n",
       "1           0          0          0  ...    0    0    0    0    0    0    0   \n",
       "10          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "11          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "12          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "13          0          0          0  ...    0    0    0    0    0    0    0   \n",
       "\n",
       "    ##  ##  ##  \n",
       "1     0    0    0  \n",
       "10    0    0    0  \n",
       "11    0    0    0  \n",
       "12    0    0    0  \n",
       "13    0    0    0  \n",
       "\n",
       "[5 rows x 30522 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_book_token_freq = pd.read_csv(os.path.join('../LessIsMore-cache','df_book_token_freq.csv'), index_col=0)\n",
    "df_book_token_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27833"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total number of unique tokens in the data\n",
    "len(np.flatnonzero(df_book_token_freq.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       7640.0\n",
       "10    901551.0\n",
       "11     36249.0\n",
       "12     40831.0\n",
       "13      6731.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total_tokens = pd.read_csv(os.path.join('../LessIsMore-cache','df_total_tokens.csv'), index_col=0).squeeze()\n",
    "df_total_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23398     0.0\n",
       "10802     0.0\n",
       "23524     0.0\n",
       "2305      0.0\n",
       "232       2.0\n",
       "22818     4.0\n",
       "19937    15.0\n",
       "22335    22.0\n",
       "20086    25.0\n",
       "23147    34.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total_tokens.sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True \n",
      "True \n",
      "True \n",
      "True \n",
      "True  _INFANT'S CABINET_\n",
      "True  _OF_\n",
      "True  BIRDS & BEASTS.\n",
      "True  _LONDON. Printed & Sold by Harvey & Darton._ 55, Gracechurch Street, 1820. Price 6d.\n",
      "True  [Illustration: The Stork.]\n",
      "True  [Illustration: The Robin.]\n",
      "True  [Illustration: The Hyena.]\n",
      "True  [Illustration: The Lion.]\n",
      "True  [Illustration: The Rhinoceros.]\n",
      "True  [Illustration: The Camel.]\n",
      "True  [Illustration: The Swan.]\n",
      "True  [Illustration: The Vulture.]\n",
      "True  [Illustration: The Lark.]\n",
      "True  [Illustration: The Turkey.]\n",
      "True  [Illustration: The Fox.]\n",
      "True  [Illustration: The Greyhound.]\n",
      "True  [Illustration: The Elephant.]\n",
      "True  [Illustration: The Zebra.]\n",
      "True  [Illustration: The Crow.]\n",
      "True  [Illustration: The Cock.]\n",
      "True  [Illustration: The Pigeon.]\n",
      "True  [Illustration: The Goldfinch.]\n",
      "True  [Illustration: The Buffalo.]\n",
      "True  [Illustration: The Hog.]\n",
      "True  [Illustration: The Horse.]\n",
      "True  [Illustration: The Stag.]\n",
      "True  [Illustration: The Chaffinch.]\n",
      "True  [Illustration: The Peacock.]\n",
      "True  [Illustration: The Guinea Hen.]\n",
      "True  [Illustration: The Blackbird.]\n",
      "True  [Illustration: The Ox.]\n",
      "True  [Illustration: The Wolf.]\n",
      "True  [Illustration: The Tiger.]\n",
      "True  [Illustration: The Baboon.]\n",
      "True  [Illustration: The Sparrow.]\n",
      "True  [Illustration: The Eagle.]\n",
      "True \n",
      "True  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Why do certain books have 0 tokens?\n",
    "#Well because it is an illustration-only book\n",
    "super_cleaner(load_etext(23398), -1, verify_deletions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True to complying with copyright laws. PGLAF has not verified that all the eBook files on these discs meet the copyright laws in countries outside of the United States. PGLAF recommends that you verify this before using these files and requests that you advise us of any problems by email to copyright AT pglaf.org\n",
      "True ** A note on CD and DVD disc capacity. It turns out that disk drive manufacturers (including the people who make CD and DVD burners and blank discs) measure disk space differently than the rest of the computer world. To them, 1MB, which is 1 megabyte, is 1,000,000 bytes. For the rest of the computer world, 1MB is 1,046,576 bytes. We mention this because people might read their DVD disc package and expect it to hold 4.7GB, but be surprised to find it can only hold about 4.37GB as the rest of the world measures space.\n",
      "True  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or because it is a DVD-cover and we only use the .txt file\n",
    "super_cleaner(load_etext(10802), -1, verify_deletions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['produced from scanned images of public domain material from the Google Print project.)']\n",
      "['by Virgil']\n"
     ]
    }
   ],
   "source": [
    "#Some books just have very little parsable information. This is often the case with books that are really really old \n",
    "#(e.g. writtenpre 1800s). The english in these books is often much different than modern day english.\n",
    "\n",
    "print(super_cleaner(load_etext(19937), -1))\n",
    "print(super_cleaner(load_etext(232), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "939505600.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many tokens do we have in total available?\n",
    "df_total_tokens.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_100K.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_100K.pkl', 'rb') as f:\n",
    "        subset_ratio_100K = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subset_booklist': ['232', '22818', '22335', '23594', '20086', '20360', '10557', '19571', '19177', '14100', '13536', '23436', '129', '21783', '11006', '19937', '22847', '1321', '23147', '21805', '22529', '12474', '13082', '14463', '23538', '13081', '116', '18589', '23446', '23450', '17124', '16780', '23146', '18935', '12554', '17254', '23429', '13203', '17365', '22236', '16169', '18417', '22579', '19634', '24044', '104', '1567', '23315', '24269', '12358', '23880'], 'subset_total_tokens': 99974.0, 'subset_present_tokens': array([  100.,   999.,  1000., ..., 29735., 29737., 29739.]), 'subset_unique_tokens': 13040}\n"
     ]
    }
   ],
   "source": [
    "print(subset_ratio_100K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_1M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_1M.pkl', 'rb') as f:\n",
    "        subset_1M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_1M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_1M.pkl', 'rb') as f:\n",
    "        subset_ratio_1M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_10M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_10M.pkl', 'rb') as f:\n",
    "        subset_10M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_10M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_10M.pkl', 'rb') as f:\n",
    "        subset_ratio_10M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cache_dir + 'subset_meta_ratio_100M.pkl'):\n",
    "    with open(cache_dir + 'subset_meta_ratio_100M.pkl', 'rb') as f:\n",
    "        subset_ratio_100M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a file with the union of all books in the subsets (easy for processing on the server)\n",
    "\n",
    "union_subsets = {'subset_booklist': np.union1d(np.union1d(np.union1d(subset_ratio_100K['subset_booklist'],\n",
    "                                                          subset_ratio_1M['subset_booklist']),\n",
    "                                                          subset_ratio_10M['subset_booklist']), \n",
    "                                               subset_ratio_100M['subset_booklist'])}\n",
    "\n",
    "with open(os.path.join(cache_dir, 'subset_meta_ratio_union.pkl'), 'wb') as f:\n",
    "    pickle.dump(union_subsets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many tokens are actually represented by the data\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load original tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of the vocabulary\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocabulary contains a bunch of [unused] tokens which allow people to add their own tokens\n",
    "num_unused = 0\n",
    "for k,v in tokenizer.vocab.items():\n",
    "    if k.startswith('[unused'):\n",
    "        num_unused += 1\n",
    "num_unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1493"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for tokens which are either alone or a continued token, e.g. 'a' or '##a'\n",
    "num_char = 0\n",
    "for k,v in tokenizer.vocab.items():\n",
    "    #'a' or '##a'\n",
    "    if not v in subset_ratio_100M['subset_present_tokens'] and (len(k) == 1 or (len(k) == 3 and k.startswith('##'))):\n",
    "        num_char += 1\n",
    "        #print(k, v)\n",
    "num_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] 0\n",
      "[CLS] 101\n",
      "[SEP] 102\n",
      "[MASK] 103\n",
      "... 2133\n",
      "km 3186\n",
      "soundtrack 6050\n",
      "remix 6136\n",
      "c 6362\n",
      "uefa 6663\n",
      "playoff 7808\n",
      "midfielder 8850\n",
      "playstation 9160\n",
      "quarterfinals 9237\n",
      "pinyin 9973\n",
      "allmusic 10477\n",
      "mlb 10901\n",
      "espn 10978\n",
      "gameplay 11247\n",
      "nsw 11524\n",
      "nascar 11838\n",
      "itunes 11943\n",
      "lgbt 12010\n",
      "mvp 12041\n",
      "xbox 12202\n",
      "eurovision 12714\n",
      "vfl 13480\n",
      "kolkata 13522\n",
      "pga 14198\n",
      "m 14241\n",
      "bundesliga 14250\n",
      "metacritic 14476\n",
      "remixes 15193\n",
      "steelers 15280\n",
      "airplay 15341\n",
      "## 15414\n",
      "paralympics 15600\n",
      "zhao 15634\n",
      "reggae 15662\n",
      "linebacker 15674\n",
      "v8 15754\n",
      "hindwings 15998\n",
      "bollywood 16046\n",
      "podcast 16110\n",
      "atletico 16132\n",
      "wwf 16779\n",
      "transgender 16824\n",
      "paralympic 17029\n",
      "postseason 17525\n",
      "vhs 17550\n",
      "campeonato 17675\n",
      "multiplayer 17762\n",
      "odz 17814\n",
      "curated 17940\n",
      "iphone 18059\n",
      "gmbh 18289\n",
      "danielle 18490\n",
      "qaeda 18659\n",
      "mixtape 18713\n",
      " 18728\n",
      "##aw 19704\n",
      "##qing 19784\n",
      "saxophonist 19977\n",
      "preseason 20038\n",
      "pmid 20117\n",
      "keyboardist 20173\n",
      "iucn 20333\n",
      "pokemon 20421\n",
      "nrl 20686\n",
      "motorsports 20711\n",
      "jaenelle 20757\n",
      "beyonce 20773\n",
      "airbus 20901\n",
      "netflix 20907\n",
      "motorsport 21044\n",
      "belgarath 21256\n",
      "iaaf 21259\n",
      "guangdong 21287\n",
      "shortlisted 21353\n",
      "seahawks 21390\n",
      "lucivar 21401\n",
      "goaltender 21437\n",
      "nanjing 21455\n",
      "crambidae 21585\n",
      "frontman 21597\n",
      "cbn 21824\n",
      "odisha 21874\n",
      "alzheimer 21901\n",
      "kinase 21903\n",
      "futsal 21921\n",
      "wta 21925\n",
      "nokia 22098\n",
      "smackdown 22120\n",
      "concacaf 22169\n",
      "oricon 22237\n",
      "twenty20 22240\n",
      "nfc 22309\n",
      "tbilisi 22406\n",
      "nrhp 22424\n",
      "showcased 22443\n",
      "nmi 22484\n",
      "kaladin 22588\n",
      "storylines 22628\n",
      "## 22646\n",
      "riaa 22716\n",
      "##a 22972\n",
      "mitochondrial 23079\n",
      "haryana 23261\n",
      "##saw 23305\n",
      "issn 23486\n",
      "saetan 23515\n",
      "rbis 23583\n",
      "deportivo 23696\n",
      "neuroscience 23700\n",
      "telangana 23764\n",
      "lviv 23814\n",
      "scientology 23845\n",
      "bwv 23860\n",
      "transitioned 23946\n",
      "yamaha 24031\n",
      "maccabi 24055\n",
      "lexie 24123\n",
      "superfamily 24169\n",
      "hapoel 24208\n",
      "wcw 24215\n",
      "soundtracks 24245\n",
      "debuting 24469\n",
      "bmg 24499\n",
      "sequencing 24558\n",
      "ho 24833\n",
      "myspace 24927\n",
      "##strae 24967\n",
      "smashwords 25151\n",
      "wrocaw 25160\n",
      "cmll 25395\n",
      "rihanna 25439\n",
      "wnba 25554\n",
      "brianna 25558\n",
      "filmfare 25648\n",
      "hezbollah 25713\n",
      "cheerleading 25721\n",
      "telenovela 25754\n",
      "shandong 25768\n",
      "## 25799\n",
      "erebidae 25875\n",
      "superliga 25922\n",
      "hyundai 25983\n",
      "springsteen 26002\n",
      "euroleague 26093\n",
      "stanisaw 26133\n",
      "tianjin 26216\n",
      "vh1 26365\n",
      "smartphone 26381\n",
      "alyssa 26442\n",
      "detainees 26485\n",
      "kayla 26491\n",
      "benfica 26542\n",
      "shenzhen 26555\n",
      "godzilla 26631\n",
      "zhejiang 26805\n",
      "2010s 26817\n",
      "cornerback 26857\n",
      "capcom 26861\n",
      "iihf 26904\n",
      "mukherjee 27040\n",
      "polgara 27041\n",
      "joyah 27098\n",
      "multicultural 27135\n",
      "mbc 27262\n",
      "jillian 27286\n",
      "## 27392\n",
      "metadata 27425\n",
      "showcasing 27696\n",
      "minogue 27736\n",
      "##rae 27807\n",
      "##1 27944\n",
      "00pm 27995\n",
      "wrestlemania 28063\n",
      "1910s 28088\n",
      "jiangsu 28091\n",
      "iqbal 28111\n",
      "biomass 28148\n",
      "prequel 28280\n",
      "fivb 28423\n",
      "kaitlyn 28584\n",
      "goalscorer 28602\n",
      "scuba 28651\n",
      "##genase 28835\n",
      "mrna 28848\n",
      "dfb 28894\n",
      "gaddafi 28924\n",
      "ghz 29066\n",
      "donetsk 29151\n",
      "britney 29168\n",
      "mentoring 29192\n",
      "postdoctoral 29272\n",
      "sql 29296\n",
      "weightlifting 29305\n",
      "quarterfinal 29380\n",
      "signage 29404\n",
      "fujian 29551\n",
      "endelle 29581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Which tokens are not represented?\n",
    "num_unrepresented = 0\n",
    "for k,v in tokenizer.vocab.items():\n",
    "    if not v in subset_ratio_100M['subset_present_tokens'] and not k.startswith('[unused') and not len(k) <= 1 and not (len(k) == 3 and k.startswith('##')):\n",
    "        num_unrepresented += 1\n",
    "        print(k, v)\n",
    "num_unrepresented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] 0\n",
      "[unused0] 1\n",
      "[unused1] 2\n",
      "[unused2] 3\n",
      "[unused3] 4\n",
      "[unused4] 5\n",
      "[unused5] 6\n",
      "[unused6] 7\n",
      "[unused7] 8\n",
      "[unused8] 9\n",
      "[unused9] 10\n",
      "[unused10] 11\n",
      "[unused11] 12\n",
      "[unused12] 13\n",
      "[unused13] 14\n",
      "[unused14] 15\n",
      "[unused15] 16\n",
      "[unused16] 17\n",
      "[unused17] 18\n",
      "[unused18] 19\n",
      "[unused19] 20\n",
      "[unused20] 21\n",
      "[unused21] 22\n",
      "[unused22] 23\n",
      "[unused23] 24\n",
      "[unused24] 25\n",
      "[unused25] 26\n",
      "[unused26] 27\n",
      "[unused27] 28\n",
      "[unused28] 29\n",
      "[unused29] 30\n",
      "[unused30] 31\n",
      "[unused31] 32\n",
      "[unused32] 33\n",
      "[unused33] 34\n",
      "[unused34] 35\n",
      "[unused35] 36\n",
      "[unused36] 37\n",
      "[unused37] 38\n",
      "[unused38] 39\n",
      "[unused39] 40\n",
      "[unused40] 41\n",
      "[unused41] 42\n",
      "[unused42] 43\n",
      "[unused43] 44\n",
      "[unused44] 45\n",
      "[unused45] 46\n",
      "[unused46] 47\n",
      "[unused47] 48\n",
      "[unused48] 49\n",
      "[unused49] 50\n",
      "[unused50] 51\n",
      "[unused51] 52\n",
      "[unused52] 53\n",
      "[unused53] 54\n",
      "[unused54] 55\n",
      "[unused55] 56\n",
      "[unused56] 57\n",
      "[unused57] 58\n",
      "[unused58] 59\n",
      "[unused59] 60\n",
      "[unused60] 61\n",
      "[unused61] 62\n",
      "[unused62] 63\n",
      "[unused63] 64\n",
      "[unused64] 65\n",
      "[unused65] 66\n",
      "[unused66] 67\n",
      "[unused67] 68\n",
      "[unused68] 69\n",
      "[unused69] 70\n",
      "[unused70] 71\n",
      "[unused71] 72\n",
      "[unused72] 73\n",
      "[unused73] 74\n",
      "[unused74] 75\n",
      "[unused75] 76\n",
      "[unused76] 77\n",
      "[unused77] 78\n",
      "[unused78] 79\n",
      "[unused79] 80\n",
      "[unused80] 81\n",
      "[unused81] 82\n",
      "[unused82] 83\n",
      "[unused83] 84\n",
      "[unused84] 85\n",
      "[unused85] 86\n",
      "[unused86] 87\n",
      "[unused87] 88\n",
      "[unused88] 89\n",
      "[unused89] 90\n",
      "[unused90] 91\n",
      "[unused91] 92\n",
      "[unused92] 93\n",
      "[unused93] 94\n",
      "[unused94] 95\n",
      "[unused95] 96\n",
      "[unused96] 97\n",
      "[unused97] 98\n",
      "[unused98] 99\n",
      "[CLS] 101\n",
      "[SEP] 102\n",
      "[MASK] 103\n",
      "[unused99] 104\n",
      "[unused100] 105\n",
      "[unused101] 106\n",
      "[unused102] 107\n",
      "[unused103] 108\n",
      "[unused104] 109\n",
      "[unused105] 110\n",
      "[unused106] 111\n",
      "[unused107] 112\n",
      "[unused108] 113\n",
      "[unused109] 114\n",
      "[unused110] 115\n",
      "[unused111] 116\n",
      "[unused112] 117\n",
      "[unused113] 118\n",
      "[unused114] 119\n",
      "[unused115] 120\n",
      "[unused116] 121\n",
      "[unused117] 122\n",
      "[unused118] 123\n",
      "[unused119] 124\n",
      "[unused120] 125\n",
      "[unused121] 126\n",
      "[unused122] 127\n",
      "[unused123] 128\n",
      "[unused124] 129\n",
      "[unused125] 130\n",
      "[unused126] 131\n",
      "[unused127] 132\n",
      "[unused128] 133\n",
      "[unused129] 134\n",
      "[unused130] 135\n",
      "[unused131] 136\n",
      "[unused132] 137\n",
      "[unused133] 138\n",
      "[unused134] 139\n",
      "[unused135] 140\n",
      "[unused136] 141\n",
      "[unused137] 142\n",
      "[unused138] 143\n",
      "[unused139] 144\n",
      "[unused140] 145\n",
      "[unused141] 146\n",
      "[unused142] 147\n",
      "[unused143] 148\n",
      "[unused144] 149\n",
      "[unused145] 150\n",
      "[unused146] 151\n",
      "[unused147] 152\n",
      "[unused148] 153\n",
      "[unused149] 154\n",
      "[unused150] 155\n",
      "[unused151] 156\n",
      "[unused152] 157\n",
      "[unused153] 158\n",
      "[unused154] 159\n",
      "[unused155] 160\n",
      "[unused156] 161\n",
      "[unused157] 162\n",
      "[unused158] 163\n",
      "[unused159] 164\n",
      "[unused160] 165\n",
      "[unused161] 166\n",
      "[unused162] 167\n",
      "[unused163] 168\n",
      "[unused164] 169\n",
      "[unused165] 170\n",
      "[unused166] 171\n",
      "[unused167] 172\n",
      "[unused168] 173\n",
      "[unused169] 174\n",
      "[unused170] 175\n",
      "[unused171] 176\n",
      "[unused172] 177\n",
      "[unused173] 178\n",
      "[unused174] 179\n",
      "[unused175] 180\n",
      "[unused176] 181\n",
      "[unused177] 182\n",
      "[unused178] 183\n",
      "[unused179] 184\n",
      "[unused180] 185\n",
      "[unused181] 186\n",
      "[unused182] 187\n",
      "[unused183] 188\n",
      "[unused184] 189\n",
      "[unused185] 190\n",
      "[unused186] 191\n",
      "[unused187] 192\n",
      "[unused188] 193\n",
      "[unused189] 194\n",
      "[unused190] 195\n",
      "[unused191] 196\n",
      "[unused192] 197\n",
      "[unused193] 198\n",
      "[unused194] 199\n",
      "[unused195] 200\n",
      "[unused196] 201\n",
      "[unused197] 202\n",
      "[unused198] 203\n",
      "[unused199] 204\n",
      "[unused200] 205\n",
      "[unused201] 206\n",
      "[unused202] 207\n",
      "[unused203] 208\n",
      "[unused204] 209\n",
      "[unused205] 210\n",
      "[unused206] 211\n",
      "[unused207] 212\n",
      "[unused208] 213\n",
      "[unused209] 214\n",
      "[unused210] 215\n",
      "[unused211] 216\n",
      "[unused212] 217\n",
      "[unused213] 218\n",
      "[unused214] 219\n",
      "[unused215] 220\n",
      "[unused216] 221\n",
      "[unused217] 222\n",
      "[unused218] 223\n",
      "[unused219] 224\n",
      "[unused220] 225\n",
      "[unused221] 226\n",
      "[unused222] 227\n",
      "[unused223] 228\n",
      "[unused224] 229\n",
      "[unused225] 230\n",
      "[unused226] 231\n",
      "[unused227] 232\n",
      "[unused228] 233\n",
      "[unused229] 234\n",
      "[unused230] 235\n",
      "[unused231] 236\n",
      "[unused232] 237\n",
      "[unused233] 238\n",
      "[unused234] 239\n",
      "[unused235] 240\n",
      "[unused236] 241\n",
      "[unused237] 242\n",
      "[unused238] 243\n",
      "[unused239] 244\n",
      "[unused240] 245\n",
      "[unused241] 246\n",
      "[unused242] 247\n",
      "[unused243] 248\n",
      "[unused244] 249\n",
      "[unused245] 250\n",
      "[unused246] 251\n",
      "[unused247] 252\n",
      "[unused248] 253\n",
      "[unused249] 254\n",
      "[unused250] 255\n",
      "[unused251] 256\n",
      "[unused252] 257\n",
      "[unused253] 258\n",
      "[unused254] 259\n",
      "[unused255] 260\n",
      "[unused256] 261\n",
      "[unused257] 262\n",
      "[unused258] 263\n",
      "[unused259] 264\n",
      "[unused260] 265\n",
      "[unused261] 266\n",
      "[unused262] 267\n",
      "[unused263] 268\n",
      "[unused264] 269\n",
      "[unused265] 270\n",
      "[unused266] 271\n",
      "[unused267] 272\n",
      "[unused268] 273\n",
      "[unused269] 274\n",
      "[unused270] 275\n",
      "[unused271] 276\n",
      "[unused272] 277\n",
      "[unused273] 278\n",
      "[unused274] 279\n",
      "[unused275] 280\n",
      "[unused276] 281\n",
      "[unused277] 282\n",
      "[unused278] 283\n",
      "[unused279] 284\n",
      "[unused280] 285\n",
      "[unused281] 286\n",
      "[unused282] 287\n",
      "[unused283] 288\n",
      "[unused284] 289\n",
      "[unused285] 290\n",
      "[unused286] 291\n",
      "[unused287] 292\n",
      "[unused288] 293\n",
      "[unused289] 294\n",
      "[unused290] 295\n",
      "[unused291] 296\n",
      "[unused292] 297\n",
      "[unused293] 298\n",
      "[unused294] 299\n",
      "[unused295] 300\n",
      "[unused296] 301\n",
      "[unused297] 302\n",
      "[unused298] 303\n",
      "[unused299] 304\n",
      "[unused300] 305\n",
      "[unused301] 306\n",
      "[unused302] 307\n",
      "[unused303] 308\n",
      "[unused304] 309\n",
      "[unused305] 310\n",
      "[unused306] 311\n",
      "[unused307] 312\n",
      "[unused308] 313\n",
      "[unused309] 314\n",
      "[unused310] 315\n",
      "[unused311] 316\n",
      "[unused312] 317\n",
      "[unused313] 318\n",
      "[unused314] 319\n",
      "[unused315] 320\n",
      "[unused316] 321\n",
      "[unused317] 322\n",
      "[unused318] 323\n",
      "[unused319] 324\n",
      "[unused320] 325\n",
      "[unused321] 326\n",
      "[unused322] 327\n",
      "[unused323] 328\n",
      "[unused324] 329\n",
      "[unused325] 330\n",
      "[unused326] 331\n",
      "[unused327] 332\n",
      "[unused328] 333\n",
      "[unused329] 334\n",
      "[unused330] 335\n",
      "[unused331] 336\n",
      "[unused332] 337\n",
      "[unused333] 338\n",
      "[unused334] 339\n",
      "[unused335] 340\n",
      "[unused336] 341\n",
      "[unused337] 342\n",
      "[unused338] 343\n",
      "[unused339] 344\n",
      "[unused340] 345\n",
      "[unused341] 346\n",
      "[unused342] 347\n",
      "[unused343] 348\n",
      "[unused344] 349\n",
      "[unused345] 350\n",
      "[unused346] 351\n",
      "[unused347] 352\n",
      "[unused348] 353\n",
      "[unused349] 354\n",
      "[unused350] 355\n",
      "[unused351] 356\n",
      "[unused352] 357\n",
      "[unused353] 358\n",
      "[unused354] 359\n",
      "[unused355] 360\n",
      "[unused356] 361\n",
      "[unused357] 362\n",
      "[unused358] 363\n",
      "[unused359] 364\n",
      "[unused360] 365\n",
      "[unused361] 366\n",
      "[unused362] 367\n",
      "[unused363] 368\n",
      "[unused364] 369\n",
      "[unused365] 370\n",
      "[unused366] 371\n",
      "[unused367] 372\n",
      "[unused368] 373\n",
      "[unused369] 374\n",
      "[unused370] 375\n",
      "[unused371] 376\n",
      "[unused372] 377\n",
      "[unused373] 378\n",
      "[unused374] 379\n",
      "[unused375] 380\n",
      "[unused376] 381\n",
      "[unused377] 382\n",
      "[unused378] 383\n",
      "[unused379] 384\n",
      "[unused380] 385\n",
      "[unused381] 386\n",
      "[unused382] 387\n",
      "[unused383] 388\n",
      "[unused384] 389\n",
      "[unused385] 390\n",
      "[unused386] 391\n",
      "[unused387] 392\n",
      "[unused388] 393\n",
      "[unused389] 394\n",
      "[unused390] 395\n",
      "[unused391] 396\n",
      "[unused392] 397\n",
      "[unused393] 398\n",
      "[unused394] 399\n",
      "[unused395] 400\n",
      "[unused396] 401\n",
      "[unused397] 402\n",
      "[unused398] 403\n",
      "[unused399] 404\n",
      "[unused400] 405\n",
      "[unused401] 406\n",
      "[unused402] 407\n",
      "[unused403] 408\n",
      "[unused404] 409\n",
      "[unused405] 410\n",
      "[unused406] 411\n",
      "[unused407] 412\n",
      "[unused408] 413\n",
      "[unused409] 414\n",
      "[unused410] 415\n",
      "[unused411] 416\n",
      "[unused412] 417\n",
      "[unused413] 418\n",
      "[unused414] 419\n",
      "[unused415] 420\n",
      "[unused416] 421\n",
      "[unused417] 422\n",
      "[unused418] 423\n",
      "[unused419] 424\n",
      "[unused420] 425\n",
      "[unused421] 426\n",
      "[unused422] 427\n",
      "[unused423] 428\n",
      "[unused424] 429\n",
      "[unused425] 430\n",
      "[unused426] 431\n",
      "[unused427] 432\n",
      "[unused428] 433\n",
      "[unused429] 434\n",
      "[unused430] 435\n",
      "[unused431] 436\n",
      "[unused432] 437\n",
      "[unused433] 438\n",
      "[unused434] 439\n",
      "[unused435] 440\n",
      "[unused436] 441\n",
      "[unused437] 442\n",
      "[unused438] 443\n",
      "[unused439] 444\n",
      "[unused440] 445\n",
      "[unused441] 446\n",
      "[unused442] 447\n",
      "[unused443] 448\n",
      "[unused444] 449\n",
      "[unused445] 450\n",
      "[unused446] 451\n",
      "[unused447] 452\n",
      "[unused448] 453\n",
      "[unused449] 454\n",
      "[unused450] 455\n",
      "[unused451] 456\n",
      "[unused452] 457\n",
      "[unused453] 458\n",
      "[unused454] 459\n",
      "[unused455] 460\n",
      "[unused456] 461\n",
      "[unused457] 462\n",
      "[unused458] 463\n",
      "[unused459] 464\n",
      "[unused460] 465\n",
      "[unused461] 466\n",
      "[unused462] 467\n",
      "[unused463] 468\n",
      "[unused464] 469\n",
      "[unused465] 470\n",
      "[unused466] 471\n",
      "[unused467] 472\n",
      "[unused468] 473\n",
      "[unused469] 474\n",
      "[unused470] 475\n",
      "[unused471] 476\n",
      "[unused472] 477\n",
      "[unused473] 478\n",
      "[unused474] 479\n",
      "[unused475] 480\n",
      "[unused476] 481\n",
      "[unused477] 482\n",
      "[unused478] 483\n",
      "[unused479] 484\n",
      "[unused480] 485\n",
      "[unused481] 486\n",
      "[unused482] 487\n",
      "[unused483] 488\n",
      "[unused484] 489\n",
      "[unused485] 490\n",
      "[unused486] 491\n",
      "[unused487] 492\n",
      "[unused488] 493\n",
      "[unused489] 494\n",
      "[unused490] 495\n",
      "[unused491] 496\n",
      "[unused492] 497\n",
      "[unused493] 498\n",
      "[unused494] 499\n",
      "[unused495] 500\n",
      "[unused496] 501\n",
      "[unused497] 502\n",
      "[unused498] 503\n",
      "[unused499] 504\n",
      "[unused500] 505\n",
      "[unused501] 506\n",
      "[unused502] 507\n",
      "[unused503] 508\n",
      "[unused504] 509\n",
      "[unused505] 510\n",
      "[unused506] 511\n",
      "[unused507] 512\n",
      "[unused508] 513\n",
      "[unused509] 514\n",
      "[unused510] 515\n",
      "[unused511] 516\n",
      "[unused512] 517\n",
      "[unused513] 518\n",
      "[unused514] 519\n",
      "[unused515] 520\n",
      "[unused516] 521\n",
      "[unused517] 522\n",
      "[unused518] 523\n",
      "[unused519] 524\n",
      "[unused520] 525\n",
      "[unused521] 526\n",
      "[unused522] 527\n",
      "[unused523] 528\n",
      "[unused524] 529\n",
      "[unused525] 530\n",
      "[unused526] 531\n",
      "[unused527] 532\n",
      "[unused528] 533\n",
      "[unused529] 534\n",
      "[unused530] 535\n",
      "[unused531] 536\n",
      "[unused532] 537\n",
      "[unused533] 538\n",
      "[unused534] 539\n",
      "[unused535] 540\n",
      "[unused536] 541\n",
      "[unused537] 542\n",
      "[unused538] 543\n",
      "[unused539] 544\n",
      "[unused540] 545\n",
      "[unused541] 546\n",
      "[unused542] 547\n",
      "[unused543] 548\n",
      "[unused544] 549\n",
      "[unused545] 550\n",
      "[unused546] 551\n",
      "[unused547] 552\n",
      "[unused548] 553\n",
      "[unused549] 554\n",
      "[unused550] 555\n",
      "[unused551] 556\n",
      "[unused552] 557\n",
      "[unused553] 558\n",
      "[unused554] 559\n",
      "[unused555] 560\n",
      "[unused556] 561\n",
      "[unused557] 562\n",
      "[unused558] 563\n",
      "[unused559] 564\n",
      "[unused560] 565\n",
      "[unused561] 566\n",
      "[unused562] 567\n",
      "[unused563] 568\n",
      "[unused564] 569\n",
      "[unused565] 570\n",
      "[unused566] 571\n",
      "[unused567] 572\n",
      "[unused568] 573\n",
      "[unused569] 574\n",
      "[unused570] 575\n",
      "[unused571] 576\n",
      "[unused572] 577\n",
      "[unused573] 578\n",
      "[unused574] 579\n",
      "[unused575] 580\n",
      "[unused576] 581\n",
      "[unused577] 582\n",
      "[unused578] 583\n",
      "[unused579] 584\n",
      "[unused580] 585\n",
      "[unused581] 586\n",
      "[unused582] 587\n",
      "[unused583] 588\n",
      "[unused584] 589\n",
      "[unused585] 590\n",
      "[unused586] 591\n",
      "[unused587] 592\n",
      "[unused588] 593\n",
      "[unused589] 594\n",
      "[unused590] 595\n",
      "[unused591] 596\n",
      "[unused592] 597\n",
      "[unused593] 598\n",
      "[unused594] 599\n",
      "[unused595] 600\n",
      "[unused596] 601\n",
      "[unused597] 602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[unused598] 603\n",
      "[unused599] 604\n",
      "[unused600] 605\n",
      "[unused601] 606\n",
      "[unused602] 607\n",
      "[unused603] 608\n",
      "[unused604] 609\n",
      "[unused605] 610\n",
      "[unused606] 611\n",
      "[unused607] 612\n",
      "[unused608] 613\n",
      "[unused609] 614\n",
      "[unused610] 615\n",
      "[unused611] 616\n",
      "[unused612] 617\n",
      "[unused613] 618\n",
      "[unused614] 619\n",
      "[unused615] 620\n",
      "[unused616] 621\n",
      "[unused617] 622\n",
      "[unused618] 623\n",
      "[unused619] 624\n",
      "[unused620] 625\n",
      "[unused621] 626\n",
      "[unused622] 627\n",
      "[unused623] 628\n",
      "[unused624] 629\n",
      "[unused625] 630\n",
      "[unused626] 631\n",
      "[unused627] 632\n",
      "[unused628] 633\n",
      "[unused629] 634\n",
      "[unused630] 635\n",
      "[unused631] 636\n",
      "[unused632] 637\n",
      "[unused633] 638\n",
      "[unused634] 639\n",
      "[unused635] 640\n",
      "[unused636] 641\n",
      "[unused637] 642\n",
      "[unused638] 643\n",
      "[unused639] 644\n",
      "[unused640] 645\n",
      "[unused641] 646\n",
      "[unused642] 647\n",
      "[unused643] 648\n",
      "[unused644] 649\n",
      "[unused645] 650\n",
      "[unused646] 651\n",
      "[unused647] 652\n",
      "[unused648] 653\n",
      "[unused649] 654\n",
      "[unused650] 655\n",
      "[unused651] 656\n",
      "[unused652] 657\n",
      "[unused653] 658\n",
      "[unused654] 659\n",
      "[unused655] 660\n",
      "[unused656] 661\n",
      "[unused657] 662\n",
      "[unused658] 663\n",
      "[unused659] 664\n",
      "[unused660] 665\n",
      "[unused661] 666\n",
      "[unused662] 667\n",
      "[unused663] 668\n",
      "[unused664] 669\n",
      "[unused665] 670\n",
      "[unused666] 671\n",
      "[unused667] 672\n",
      "[unused668] 673\n",
      "[unused669] 674\n",
      "[unused670] 675\n",
      "[unused671] 676\n",
      "[unused672] 677\n",
      "[unused673] 678\n",
      "[unused674] 679\n",
      "[unused675] 680\n",
      "[unused676] 681\n",
      "[unused677] 682\n",
      "[unused678] 683\n",
      "[unused679] 684\n",
      "[unused680] 685\n",
      "[unused681] 686\n",
      "[unused682] 687\n",
      "[unused683] 688\n",
      "[unused684] 689\n",
      "[unused685] 690\n",
      "[unused686] 691\n",
      "[unused687] 692\n",
      "[unused688] 693\n",
      "[unused689] 694\n",
      "[unused690] 695\n",
      "[unused691] 696\n",
      "[unused692] 697\n",
      "[unused693] 698\n",
      "[unused694] 699\n",
      "[unused695] 700\n",
      "[unused696] 701\n",
      "[unused697] 702\n",
      "[unused698] 703\n",
      "[unused699] 704\n",
      "[unused700] 705\n",
      "[unused701] 706\n",
      "[unused702] 707\n",
      "[unused703] 708\n",
      "[unused704] 709\n",
      "[unused705] 710\n",
      "[unused706] 711\n",
      "[unused707] 712\n",
      "[unused708] 713\n",
      "[unused709] 714\n",
      "[unused710] 715\n",
      "[unused711] 716\n",
      "[unused712] 717\n",
      "[unused713] 718\n",
      "[unused714] 719\n",
      "[unused715] 720\n",
      "[unused716] 721\n",
      "[unused717] 722\n",
      "[unused718] 723\n",
      "[unused719] 724\n",
      "[unused720] 725\n",
      "[unused721] 726\n",
      "[unused722] 727\n",
      "[unused723] 728\n",
      "[unused724] 729\n",
      "[unused725] 730\n",
      "[unused726] 731\n",
      "[unused727] 732\n",
      "[unused728] 733\n",
      "[unused729] 734\n",
      "[unused730] 735\n",
      "[unused731] 736\n",
      "[unused732] 737\n",
      "[unused733] 738\n",
      "[unused734] 739\n",
      "[unused735] 740\n",
      "[unused736] 741\n",
      "[unused737] 742\n",
      "[unused738] 743\n",
      "[unused739] 744\n",
      "[unused740] 745\n",
      "[unused741] 746\n",
      "[unused742] 747\n",
      "[unused743] 748\n",
      "[unused744] 749\n",
      "[unused745] 750\n",
      "[unused746] 751\n",
      "[unused747] 752\n",
      "[unused748] 753\n",
      "[unused749] 754\n",
      "[unused750] 755\n",
      "[unused751] 756\n",
      "[unused752] 757\n",
      "[unused753] 758\n",
      "[unused754] 759\n",
      "[unused755] 760\n",
      "[unused756] 761\n",
      "[unused757] 762\n",
      "[unused758] 763\n",
      "[unused759] 764\n",
      "[unused760] 765\n",
      "[unused761] 766\n",
      "[unused762] 767\n",
      "[unused763] 768\n",
      "[unused764] 769\n",
      "[unused765] 770\n",
      "[unused766] 771\n",
      "[unused767] 772\n",
      "[unused768] 773\n",
      "[unused769] 774\n",
      "[unused770] 775\n",
      "[unused771] 776\n",
      "[unused772] 777\n",
      "[unused773] 778\n",
      "[unused774] 779\n",
      "[unused775] 780\n",
      "[unused776] 781\n",
      "[unused777] 782\n",
      "[unused778] 783\n",
      "[unused779] 784\n",
      "[unused780] 785\n",
      "[unused781] 786\n",
      "[unused782] 787\n",
      "[unused783] 788\n",
      "[unused784] 789\n",
      "[unused785] 790\n",
      "[unused786] 791\n",
      "[unused787] 792\n",
      "[unused788] 793\n",
      "[unused789] 794\n",
      "[unused790] 795\n",
      "[unused791] 796\n",
      "[unused792] 797\n",
      "[unused793] 798\n",
      "[unused794] 799\n",
      "[unused795] 800\n",
      "[unused796] 801\n",
      "[unused797] 802\n",
      "[unused798] 803\n",
      "[unused799] 804\n",
      "[unused800] 805\n",
      "[unused801] 806\n",
      "[unused802] 807\n",
      "[unused803] 808\n",
      "[unused804] 809\n",
      "[unused805] 810\n",
      "[unused806] 811\n",
      "[unused807] 812\n",
      "[unused808] 813\n",
      "[unused809] 814\n",
      "[unused810] 815\n",
      "[unused811] 816\n",
      "[unused812] 817\n",
      "[unused813] 818\n",
      "[unused814] 819\n",
      "[unused815] 820\n",
      "[unused816] 821\n",
      "[unused817] 822\n",
      "[unused818] 823\n",
      "[unused819] 824\n",
      "[unused820] 825\n",
      "[unused821] 826\n",
      "[unused822] 827\n",
      "[unused823] 828\n",
      "[unused824] 829\n",
      "[unused825] 830\n",
      "[unused826] 831\n",
      "[unused827] 832\n",
      "[unused828] 833\n",
      "[unused829] 834\n",
      "[unused830] 835\n",
      "[unused831] 836\n",
      "[unused832] 837\n",
      "[unused833] 838\n",
      "[unused834] 839\n",
      "[unused835] 840\n",
      "[unused836] 841\n",
      "[unused837] 842\n",
      "[unused838] 843\n",
      "[unused839] 844\n",
      "[unused840] 845\n",
      "[unused841] 846\n",
      "[unused842] 847\n",
      "[unused843] 848\n",
      "[unused844] 849\n",
      "[unused845] 850\n",
      "[unused846] 851\n",
      "[unused847] 852\n",
      "[unused848] 853\n",
      "[unused849] 854\n",
      "[unused850] 855\n",
      "[unused851] 856\n",
      "[unused852] 857\n",
      "[unused853] 858\n",
      "[unused854] 859\n",
      "[unused855] 860\n",
      "[unused856] 861\n",
      "[unused857] 862\n",
      "[unused858] 863\n",
      "[unused859] 864\n",
      "[unused860] 865\n",
      "[unused861] 866\n",
      "[unused862] 867\n",
      "[unused863] 868\n",
      "[unused864] 869\n",
      "[unused865] 870\n",
      "[unused866] 871\n",
      "[unused867] 872\n",
      "[unused868] 873\n",
      "[unused869] 874\n",
      "[unused870] 875\n",
      "[unused871] 876\n",
      "[unused872] 877\n",
      "[unused873] 878\n",
      "[unused874] 879\n",
      "[unused875] 880\n",
      "[unused876] 881\n",
      "[unused877] 882\n",
      "[unused878] 883\n",
      "[unused879] 884\n",
      "[unused880] 885\n",
      "[unused881] 886\n",
      "[unused882] 887\n",
      "[unused883] 888\n",
      "[unused884] 889\n",
      "[unused885] 890\n",
      "[unused886] 891\n",
      "[unused887] 892\n",
      "[unused888] 893\n",
      "[unused889] 894\n",
      "[unused890] 895\n",
      "[unused891] 896\n",
      "[unused892] 897\n",
      "[unused893] 898\n",
      "[unused894] 899\n",
      "[unused895] 900\n",
      "[unused896] 901\n",
      "[unused897] 902\n",
      "[unused898] 903\n",
      "[unused899] 904\n",
      "[unused900] 905\n",
      "[unused901] 906\n",
      "[unused902] 907\n",
      "[unused903] 908\n",
      "[unused904] 909\n",
      "[unused905] 910\n",
      "[unused906] 911\n",
      "[unused907] 912\n",
      "[unused908] 913\n",
      "[unused909] 914\n",
      "[unused910] 915\n",
      "[unused911] 916\n",
      "[unused912] 917\n",
      "[unused913] 918\n",
      "[unused914] 919\n",
      "[unused915] 920\n",
      "[unused916] 921\n",
      "[unused917] 922\n",
      "[unused918] 923\n",
      "[unused919] 924\n",
      "[unused920] 925\n",
      "[unused921] 926\n",
      "[unused922] 927\n",
      "[unused923] 928\n",
      "[unused924] 929\n",
      "[unused925] 930\n",
      "[unused926] 931\n",
      "[unused927] 932\n",
      "[unused928] 933\n",
      "[unused929] 934\n",
      "[unused930] 935\n",
      "[unused931] 936\n",
      "[unused932] 937\n",
      "[unused933] 938\n",
      "[unused934] 939\n",
      "[unused935] 940\n",
      "[unused936] 941\n",
      "[unused937] 942\n",
      "[unused938] 943\n",
      "[unused939] 944\n",
      "[unused940] 945\n",
      "[unused941] 946\n",
      "[unused942] 947\n",
      "[unused943] 948\n",
      "[unused944] 949\n",
      "[unused945] 950\n",
      "[unused946] 951\n",
      "[unused947] 952\n",
      "[unused948] 953\n",
      "[unused949] 954\n",
      "[unused950] 955\n",
      "[unused951] 956\n",
      "[unused952] 957\n",
      "[unused953] 958\n",
      "[unused954] 959\n",
      "[unused955] 960\n",
      "[unused956] 961\n",
      "[unused957] 962\n",
      "[unused958] 963\n",
      "[unused959] 964\n",
      "[unused960] 965\n",
      "[unused961] 966\n",
      "[unused962] 967\n",
      "[unused963] 968\n",
      "[unused964] 969\n",
      "[unused965] 970\n",
      "[unused966] 971\n",
      "[unused967] 972\n",
      "[unused968] 973\n",
      "[unused969] 974\n",
      "[unused970] 975\n",
      "[unused971] 976\n",
      "[unused972] 977\n",
      "[unused973] 978\n",
      "[unused974] 979\n",
      "[unused975] 980\n",
      "[unused976] 981\n",
      "[unused977] 982\n",
      "[unused978] 983\n",
      "[unused979] 984\n",
      "[unused980] 985\n",
      "[unused981] 986\n",
      "[unused982] 987\n",
      "[unused983] 988\n",
      "[unused984] 989\n",
      "[unused985] 990\n",
      "[unused986] 991\n",
      "[unused987] 992\n",
      "[unused988] 993\n",
      "[unused989] 994\n",
      "[unused990] 995\n",
      "[unused991] 996\n",
      "[unused992] 997\n",
      "[unused993] 998\n",
      " 1102\n",
      " 1108\n",
      " 1109\n",
      " 1111\n",
      " 1113\n",
      " 1115\n",
      " 1116\n",
      " 1117\n",
      " 1118\n",
      " 1119\n",
      " 1120\n",
      " 1121\n",
      " 1123\n",
      " 1124\n",
      " 1125\n",
      " 1126\n",
      " 1127\n",
      " 1128\n",
      " 1129\n",
      " 1130\n",
      " 1131\n",
      " 1132\n",
      " 1133\n",
      " 1134\n",
      " 1135\n",
      " 1136\n",
      " 1137\n",
      " 1139\n",
      " 1140\n",
      " 1141\n",
      " 1142\n",
      " 1143\n",
      " 1144\n",
      " 1146\n",
      " 1147\n",
      " 1148\n",
      " 1149\n",
      " 1150\n",
      " 1151\n",
      " 1152\n",
      " 1153\n",
      " 1154\n",
      " 1204\n",
      " 1205\n",
      " 1206\n",
      " 1207\n",
      " 1209\n",
      " 1211\n",
      " 1214\n",
      " 1215\n",
      " 1216\n",
      " 1217\n",
      " 1218\n",
      " 1219\n",
      " 1220\n",
      " 1221\n",
      " 1222\n",
      " 1223\n",
      " 1224\n",
      " 1225\n",
      " 1226\n",
      " 1227\n",
      " 1228\n",
      " 1229\n",
      " 1230\n",
      " 1231\n",
      " 1232\n",
      " 1233\n",
      " 1234\n",
      " 1235\n",
      " 1236\n",
      " 1237\n",
      " 1238\n",
      " 1239\n",
      " 1256\n",
      " 1260\n",
      " 1262\n",
      " 1269\n",
      " 1290\n",
      " 1301\n",
      " 1306\n",
      " 1307\n",
      " 1308\n",
      " 1310\n",
      " 1311\n",
      " 1312\n",
      " 1313\n",
      " 1314\n",
      " 1315\n",
      " 1316\n",
      " 1317\n",
      " 1318\n",
      " 1319\n",
      " 1320\n",
      " 1321\n",
      " 1322\n",
      " 1323\n",
      " 1324\n",
      " 1325\n",
      " 1326\n",
      " 1327\n",
      " 1328\n",
      " 1329\n",
      " 1330\n",
      " 1331\n",
      " 1332\n",
      " 1333\n",
      " 1334\n",
      " 1335\n",
      " 1336\n",
      " 1337\n",
      " 1338\n",
      " 1339\n",
      " 1340\n",
      " 1341\n",
      " 1342\n",
      " 1343\n",
      " 1344\n",
      " 1345\n",
      " 1346\n",
      " 1347\n",
      " 1348\n",
      " 1349\n",
      " 1350\n",
      " 1351\n",
      " 1352\n",
      " 1353\n",
      " 1354\n",
      " 1355\n",
      " 1356\n",
      " 1357\n",
      " 1358\n",
      " 1359\n",
      " 1360\n",
      " 1361\n",
      " 1362\n",
      " 1363\n",
      " 1364\n",
      " 1365\n",
      " 1366\n",
      " 1367\n",
      " 1368\n",
      " 1369\n",
      " 1370\n",
      " 1371\n",
      " 1372\n",
      " 1373\n",
      " 1374\n",
      " 1375\n",
      " 1376\n",
      " 1377\n",
      " 1378\n",
      " 1379\n",
      " 1380\n",
      " 1381\n",
      " 1382\n",
      " 1383\n",
      " 1384\n",
      " 1385\n",
      " 1386\n",
      " 1387\n",
      " 1388\n",
      " 1389\n",
      " 1390\n",
      " 1391\n",
      " 1392\n",
      " 1393\n",
      " 1394\n",
      " 1395\n",
      " 1396\n",
      " 1397\n",
      " 1398\n",
      " 1399\n",
      " 1400\n",
      " 1401\n",
      " 1402\n",
      " 1403\n",
      " 1404\n",
      " 1405\n",
      " 1406\n",
      " 1407\n",
      " 1408\n",
      " 1409\n",
      " 1410\n",
      " 1411\n",
      " 1412\n",
      " 1413\n",
      " 1414\n",
      " 1415\n",
      " 1416\n",
      " 1417\n",
      " 1418\n",
      " 1419\n",
      " 1420\n",
      " 1421\n",
      " 1422\n",
      " 1423\n",
      " 1424\n",
      " 1425\n",
      " 1426\n",
      " 1427\n",
      " 1428\n",
      " 1429\n",
      " 1430\n",
      " 1431\n",
      " 1432\n",
      " 1433\n",
      " 1434\n",
      " 1435\n",
      " 1436\n",
      " 1437\n",
      " 1438\n",
      " 1439\n",
      " 1440\n",
      " 1441\n",
      " 1442\n",
      " 1443\n",
      " 1444\n",
      " 1445\n",
      " 1446\n",
      " 1447\n",
      " 1448\n",
      " 1449\n",
      " 1450\n",
      " 1451\n",
      " 1452\n",
      " 1453\n",
      " 1454\n",
      " 1455\n",
      " 1456\n",
      " 1457\n",
      " 1458\n",
      " 1459\n",
      " 1460\n",
      " 1461\n",
      " 1462\n",
      " 1463\n",
      " 1464\n",
      " 1465\n",
      " 1466\n",
      " 1467\n",
      " 1468\n",
      " 1469\n",
      " 1470\n",
      " 1471\n",
      " 1472\n",
      " 1473\n",
      " 1474\n",
      " 1475\n",
      " 1476\n",
      " 1477\n",
      " 1478\n",
      " 1479\n",
      " 1480\n",
      " 1481\n",
      " 1482\n",
      " 1483\n",
      " 1484\n",
      " 1485\n",
      " 1486\n",
      " 1487\n",
      " 1488\n",
      " 1489\n",
      " 1490\n",
      " 1491\n",
      " 1492\n",
      " 1493\n",
      " 1494\n",
      " 1495\n",
      " 1496\n",
      " 1497\n",
      " 1498\n",
      " 1499\n",
      " 1500\n",
      " 1501\n",
      " 1502\n",
      " 1503\n",
      " 1504\n",
      " 1505\n",
      " 1506\n",
      " 1507\n",
      " 1508\n",
      " 1509\n",
      " 1510\n",
      " 1511\n",
      " 1512\n",
      " 1514\n",
      " 1515\n",
      " 1533\n",
      " 1534\n",
      " 1535\n",
      " 1536\n",
      " 1537\n",
      " 1539\n",
      " 1540\n",
      " 1541\n",
      " 1542\n",
      " 1543\n",
      " 1544\n",
      " 1545\n",
      " 1547\n",
      " 1548\n",
      " 1549\n",
      " 1550\n",
      " 1551\n",
      " 1552\n",
      " 1553\n",
      " 1554\n",
      " 1555\n",
      " 1556\n",
      " 1557\n",
      " 1558\n",
      " 1559\n",
      " 1560\n",
      " 1561\n",
      " 1562\n",
      " 1563\n",
      " 1564\n",
      " 1565\n",
      " 1566\n",
      " 1567\n",
      " 1568\n",
      " 1569\n",
      " 1570\n",
      " 1571\n",
      " 1572\n",
      " 1573\n",
      " 1575\n",
      " 1576\n",
      " 1577\n",
      " 1579\n",
      " 1580\n",
      " 1583\n",
      " 1585\n",
      " 1587\n",
      " 1588\n",
      " 1589\n",
      " 1590\n",
      " 1591\n",
      " 1592\n",
      " 1593\n",
      " 1595\n",
      " 1596\n",
      " 1597\n",
      " 1598\n",
      " 1599\n",
      " 1602\n",
      " 1603\n",
      " 1604\n",
      " 1605\n",
      " 1606\n",
      " 1607\n",
      " 1608\n",
      " 1609\n",
      " 1610\n",
      " 1611\n",
      " 1612\n",
      " 1613\n",
      " 1614\n",
      " 1615\n",
      " 1616\n",
      " 1617\n",
      " 1618\n",
      " 1619\n",
      " 1620\n",
      " 1621\n",
      " 1623\n",
      " 1624\n",
      " 1625\n",
      " 1626\n",
      " 1629\n",
      " 1630\n",
      " 1631\n",
      " 1632\n",
      " 1633\n",
      " 1634\n",
      " 1635\n",
      " 1636\n",
      " 1637\n",
      " 1638\n",
      " 1639\n",
      " 1640\n",
      " 1641\n",
      " 1642\n",
      " 1643\n",
      " 1644\n",
      " 1645\n",
      " 1646\n",
      " 1647\n",
      " 1648\n",
      " 1649\n",
      " 1650\n",
      " 1651\n",
      " 1652\n",
      " 1653\n",
      " 1654\n",
      " 1655\n",
      " 1656\n",
      " 1657\n",
      " 1658\n",
      " 1659\n",
      " 1660\n",
      " 1661\n",
      " 1662\n",
      " 1663\n",
      " 1664\n",
      " 1665\n",
      " 1666\n",
      " 1667\n",
      " 1668\n",
      " 1669\n",
      " 1670\n",
      " 1671\n",
      " 1672\n",
      " 1673\n",
      " 1674\n",
      " 1675\n",
      " 1676\n",
      " 1677\n",
      " 1678\n",
      " 1679\n",
      " 1680\n",
      " 1681\n",
      " 1682\n",
      " 1683\n",
      " 1684\n",
      " 1685\n",
      " 1686\n",
      " 1687\n",
      " 1688\n",
      " 1689\n",
      " 1690\n",
      " 1691\n",
      " 1692\n",
      " 1693\n",
      " 1694\n",
      " 1695\n",
      " 1696\n",
      " 1697\n",
      " 1698\n",
      " 1699\n",
      " 1700\n",
      " 1701\n",
      " 1702\n",
      " 1703\n",
      " 1704\n",
      " 1705\n",
      " 1706\n",
      " 1707\n",
      " 1708\n",
      " 1709\n",
      " 1710\n",
      " 1711\n",
      " 1712\n",
      " 1713\n",
      " 1714\n",
      " 1715\n",
      " 1716\n",
      " 1717\n",
      " 1718\n",
      " 1719\n",
      " 1720\n",
      " 1721\n",
      " 1722\n",
      " 1723\n",
      " 1724\n",
      " 1725\n",
      " 1726\n",
      " 1727\n",
      " 1728\n",
      " 1729\n",
      " 1730\n",
      " 1731\n",
      " 1732\n",
      " 1733\n",
      " 1734\n",
      " 1735\n",
      " 1736\n",
      " 1737\n",
      " 1738\n",
      " 1739\n",
      " 1741\n",
      " 1743\n",
      " 1745\n",
      " 1746\n",
      " 1747\n",
      " 1748\n",
      " 1749\n",
      " 1750\n",
      " 1751\n",
      " 1752\n",
      " 1753\n",
      " 1754\n",
      " 1755\n",
      " 1757\n",
      " 1758\n",
      " 1759\n",
      " 1760\n",
      " 1761\n",
      " 1762\n",
      " 1763\n",
      " 1764\n",
      " 1765\n",
      " 1766\n",
      " 1767\n",
      " 1768\n",
      " 1769\n",
      " 1770\n",
      " 1773\n",
      " 1774\n",
      " 1775\n",
      " 1776\n",
      " 1777\n",
      " 1778\n",
      " 1779\n",
      " 1780\n",
      " 1781\n",
      " 1782\n",
      " 1783\n",
      " 1784\n",
      " 1785\n",
      " 1786\n",
      " 1787\n",
      " 1789\n",
      " 1790\n",
      " 1791\n",
      " 1792\n",
      " 1793\n",
      " 1794\n",
      " 1795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1796\n",
      " 1797\n",
      " 1798\n",
      " 1799\n",
      " 1800\n",
      " 1801\n",
      " 1802\n",
      " 1803\n",
      " 1804\n",
      " 1805\n",
      " 1806\n",
      " 1807\n",
      " 1808\n",
      " 1809\n",
      " 1812\n",
      " 1813\n",
      " 1814\n",
      " 1817\n",
      " 1818\n",
      " 1819\n",
      " 1821\n",
      " 1822\n",
      " 1823\n",
      " 1824\n",
      " 1825\n",
      " 1826\n",
      " 1827\n",
      " 1828\n",
      " 1829\n",
      " 1830\n",
      " 1832\n",
      " 1833\n",
      " 1834\n",
      " 1835\n",
      " 1836\n",
      " 1837\n",
      " 1838\n",
      " 1839\n",
      " 1840\n",
      " 1841\n",
      " 1842\n",
      " 1843\n",
      " 1844\n",
      " 1845\n",
      " 1846\n",
      " 1847\n",
      " 1848\n",
      " 1849\n",
      " 1850\n",
      " 1851\n",
      " 1852\n",
      " 1853\n",
      " 1854\n",
      " 1856\n",
      " 1857\n",
      " 1859\n",
      " 1860\n",
      " 1861\n",
      " 1862\n",
      " 1863\n",
      " 1866\n",
      " 1867\n",
      " 1868\n",
      " 1869\n",
      " 1870\n",
      " 1873\n",
      " 1874\n",
      " 1876\n",
      " 1877\n",
      " 1878\n",
      " 1882\n",
      " 1883\n",
      " 1884\n",
      " 1885\n",
      " 1886\n",
      " 1887\n",
      " 1888\n",
      " 1889\n",
      " 1890\n",
      " 1891\n",
      " 1892\n",
      " 1893\n",
      " 1894\n",
      " 1895\n",
      " 1896\n",
      " 1897\n",
      " 1898\n",
      " 1899\n",
      " 1900\n",
      " 1901\n",
      " 1902\n",
      " 1903\n",
      " 1904\n",
      " 1905\n",
      " 1906\n",
      " 1907\n",
      " 1908\n",
      " 1909\n",
      " 1910\n",
      " 1911\n",
      " 1912\n",
      " 1914\n",
      " 1916\n",
      " 1917\n",
      " 1918\n",
      " 1919\n",
      " 1920\n",
      " 1921\n",
      " 1922\n",
      " 1924\n",
      " 1925\n",
      " 1926\n",
      " 1927\n",
      " 1928\n",
      " 1929\n",
      " 1930\n",
      " 1931\n",
      " 1932\n",
      " 1933\n",
      " 1934\n",
      " 1935\n",
      " 1936\n",
      " 1937\n",
      " 1938\n",
      " 1940\n",
      " 1941\n",
      " 1942\n",
      " 1943\n",
      " 1944\n",
      " 1945\n",
      " 1946\n",
      " 1948\n",
      " 1949\n",
      " 1950\n",
      " 1951\n",
      " 1952\n",
      " 1953\n",
      " 1954\n",
      " 1955\n",
      " 1956\n",
      " 1957\n",
      " 1958\n",
      " 1959\n",
      " 1960\n",
      " 1961\n",
      " 1962\n",
      " 1963\n",
      " 1965\n",
      " 1966\n",
      " 1967\n",
      " 1968\n",
      " 1969\n",
      " 1970\n",
      " 1971\n",
      " 1972\n",
      " 1973\n",
      " 1974\n",
      " 1975\n",
      " 1976\n",
      " 1977\n",
      " 1978\n",
      " 1979\n",
      " 1980\n",
      " 1981\n",
      " 1982\n",
      " 1983\n",
      " 1984\n",
      " 1985\n",
      " 1986\n",
      " 1987\n",
      " 1988\n",
      " 1989\n",
      " 1990\n",
      " 1991\n",
      " 1992\n",
      " 1993\n",
      " 1994\n",
      " 1995\n",
      "... 2133\n",
      "km 3186\n",
      "soundtrack 6050\n",
      "remix 6136\n",
      "c 6362\n",
      "uefa 6663\n",
      "playoff 7808\n",
      "midfielder 8850\n",
      "playstation 9160\n",
      "quarterfinals 9237\n",
      "pinyin 9973\n",
      "allmusic 10477\n",
      "mlb 10901\n",
      "espn 10978\n",
      "gameplay 11247\n",
      "nsw 11524\n",
      "nascar 11838\n",
      "itunes 11943\n",
      "lgbt 12010\n",
      "mvp 12041\n",
      "xbox 12202\n",
      "eurovision 12714\n",
      "## 12744\n",
      "vfl 13480\n",
      "kolkata 13522\n",
      "pga 14198\n",
      "m 14241\n",
      "bundesliga 14250\n",
      "metacritic 14476\n",
      "remixes 15193\n",
      "steelers 15280\n",
      "airplay 15341\n",
      "## 15414\n",
      "paralympics 15600\n",
      "zhao 15634\n",
      "reggae 15662\n",
      "linebacker 15674\n",
      "v8 15754\n",
      "hindwings 15998\n",
      "bollywood 16046\n",
      "podcast 16110\n",
      "atletico 16132\n",
      "wwf 16779\n",
      "transgender 16824\n",
      "paralympic 17029\n",
      "## 17110\n",
      "postseason 17525\n",
      "vhs 17550\n",
      "campeonato 17675\n",
      "multiplayer 17762\n",
      "odz 17814\n",
      "curated 17940\n",
      "iphone 18059\n",
      "gmbh 18289\n",
      "danielle 18490\n",
      "qaeda 18659\n",
      "mixtape 18713\n",
      " 18728\n",
      "## 19109\n",
      "## 19110\n",
      "##aw 19704\n",
      "##qing 19784\n",
      "saxophonist 19977\n",
      "preseason 20038\n",
      "pmid 20117\n",
      "keyboardist 20173\n",
      "iucn 20333\n",
      "pokemon 20421\n",
      "nrl 20686\n",
      "motorsports 20711\n",
      "jaenelle 20757\n",
      "beyonce 20773\n",
      "airbus 20901\n",
      "netflix 20907\n",
      "motorsport 21044\n",
      "belgarath 21256\n",
      "iaaf 21259\n",
      "guangdong 21287\n",
      "shortlisted 21353\n",
      "seahawks 21390\n",
      "lucivar 21401\n",
      "goaltender 21437\n",
      "nanjing 21455\n",
      "crambidae 21585\n",
      "frontman 21597\n",
      "cbn 21824\n",
      "odisha 21874\n",
      "alzheimer 21901\n",
      "kinase 21903\n",
      "futsal 21921\n",
      "wta 21925\n",
      "nokia 22098\n",
      "smackdown 22120\n",
      "concacaf 22169\n",
      "oricon 22237\n",
      "twenty20 22240\n",
      "nfc 22309\n",
      "tbilisi 22406\n",
      "nrhp 22424\n",
      "showcased 22443\n",
      "nmi 22484\n",
      "kaladin 22588\n",
      "storylines 22628\n",
      "## 22646\n",
      "riaa 22716\n",
      "##a 22972\n",
      "mitochondrial 23079\n",
      "haryana 23261\n",
      "##saw 23305\n",
      "## 23432\n",
      "issn 23486\n",
      "saetan 23515\n",
      "rbis 23583\n",
      "deportivo 23696\n",
      "neuroscience 23700\n",
      "telangana 23764\n",
      "lviv 23814\n",
      "scientology 23845\n",
      "bwv 23860\n",
      "transitioned 23946\n",
      "yamaha 24031\n",
      "maccabi 24055\n",
      "lexie 24123\n",
      "superfamily 24169\n",
      "hapoel 24208\n",
      "wcw 24215\n",
      "soundtracks 24245\n",
      "debuting 24469\n",
      "bmg 24499\n",
      "sequencing 24558\n",
      "ho 24833\n",
      "myspace 24927\n",
      "##strae 24967\n",
      "smashwords 25151\n",
      "wrocaw 25160\n",
      "cmll 25395\n",
      "rihanna 25439\n",
      "wnba 25554\n",
      "brianna 25558\n",
      "filmfare 25648\n",
      "hezbollah 25713\n",
      "cheerleading 25721\n",
      "telenovela 25754\n",
      "shandong 25768\n",
      "## 25799\n",
      "erebidae 25875\n",
      "superliga 25922\n",
      "hyundai 25983\n",
      "springsteen 26002\n",
      "euroleague 26093\n",
      "stanisaw 26133\n",
      "tianjin 26216\n",
      "vh1 26365\n",
      "smartphone 26381\n",
      "alyssa 26442\n",
      "detainees 26485\n",
      "kayla 26491\n",
      "benfica 26542\n",
      "shenzhen 26555\n",
      "godzilla 26631\n",
      "zhejiang 26805\n",
      "2010s 26817\n",
      "cornerback 26857\n",
      "capcom 26861\n",
      "iihf 26904\n",
      "mukherjee 27040\n",
      "polgara 27041\n",
      "joyah 27098\n",
      "multicultural 27135\n",
      "mbc 27262\n",
      "jillian 27286\n",
      "## 27392\n",
      "metadata 27425\n",
      "showcasing 27696\n",
      "minogue 27736\n",
      "##rae 27807\n",
      "##1 27944\n",
      "00pm 27995\n",
      "wrestlemania 28063\n",
      "1910s 28088\n",
      "jiangsu 28091\n",
      "iqbal 28111\n",
      "biomass 28148\n",
      "prequel 28280\n",
      "fivb 28423\n",
      "kaitlyn 28584\n",
      "goalscorer 28602\n",
      "scuba 28651\n",
      "##genase 28835\n",
      "mrna 28848\n",
      "dfb 28894\n",
      "gaddafi 28924\n",
      "ghz 29066\n",
      "donetsk 29151\n",
      "britney 29168\n",
      "mentoring 29192\n",
      "postdoctoral 29272\n",
      "sql 29296\n",
      "weightlifting 29305\n",
      "quarterfinal 29380\n",
      "signage 29404\n",
      "fujian 29551\n",
      "endelle 29581\n",
      "##! 29612\n",
      "##\" 29613\n",
      "### 29614\n",
      "##$ 29615\n",
      "##% 29616\n",
      "##& 29617\n",
      "##' 29618\n",
      "##( 29619\n",
      "##) 29620\n",
      "##* 29621\n",
      "##+ 29622\n",
      "##, 29623\n",
      "##- 29624\n",
      "##. 29625\n",
      "##/ 29626\n",
      "##: 29627\n",
      "##; 29628\n",
      "##< 29629\n",
      "##= 29630\n",
      "##> 29631\n",
      "##? 29632\n",
      "##@ 29633\n",
      "##[ 29634\n",
      "##\\ 29635\n",
      "##] 29636\n",
      "##^ 29637\n",
      "##_ 29638\n",
      "##` 29639\n",
      "##{ 29640\n",
      "##| 29641\n",
      "##} 29642\n",
      "##~ 29643\n",
      "## 29644\n",
      "## 29650\n",
      "## 29654\n",
      "## 29660\n",
      "## 29661\n",
      "## 29663\n",
      "## 29666\n",
      "## 29675\n",
      "## 29676\n",
      "## 29677\n",
      "## 29678\n",
      "## 29680\n",
      "## 29682\n",
      "## 29683\n",
      "## 29685\n",
      "## 29686\n",
      "## 29687\n",
      "## 29688\n",
      "## 29690\n",
      "## 29691\n",
      "## 29692\n",
      "## 29693\n",
      "## 29694\n",
      "## 29695\n",
      "## 29696\n",
      "## 29697\n",
      "## 29698\n",
      "## 29699\n",
      "## 29700\n",
      "## 29701\n",
      "## 29702\n",
      "## 29703\n",
      "## 29705\n",
      "## 29706\n",
      "## 29707\n",
      "## 29708\n",
      "## 29709\n",
      "## 29710\n",
      "## 29712\n",
      "## 29713\n",
      "## 29714\n",
      "## 29715\n",
      "## 29716\n",
      "## 29717\n",
      "## 29718\n",
      "## 29719\n",
      "## 29758\n",
      "## 29759\n",
      "## 29761\n",
      "## 29762\n",
      "## 29763\n",
      "## 29764\n",
      "## 29765\n",
      "## 29766\n",
      "## 29767\n",
      "## 29768\n",
      "## 29769\n",
      "## 29770\n",
      "## 29771\n",
      "## 29772\n",
      "## 29773\n",
      "## 29774\n",
      "## 29775\n",
      "## 29776\n",
      "## 29777\n",
      "## 29778\n",
      "## 29779\n",
      "## 29780\n",
      "## 29781\n",
      "## 29782\n",
      "## 29783\n",
      "## 29784\n",
      "## 29785\n",
      "## 29786\n",
      "## 29787\n",
      "## 29808\n",
      "## 29814\n",
      "## 29832\n",
      "## 29838\n",
      "## 29843\n",
      "## 29844\n",
      "## 29845\n",
      "## 29846\n",
      "## 29847\n",
      "## 29848\n",
      "## 29849\n",
      "## 29850\n",
      "## 29851\n",
      "## 29852\n",
      "## 29853\n",
      "## 29854\n",
      "## 29855\n",
      "## 29856\n",
      "## 29857\n",
      "## 29858\n",
      "## 29859\n",
      "## 29860\n",
      "## 29861\n",
      "## 29862\n",
      "## 29863\n",
      "## 29864\n",
      "## 29865\n",
      "## 29866\n",
      "## 29867\n",
      "## 29868\n",
      "## 29869\n",
      "## 29870\n",
      "## 29871\n",
      "## 29872\n",
      "## 29873\n",
      "## 29874\n",
      "## 29875\n",
      "## 29876\n",
      "## 29877\n",
      "## 29878\n",
      "## 29879\n",
      "## 29880\n",
      "## 29881\n",
      "## 29882\n",
      "## 29883\n",
      "## 29884\n",
      "## 29885\n",
      "## 29886\n",
      "## 29887\n",
      "## 29888\n",
      "## 29889\n",
      "## 29890\n",
      "## 29891\n",
      "## 29892\n",
      "## 29893\n",
      "## 29894\n",
      "## 29895\n",
      "## 29896\n",
      "## 29897\n",
      "## 29898\n",
      "## 29899\n",
      "## 29900\n",
      "## 29901\n",
      "## 29902\n",
      "## 29903\n",
      "## 29904\n",
      "## 29905\n",
      "## 29906\n",
      "## 29907\n",
      "## 29908\n",
      "## 29909\n",
      "## 29910\n",
      "## 29911\n",
      "## 29912\n",
      "## 29913\n",
      "## 29914\n",
      "## 29915\n",
      "## 29916\n",
      "## 29917\n",
      "## 29918\n",
      "## 29919\n",
      "## 29920\n",
      "## 29921\n",
      "## 29922\n",
      "## 29923\n",
      "## 29924\n",
      "## 29925\n",
      "## 29926\n",
      "## 29927\n",
      "## 29928\n",
      "## 29929\n",
      "## 29930\n",
      "## 29931\n",
      "## 29932\n",
      "## 29933\n",
      "## 29934\n",
      "## 29935\n",
      "## 29936\n",
      "## 29937\n",
      "## 29938\n",
      "## 29939\n",
      "## 29940\n",
      "## 29941\n",
      "## 29942\n",
      "## 29943\n",
      "## 29944\n",
      "## 29945\n",
      "## 29946\n",
      "## 29947\n",
      "## 29948\n",
      "## 29949\n",
      "## 29950\n",
      "## 29951\n",
      "## 29952\n",
      "## 29953\n",
      "## 29954\n",
      "## 29955\n",
      "## 29956\n",
      "## 29957\n",
      "## 29958\n",
      "## 29959\n",
      "## 29960\n",
      "## 29961\n",
      "## 29962\n",
      "## 29963\n",
      "## 29964\n",
      "## 29965\n",
      "## 29966\n",
      "## 29967\n",
      "## 29968\n",
      "## 29969\n",
      "## 29970\n",
      "## 29971\n",
      "## 29972\n",
      "## 29973\n",
      "## 29974\n",
      "## 29975\n",
      "## 29976\n",
      "## 29977\n",
      "## 29978\n",
      "## 29979\n",
      "## 29980\n",
      "## 29981\n",
      "## 29982\n",
      "## 29983\n",
      "## 29984\n",
      "## 29985\n",
      "## 29986\n",
      "## 29987\n",
      "## 29988\n",
      "## 29989\n",
      "## 29990\n",
      "## 29991\n",
      "## 29992\n",
      "## 29993\n",
      "## 29994\n",
      "## 29995\n",
      "## 29996\n",
      "## 29997\n",
      "## 29998\n",
      "## 29999\n",
      "## 30000\n",
      "## 30001\n",
      "## 30002\n",
      "## 30003\n",
      "## 30004\n",
      "## 30005\n",
      "## 30006\n",
      "## 30007\n",
      "## 30008\n",
      "## 30009\n",
      "## 30010\n",
      "## 30011\n",
      "## 30012\n",
      "## 30013\n",
      "## 30014\n",
      "## 30015\n",
      "## 30016\n",
      "## 30017\n",
      "## 30018\n",
      "## 30019\n",
      "## 30020\n",
      "## 30021\n",
      "## 30022\n",
      "## 30023\n",
      "## 30024\n",
      "## 30025\n",
      "## 30026\n",
      "## 30027\n",
      "## 30028\n",
      "## 30029\n",
      "## 30030\n",
      "## 30031\n",
      "## 30032\n",
      "## 30033\n",
      "## 30034\n",
      "## 30035\n",
      "## 30037\n",
      "## 30039\n",
      "## 30040\n",
      "## 30041\n",
      "## 30043\n",
      "## 30044\n",
      "## 30045\n",
      "## 30046\n",
      "## 30047\n",
      "## 30048\n",
      "## 30049\n",
      "## 30050\n",
      "## 30051\n",
      "## 30052\n",
      "## 30053\n",
      "## 30054\n",
      "## 30055\n",
      "## 30056\n",
      "## 30057\n",
      "## 30058\n",
      "## 30059\n",
      "## 30060\n",
      "## 30061\n",
      "## 30062\n",
      "## 30063\n",
      "## 30064\n",
      "## 30065\n",
      "## 30066\n",
      "## 30067\n",
      "## 30068\n",
      "## 30069\n",
      "## 30070\n",
      "## 30071\n",
      "## 30072\n",
      "## 30073\n",
      "## 30074\n",
      "## 30075\n",
      "## 30076\n",
      "## 30078\n",
      "## 30079\n",
      "## 30081\n",
      "## 30082\n",
      "## 30083\n",
      "## 30084\n",
      "## 30085\n",
      "## 30086\n",
      "## 30087\n",
      "## 30088\n",
      "## 30089\n",
      "## 30090\n",
      "## 30091\n",
      "## 30092\n",
      "## 30093\n",
      "## 30094\n",
      "## 30095\n",
      "## 30096\n",
      "## 30097\n",
      "## 30098\n",
      "## 30099\n",
      "## 30100\n",
      "## 30101\n",
      "## 30102\n",
      "## 30103\n",
      "## 30104\n",
      "## 30105\n",
      "## 30106\n",
      "## 30107\n",
      "## 30108\n",
      "## 30111\n",
      "## 30112\n",
      "## 30113\n",
      "## 30114\n",
      "## 30115\n",
      "## 30116\n",
      "## 30117\n",
      "## 30118\n",
      "## 30119\n",
      "## 30120\n",
      "## 30121\n",
      "## 30122\n",
      "## 30123\n",
      "## 30124\n",
      "## 30125\n",
      "## 30126\n",
      "## 30129\n",
      "## 30130\n",
      "## 30131\n",
      "## 30132\n",
      "## 30133\n",
      "## 30134\n",
      "## 30135\n",
      "## 30136\n",
      "## 30137\n",
      "## 30138\n",
      "## 30139\n",
      "## 30140\n",
      "## 30141\n",
      "## 30142\n",
      "## 30143\n",
      "## 30144\n",
      "## 30145\n",
      "## 30146\n",
      "## 30147\n",
      "## 30148\n",
      "## 30149\n",
      "## 30150\n",
      "## 30151\n",
      "## 30152\n",
      "## 30153\n",
      "## 30155\n",
      "## 30156\n",
      "## 30157\n",
      "## 30158\n",
      "## 30159\n",
      "## 30160\n",
      "## 30161\n",
      "## 30162\n",
      "## 30163\n",
      "## 30164\n",
      "## 30165\n",
      "## 30166\n",
      "## 30167\n",
      "## 30168\n",
      "## 30169\n",
      "## 30170\n",
      "## 30171\n",
      "## 30172\n",
      "## 30173\n",
      "## 30174\n",
      "## 30175\n",
      "## 30176\n",
      "## 30177\n",
      "## 30178\n",
      "## 30179\n",
      "## 30180\n",
      "## 30181\n",
      "## 30182\n",
      "## 30183\n",
      "## 30184\n",
      "## 30185\n",
      "## 30186\n",
      "## 30187\n",
      "## 30188\n",
      "## 30189\n",
      "## 30190\n",
      "## 30191\n",
      "## 30192\n",
      "## 30193\n",
      "## 30194\n",
      "## 30195\n",
      "## 30196\n",
      "## 30197\n",
      "## 30198\n",
      "## 30199\n",
      "## 30200\n",
      "## 30201\n",
      "## 30202\n",
      "## 30203\n",
      "## 30204\n",
      "## 30205\n",
      "## 30206\n",
      "## 30207\n",
      "## 30208\n",
      "## 30209\n",
      "## 30210\n",
      "## 30211\n",
      "## 30212\n",
      "## 30213\n",
      "## 30214\n",
      "## 30215\n",
      "## 30216\n",
      "## 30217\n",
      "## 30218\n",
      "## 30219\n",
      "## 30220\n",
      "## 30221\n",
      "## 30222\n",
      "## 30223\n",
      "## 30224\n",
      "## 30225\n",
      "## 30226\n",
      "## 30227\n",
      "## 30228\n",
      "## 30229\n",
      "## 30230\n",
      "## 30231\n",
      "## 30232\n",
      "## 30233\n",
      "## 30234\n",
      "## 30235\n",
      "## 30236\n",
      "## 30237\n",
      "## 30238\n",
      "## 30239\n",
      "## 30240\n",
      "## 30241\n",
      "## 30242\n",
      "## 30243\n",
      "## 30244\n",
      "## 30245\n",
      "## 30246\n",
      "## 30247\n",
      "## 30248\n",
      "## 30249\n",
      "## 30250\n",
      "## 30251\n",
      "## 30252\n",
      "## 30253\n",
      "## 30254\n",
      "## 30255\n",
      "## 30256\n",
      "## 30257\n",
      "## 30258\n",
      "## 30259\n",
      "## 30260\n",
      "## 30261\n",
      "## 30262\n",
      "## 30263\n",
      "## 30264\n",
      "## 30265\n",
      "## 30266\n",
      "## 30267\n",
      "## 30268\n",
      "## 30269\n",
      "## 30270\n",
      "## 30271\n",
      "## 30272\n",
      "## 30273\n",
      "## 30274\n",
      "## 30275\n",
      "## 30276\n",
      "## 30277\n",
      "## 30278\n",
      "## 30279\n",
      "## 30280\n",
      "## 30281\n",
      "## 30282\n",
      "## 30283\n",
      "## 30284\n",
      "## 30285\n",
      "## 30286\n",
      "## 30287\n",
      "## 30288\n",
      "## 30289\n",
      "## 30290\n",
      "## 30291\n",
      "## 30292\n",
      "## 30293\n",
      "## 30294\n",
      "## 30295\n",
      "## 30296\n",
      "## 30297\n",
      "## 30298\n",
      "## 30299\n",
      "## 30300\n",
      "## 30301\n",
      "## 30302\n",
      "## 30303\n",
      "## 30304\n",
      "## 30305\n",
      "## 30306\n",
      "## 30307\n",
      "## 30308\n",
      "## 30309\n",
      "## 30310\n",
      "## 30311\n",
      "## 30312\n",
      "## 30313\n",
      "## 30314\n",
      "## 30315\n",
      "## 30316\n",
      "## 30317\n",
      "## 30318\n",
      "## 30319\n",
      "## 30320\n",
      "## 30321\n",
      "## 30322\n",
      "## 30323\n",
      "## 30324\n",
      "## 30325\n",
      "## 30326\n",
      "## 30327\n",
      "## 30328\n",
      "## 30329\n",
      "## 30330\n",
      "## 30331\n",
      "## 30332\n",
      "## 30333\n",
      "## 30334\n",
      "## 30335\n",
      "## 30336\n",
      "## 30337\n",
      "## 30338\n",
      "## 30339\n",
      "## 30340\n",
      "## 30341\n",
      "## 30342\n",
      "## 30343\n",
      "## 30344\n",
      "## 30345\n",
      "## 30346\n",
      "## 30347\n",
      "## 30348\n",
      "## 30349\n",
      "## 30350\n",
      "## 30351\n",
      "## 30352\n",
      "## 30353\n",
      "## 30354\n",
      "## 30355\n",
      "## 30356\n",
      "## 30357\n",
      "## 30358\n",
      "## 30359\n",
      "## 30360\n",
      "## 30361\n",
      "## 30362\n",
      "## 30363\n",
      "## 30364\n",
      "## 30365\n",
      "## 30366\n",
      "## 30367\n",
      "## 30368\n",
      "## 30369\n",
      "## 30370\n",
      "## 30371\n",
      "## 30372\n",
      "## 30373\n",
      "## 30374\n",
      "## 30375\n",
      "## 30376\n",
      "## 30377\n",
      "## 30378\n",
      "## 30379\n",
      "## 30380\n",
      "## 30381\n",
      "## 30382\n",
      "## 30383\n",
      "## 30384\n",
      "## 30385\n",
      "## 30386\n",
      "## 30387\n",
      "## 30388\n",
      "## 30389\n",
      "## 30390\n",
      "## 30391\n",
      "## 30392\n",
      "## 30393\n",
      "## 30394\n",
      "## 30395\n",
      "## 30396\n",
      "## 30397\n",
      "## 30398\n",
      "## 30399\n",
      "## 30400\n",
      "## 30401\n",
      "## 30402\n",
      "## 30403\n",
      "## 30404\n",
      "## 30405\n",
      "## 30406\n",
      "## 30407\n",
      "## 30408\n",
      "## 30409\n",
      "## 30410\n",
      "## 30411\n",
      "## 30412\n",
      "## 30413\n",
      "## 30414\n",
      "## 30415\n",
      "## 30416\n",
      "## 30417\n",
      "## 30418\n",
      "## 30419\n",
      "## 30420\n",
      "## 30421\n",
      "## 30422\n",
      "## 30423\n",
      "## 30424\n",
      "## 30425\n",
      "## 30426\n",
      "## 30427\n",
      "## 30428\n",
      "## 30429\n",
      "## 30430\n",
      "## 30431\n",
      "## 30432\n",
      "## 30433\n",
      "## 30434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 30435\n",
      "## 30436\n",
      "## 30437\n",
      "## 30438\n",
      "## 30439\n",
      "## 30440\n",
      "## 30441\n",
      "## 30442\n",
      "## 30443\n",
      "## 30444\n",
      "## 30445\n",
      "## 30446\n",
      "## 30447\n",
      "## 30448\n",
      "## 30449\n",
      "## 30450\n",
      "## 30451\n",
      "## 30452\n",
      "## 30453\n",
      "## 30454\n",
      "## 30455\n",
      "## 30456\n",
      "## 30457\n",
      "## 30458\n",
      "## 30459\n",
      "## 30460\n",
      "## 30461\n",
      "## 30462\n",
      "## 30463\n",
      "## 30464\n",
      "## 30465\n",
      "## 30466\n",
      "## 30467\n",
      "## 30468\n",
      "## 30469\n",
      "## 30470\n",
      "## 30471\n",
      "## 30472\n",
      "## 30473\n",
      "## 30474\n",
      "## 30475\n",
      "## 30476\n",
      "## 30477\n",
      "## 30478\n",
      "## 30479\n",
      "## 30480\n",
      "## 30481\n",
      "## 30482\n",
      "## 30483\n",
      "## 30484\n",
      "## 30485\n",
      "## 30486\n",
      "## 30487\n",
      "## 30488\n",
      "## 30489\n",
      "## 30490\n",
      "## 30491\n",
      "## 30492\n",
      "## 30493\n",
      "## 30494\n",
      "## 30495\n",
      "## 30496\n",
      "## 30497\n",
      "## 30498\n",
      "## 30499\n",
      "## 30500\n",
      "## 30501\n",
      "## 30502\n",
      "## 30503\n",
      "## 30504\n",
      "## 30505\n",
      "## 30506\n",
      "## 30507\n",
      "## 30508\n",
      "## 30509\n",
      "## 30510\n",
      "## 30511\n",
      "## 30512\n",
      "## 30513\n",
      "## 30514\n",
      "## 30515\n",
      "## 30516\n",
      "## 30517\n",
      "## 30518\n",
      "## 30519\n",
      "## 30520\n",
      "## 30521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2689"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_used = 0\n",
    "for k,v in tokenizer.vocab.items():\n",
    "    if not v in subset_ratio_100M['subset_present_tokens']:\n",
    "        not_used += 1\n",
    "        print(k,v)\n",
    "not_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27833"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset_ratio_100M['subset_present_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_metadata(subset_dict):\n",
    "    '''\n",
    "    prints:\n",
    "    Number of books used in subset\n",
    "    Number of tokens present in subset\n",
    "    Number of tokens represented by subset\n",
    "    '''\n",
    "    print(len(subset_dict['subset_booklist']))\n",
    "    print(subset_dict['subset_total_tokens'])\n",
    "    print(subset_dict['subset_unique_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "99974.0\n",
      "13040\n"
     ]
    }
   ],
   "source": [
    "subset_metadata(subset_ratio_100K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "999825.0\n",
      "24294\n"
     ]
    }
   ],
   "source": [
    "subset_metadata(subset_ratio_1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656\n",
      "9977907.0\n",
      "27607\n"
     ]
    }
   ],
   "source": [
    "subset_metadata(subset_ratio_10M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828\n",
      "28660288.0\n",
      "27833\n"
     ]
    }
   ],
   "source": [
    "subset_metadata(subset_ratio_100M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' a b c d e f g h i j k l m n o p q r s t u v w x y z &.',\n",
       " 'Online Distributed Proofreading Team at http://www.pgdp.net (This file was produced from images generously made available by The Internet Archive/American Libraries.)',\n",
       " 'fi ff fl ffl ffi.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It seems the cleaner leaves in some other stuff, we leave this in given that it includes the alphabet.\n",
    "super_cleaner(load_etext(23594), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id = 22818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stored_sentences(book_id):\n",
    "    print(book_id)\n",
    "    filenames = ['sentences_8.pkl', 'sentences_32.pkl', 'sentences_128.pkl']\n",
    "    for file in filenames:\n",
    "        with open(os.path.join('../pretraining_data_chunked', str(book_id), file), 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "            print(sentences)\n",
    "    print('==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n",
      "['by Virgil']\n",
      "[]\n",
      "[]\n",
      "==============\n",
      "22818\n",
      "['   An Alphabet   of Celebrities']\n",
      "[]\n",
      "[]\n",
      "==============\n",
      "22335\n",
      "[]\n",
      "['Transcriber\\'s Note: Original spells the title \"Nursury.\" This was retained.']\n",
      "[]\n",
      "==============\n",
      "23594\n",
      "[]\n",
      "[' a b c d e f g h i j k l m n o p q r s t u v w x y z &.', 'fi ff fl ffl ffi.']\n",
      "['Online Distributed Proofreading Team at http://www.pgdp.net (This file was produced from images generously made available by The Internet Archive/American Libraries.)']\n",
      "==============\n",
      "20086\n",
      "[]\n",
      "['       Where differences between the list of illustrations and the       caption text existed in the original the most comprehensive       description was used for both.']\n",
      "[]\n",
      "==============\n",
      "20360\n",
      "[]\n",
      "[\"   Entered at Stationer's Hall\", '  P. 13, l. 7, for mighty read magick.']\n",
      "['      Go we to the Committee room,     There gleams of light conflict with gloom,     While unread rheams in chaos lye,     Our water closets to supply.', \"    Noodles{3}, who rave for abolition     Of th' African's improv'd condition{4},     At your own cost fine projects try;     Dont rob--from pure humanity.\"]\n",
      "==============\n",
      "10557\n",
      "['And improved his little Garden.', \"Didn't understand her;\", 'Got entangled with the rake', 'Had a very nasty knock;', 'Looked quite regal', 'Made a very big Meal;', 'Sang a sentimental Air,', 'Tried to paint the Roses blue', 'Used him for a pillow;', 'Went to sleep,']\n",
      "['\"We\\'ll never come again', 'And the Cockatoo Said \"Comment vous portez vous?\"', 'And the Reindeer Said: \"I\\'m sorry for your pain, dear!\"', 'But the Flamingo Talked the same lingo', 'But the Giraffe Was inclined to laugh;', 'Of Johnny Crow and his Garden.', 'Said: \"Wake me if for talk you pine!\"', 'So the Chimpanzee Put the Kettle on for Tea;', 'Then they picked the Flowers, and Wandered in the Maze, And before they went their several ways', 'They all joined together In a Hearty Vote of Praise', 'Till the Camel Swallowed the Enamel.']\n",
      "[]\n",
      "==============\n",
      "19571\n",
      "[' Audio formats available:']\n",
      "[' This audio reading of A Noiseless Patient Spider is read by', 'Librivox volunteers bring you eight different readings of Walt Whitmans A Noiseless Patient Spider, a weekly poetry project.']\n",
      "['128kbit MP3 - MP3 subfolder 64kbit Ogg Vorbis (variable bit rate) - OGG subfolder Apple AAC audiobook (16kbit mono) - M4B subfolder Speex - SPX subfolder', 'Dedicator recognizes that, once placed in the public domain, the Work may be freely reproduced, distributed, transmitted, used, modified, built upon, or otherwise exploited by anyone for any purpose, commercial or non-commercial, and in any way, including by methods that have not yet been invented or conceived.']\n",
      "==============\n",
      "19177\n",
      "['13 Come Lasses and Lads', '7 The Queen of Hearts']\n",
      "['10 Hey-Diddle-Diddle and Baby Bunting', '14 Ride a Cock Horse to Banbury Cross, &c.', '3 The Babes in the Wood', '6 Sing a Song for Sixpence', 'Collection of Pictures and Songs No. 1 containing the first 8 books listed above with their Color Pictures and numerous Outline Sketches', 'Collection of Pictures and Songs No. 2 containing the second 8 books listed above with their Color Pictures and numerous Outline Sketches']\n",
      "[' \"The humour of Randolph Caldecott\\'s drawings is simply irresistible, no healthy-minded man, woman, or child could look at them without laughing.\"']\n",
      "==============\n",
      "14100\n",
      "[]\n",
      "[]\n",
      "[\"With throbbing bosoms shall the wanderers tread The hallowed mansions of the silent dead, Shall enter the long isle and vaulted dome Where Genius and where Valour find a home; Awe-struck, midst chill sepulchral marbles breathe, Where all above is still, as all beneath; Bend at each antique shrine, and frequent turn To clasp with fond delight some sculptured urn, The ponderous mass of Johnson's form to greet, Or breathe the prayer at Howard's sainted feet.\", \"Yes, thou must droop; thy Midas dream is o'er; The golden tide of Commerce leaves thy shore, Leaves thee to prove the alternate ills that haunt      [6] Enfeebling Luxury and ghastly Want; Leaves thee, perhaps, to visit distant lands, And deal the gifts of Heaven with equal hands.\", \"Bounteous in vain, with frantic man at strife, Glad Nature pours the means--the joys of life;In vain with orange blossoms scents the gale, The hills with olives clothes, with corn the vale; Man calls to Famine, nor invokes in vain, Disease and Rapine follow in her train; The tramp of marching hosts disturbs the plough, The sword, not sickle, reaps the harvest now, And where the Soldier gleans the scant supply.The helpless Peasant but retires to die; No laws his hut from licensed outrage shield,           [3] And war's least horror is the ensanguined field.\", '', \"There walks a Spirit o'er the peopled earth, Secret his progress is, unknown his birth; Moody and viewless as the changing wind, No force arrests his foot, no chains can bind; Where'er he turns, the human brute awakes, And, roused to better life, his sordid hut forsakes: He thinks, he reasons, glows with purer fires, Feels finer wants, and burns with new desires: Obedient Nature follows where he leads; The steaming marsh is changed to fruitful meads; The beasts retire from man's asserted reign, And prove his kingdom was not given in vain.\", \"Then from its bed is drawn the ponderous ore,           [18]Then Commerce pours her gifts on every shore, Then Babel's towers and terrassed gardens rise, And pointed obelisks invade the skies; The prince commands, in Tyrian purple drest, And gypt's virgins weave the linen vest. Then spans the graceful arch the roaring tide, And stricter bounds the cultured fields divide.Then kindles Fancy, then expands the heart, Then blow the flowers of Genius and of Art; Saints, Heroes, Sages, who the land adorn, Seem rather to descend than to be born;\", 'Whilst History, midst the rolls consigned to fame, With pen of adamant inscribes their name.']\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "for book_id in subset_ratio_100K['subset_booklist'][:10]:\n",
    "    stored_sentences(book_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original plan for tokenization may not work as well as desired for certain books with specific text entries\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take for example book 23880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23880\n",
      "[]\n",
      "['Lasiurus borealis ornatus new subspecies', 'The Mexican red bat, thus, is left without a name, and for it I propose', 'University of Kansas Publications Museum of Natural History Volume 5, No. 14, pp. 223-226 December 15, 1951', 'Volume 5, No. 14, pp. 223-226 December 15, 1951']\n",
      "['Accordingly, the name A[talapha]. mexicana Saussure 1861 falls as a synonym of Lasiurus cinereus cinereus (Beauvois 1796); if the hoary bat of the southern end of the Mexican table land should prove to be subspecifically separable, the name Lasiurus cinereus mexicanus would be available for it.', 'As may be readily seen by comparing specimens of L. borealis and L. cinereus from Mexico (or also from any place in North America north of Mexico), the description by Saussure applies to the hoary bat (Lasiurus cinereus) and not to the red bat (Lasiurus borealis).', 'Long inrolled tail; femoral patagium as in the vespertilios. Teeth 4/2, 1/1, 4/5 or 5/5.']\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "stored_sentences(23880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101.,   103.,  1010.,  ...,     0.,     0.,     0.],\n",
       "         [  101., 11914.,   103.,  ...,     0.,     0.,     0.],\n",
       "         [  101., 11914.,  1010.,  ...,     0.,     0.,     0.],\n",
       "         ...,\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.],\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.],\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.]]),\n",
       " 'attention_mask': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]]),\n",
       " 'labels': tensor([[  101., 11914.,  1010.,  ...,     0.,     0.,     0.],\n",
       "         [  101., 11914.,  1010.,  ...,     0.,     0.,     0.],\n",
       "         [  101., 11914.,  1010.,  ...,     0.,     0.,     0.],\n",
       "         ...,\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.],\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.],\n",
       "         [  101.,  2146.,  1999.,  ...,     0.,     0.,     0.]])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors = torch.load('../pretraining_data_chunked/23880/tensors_128.pt')\n",
    "tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the ratios as given at the end of one of the longer sentences gets masked because of how we replace text by masks in the sentence (a result of the whole-word mask strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] as may be readily seen by comparing specimens of l . borealis and l . cinereus from mexico ( or also from any place in north america north of mexico ) , the description by saussure applies to the hoary bat ( lasiurus cinereus ) and [MASK] to the red bat ( lasiurus borealis ) . [SEP]\n",
      "[CLS] as may be readily see by compare specimen of l . boreali and l . cinereus from mexico ( or also from any place in north america north of mexico ) , the description by saussure apply to the hoary bat ( lasiurus cinereus ) and not to the red bat ( lasiurus boreali ) . [SEP]\n",
      "[CLS] as may be readily seen by comparing specimens of l . borealis and l . cinereus from mexico ( or also from any place in north america north of mexico ) , the description by saussure applies to the hoary bat ( lasiurus cinereus ) and not to the red bat ( lasiurus borealis ) . [SEP]\n",
      "[CLS] [MASK] inrolled tail ; femoral patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long [MASK] [MASK] [MASK] tail ; femoral patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled [MASK] ; femoral [MASK] [MASK] [MASK] as in the [MASK] [MASK] [MASK] [MASK] [MASK] . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail [MASK] femoral patagium as in the vespertilios [MASK] teeth 4 / 2 [MASK] 1 / 1 [MASK] 4 / 5 or 5 / 5 [MASK] [SEP]\n",
      "[CLS] long inrolled tail ; [MASK] [MASK] patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long [MASK] rolled tail ; femoral patagium [MASK] [MASK] the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail ; femoral patagium as in [MASK] vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail ; femoral patagium as in the vespertilios . [MASK] 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail ; femoral patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail ; femoral patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 [MASK] 5 / 5 . [SEP]\n",
      "[CLS] long inrolle tail ; femoral patagium as in the vespertilio . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n",
      "[CLS] long inrolled tail ; femoral patagium as in the vespertilios . teeth 4 / 2 , 1 / 1 , 4 / 5 or 5 / 5 . [SEP]\n"
     ]
    }
   ],
   "source": [
    "for row in tensors['input_ids'][-15:]:\n",
    "    print(tokenizer.convert_tokens_to_string([x for x in tokenizer.convert_ids_to_tokens(row) if x != '[PAD]']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Long inrolled tail; femoral patagium as in the vespertilios. Teeth 4/2, 1/1, 4/5 or 5/5.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'long', 'in', '##roll', '##ed', 'tail', ';', 'fe', '##moral', 'pat', '##agi', '##um', 'as', 'in', 'the', 've', '##sper', '##ti', '##lio', '##s', '.', 'teeth', '4', '/', '2', ',', '1', '/', '1', ',', '4', '/', '5', 'or', '5', '/', '5', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenizer('Long inrolled tail; femoral patagium as in the vespertilios. Teeth 4/2, 1/1, 4/5 or 5/5.')['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long | ADV | long\n",
      "inrolled | VERB | inrolle\n",
      "tail | NOUN | tail\n",
      "; | PUNCT | ;\n",
      "femoral | ADJ | femoral\n",
      "patagium | NOUN | patagium\n",
      "as | ADP | as\n",
      "in | ADP | in\n",
      "the | DET | the\n",
      "vespertilios | NOUN | vespertilio\n",
      ". | PUNCT | .\n",
      "Teeth | PROPN | Teeth\n",
      "4/2 | NUM | 4/2\n",
      ", | PUNCT | ,\n",
      "1/1 | NUM | 1/1\n",
      ", | PUNCT | ,\n",
      "4/5 | NUM | 4/5\n",
      "or | CCONJ | or\n",
      "5/5 | NUM | 5/5\n",
      ". | PUNCT | .\n"
     ]
    }
   ],
   "source": [
    "#Text is parsed in 1 go by Spacy, but is recognized as seperate tokens by BERT\n",
    "for token in doc:\n",
    "    print(token.text, '|', token.pos_, '|', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10625', '22', '19447', '19217', '15476']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_ratio_100M['subset_booklist'][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro M1200'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Anne went to the Albert Heijn at 5 o'clock to buy some milk for me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 4776, 2253, 2000, 1996, 4789, 2002, 28418, 2078, 2012, 1019, 1051, 1005, 5119, 2000, 4965, 2070, 6501, 2005, 2033, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "default_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,  2253,  2000,  1996,   103,   103,   103,   103,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,   103,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,   103,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,   103,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,   103,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,   103,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,   103,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "           103,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,   103,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,   103,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,   103,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,   103,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "           103,   102],\n",
       "        [  101,  4776,  2175,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  1045,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1051,  1005,  5119,  1019,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102],\n",
       "        [  101,  4776,  2253,  2000,  1996,  4789,  2002, 28418,  2078,  2012,\n",
       "          1019,  1051,  1005,  5119,  2000,  4965,  2070,  6501,  2005,  2033,\n",
       "          1012,   102]])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ST_tokenizer = StrategizedTokenizer(padding=True)\n",
    "inputs = ST_tokenizer.tokenize(text)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1344\n",
    "#14596\n",
    "\n",
    "test_book = super_cleaner(load_etext(14596), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7685"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_sentence = str(test_book[np.argmax([len(par) for par in test_book])])\n",
    "len(longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC = SentenceChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens, sentences = SC.sentence_chunker(longest_sentence, 512, return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is another expression which must be considered in connexion with the medival doctrine of deification. This is the intellectus agens, or [Greek: nous poitikos], which began its long history in Aristotle (De Anima, iii. 5). Aristotle there distinguishes two forms of Reason, which are related to each other as form and matter. Reason becomes all things, for the matter of anything is potentially the whole class to which it belongs; but Reason also makes all things, that is to say, it communicates to things those categories by which they become objects of thought. This higher Reason is separate and impassible ([Greek: christos kai amigs kai apaths]); it is eternal and immortal; while the passive reason perishes with the body. The creative Reason is immanent both in the human mind and in the external world; and thus only is it possible for the mind to know things. Unfortunately, Aristotle says very little more about his [Greek: nous poitikos], and does not explain how the two Reasons are related to each other, thereby leaving the problem for his successors to work out. The most fruitful attempt to form a consistent theory, on an idealistic basis, out of the ambiguous and perhaps irreconcilable statements in the De Anima, was made by Alexander of Aphrodisias (about 200 A.D.), who taught that the Active Reason \"is not a part or faculty of our soul, but comes to us from without\"--it is, in fact, identified with the Spirit of God working in us. Whether Aristotle would have accepted this interpretation of his theory may be doubted; but the commentary of Alexander of Aphrodisias was translated into Arabic, and this view of the Active Reason became the basis of the philosophy of Averroes. Averroes teaches that it is possible for the passive reason to unite itself with the Active Reason, and that this union may be attained or prepared for by ascetic purification and study. But he denies that the passive reason is perishable, not wishing entirely to depersonalise man. Herein he follows, he says, Themistius, whose views he tries to combine with those of Alexander.Avicenna introduces a celestial hierarchy, in which the higher intelligences shed their light upon the lower, till they reach the Active Reason, which lies nearest to man, \"a quo, ut ipse dicit, effluunt species intelligibiles in animas nostras\" (Aquinas).',\n",
       " 'The doctrine of \"monopsychism\" was, of course, condemned by the Church. Aquinas makes both the Active and Passive Reason parts of the human soul. Eckhart, as I have said in the fourth Lecture, at one period of his teaching expressly identifies the \"intellectus agens\" with the \"spark,\" in reference to which he says that \"here God\\'s ground is my ground, and my ground God\\'s ground. \" This doctrine of the Divinity of the ground of the soul is very like the Cabbalistic doctrine of the Neschamah, and the Neoplatonic doctrine of [Greek: Nous] (cf. Stckl, vol. ii. p. 1007). Eckhart was condemned for saying, \"aliquid est in anima quod est increatum et increabile; si tota anima esset talis, esset increata et increabilis. Hoc est intellectus. \" Eckhart certainly says explicitly that \"as fire turns all that it touches into itself, so the birth of the Son of God in the soul turns us into God, so that God no longer knows anything in us but His Son. \" Man thus becomes \"filius naturalis Dei,\" instead of only \"filius adoptivus. \" We have seen that Eckhart, towards the end of his life, inclined more and more to separate the spark, the organ of Divine contemplation, from the reason. This is, of course, an approximation to the other view of deification--that of substitution or miraculous infusion from without, unless we see in it a tendency to divorce the personality from the reason. Ruysbroek states his doctrine of the Divine spark very clearly: \"The unity of our spirit in God exists in two ways, essentially and actively. The essential existence of the soul, qu secundum ternam ideam in Deo nos sumus, itemque quam in nobis habemus, medii ac discriminis expers est. Spiritus Deum in nuda natura essentialiter possidet, et spiritum Deus.Vivit namque in Deo et Deus in ipso; et secundum supremam sui partem Dei claritatem suscipere absque medio idoneus est; quin etiam per terni exemplaris sui claritudinem essentialiter ac personaliter in ipso lucentis, secundum supremam vivacitatis su portionem, in divinam sese demittit ac demergit essentiam, ibidemque perseveranter secundum ideam manendo ternam suam possidet beatitudinem; rursusque cum creaturis omnibus per ternam Verbi generationem inde emanans, in esse suo creato constituitur.',\n",
       " 'The \"natural union,\" though it is the first cause of all holiness and blessedness, does not make us holy and blessed, being common to good and bad alike. \"Similitude\" to God is the work of grace, \"qu lux qudam deiformis est. \" We cannot lose the \"unitas,\" but we can lose the \"similitudo qu est gratia. \" The highest part of the soul is capable of receiving a perfect and immediate impression of the Divine essence; by this \"apex mentis\" we may \"sink into the Divine essence, and by a new (continuous) creation return to our created being according to the idea of God. \" The question whether the \"ground of the soul\" is created or not is obviously a form of the question which we are now discussing. Giseler, as I have said, holds that it was created with the soul. Sterngassen says: \"That which God has in eternity in uncreated wise, that has the soul in time in created wise.\"But the author of the Treatise on Love, which belongs to this period, speaks of the spark as \"the Active Reason, which is God.\"And again, \"This is the Uncreated in the soul of which Master Eckhart speaks. \" Suso seems to imply that he believed the ground of the soul to be uncreated, an emanation of the Divine nature; and Tauler uses similar language. Ruysbroek, in the last chapter of the Spiritual Nuptials, says that contemplative men \"see that they are the same simple ground as to their uncreated nature, and are one with the same light by which they see, and which they see. \" The later German mystics taught that the Divine essence is the material substratum of the world, the creative will of God having, so to speak, alienated for the purpose a portion of His own essence. If, then, the created form is broken through, God Himself becomes the ground of the soul. Even Augustine countenances some such notion when he says, \"From a good man, or from a good angel, take away \\'man\\' or \\'angel,\\' and you find God.\"But one of the chief differences between the older and later Mysticism is that the former regarded union with God as achieved through the faculties of the soul, the latter as inherent in its essence.The doctrine of immanence, more and more emphasised, tended to encourage the belief that the Divine element in the soul is not merely something potential, something which the faculties may acquire, but is immanent and basal.',\n",
       " 'Tauler mentions both views, and prefers the latter. Some hesitation may be traced in the Theologia Germanica on this point (p. 109, \"Golden Treasury\" edition): \"The true light is that eternal Light which is God; or else it is a created light, but yet Divine, which is called grace. \" Our Cambridge Platonists naturally revived this Platonic doctrine of deification, much to the dissatisfaction of some of their contemporaries. Tuckney speaks of their teaching as \"a kind of moral divinity minted only with a little tincture of Christ added. Nay, a Platonic faith unites to God!\"Notwithstanding such protests, the Platonists persisted that all true happiness consists in a participation of God; and that \"we cannot enjoy God by any external conjunction with Him.\"']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['232', '22818', '22335', '23594', '20086', '20360', '10557', '19571', '19177', '14100', '13536', '23436', '129', '21783', '11006', '19937', '22847', '1321', '23147', '21805', '22529', '12474', '13082', '14463', '23538', '13081', '116', '18589', '23446', '23450', '17124', '16780', '23146', '18935', '12554', '17254', '23429', '13203', '17365', '22236', '16169', '18417', '22579', '19634', '24044', '104', '1567', '23315', '24269', '12358', '23880']\n"
     ]
    }
   ],
   "source": [
    "print(subset_ratio_100K['subset_booklist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-18 10:43:48.084747\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_data_splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-3ccf67576d31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext_splits_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_data_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14596\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_lengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'chunk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtext_splits_trunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_data_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14596\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_lengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_data_splits' is not defined"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "text_splits_chunk = make_data_splits(14596, max_seq_lengths=[8,32,128], truncate='chunk')\n",
    "text_splits_trunc = make_data_splits(14596, max_seq_lengths=[8,32,128], truncate=True)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../pretraining_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StrategizedTokenizerDataset()\n",
    "train_dataset.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        examples = torch.tensor(np.array([[101, 1996, 2622, 9535, 11029, 26885, 1997, 102, 0, 0, 0, 0, 0], \n",
    "                             [101,2198, 9535, 11029, 1010, 2011, 8965, 3854, 22033, 9050, 3064, 102, 0],\n",
    "                             [101, 2102, 2023, 26885, 2003, 2005, 1996, 2224, 1997, 3087, 5973, 2012, 102]])).long()\n",
    "        self.encodings = examples\n",
    "        self.labels = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {'input_ids': self.encodings[i],\n",
    "                'labels': self.labels[i]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ExampleListDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1996,  2622,  9535, 11029, 26885,  1997,   102,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2198,  9535, 11029,  1010,  2011,  8965,  3854, 22033,  9050,\n",
       "          3064,   102,     0],\n",
       "        [  101,  2102,  2023, 26885,  2003,  2005,  1996,  2224,  1997,  3087,\n",
       "          5973,  2012,   102]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny_config = {\"hidden_size\": 128, \n",
    "                    \"hidden_act\": \"gelu\", \n",
    "                    \"initializer_range\": 0.02, \n",
    "                    \"vocab_size\": 30522, \n",
    "                    \"hidden_dropout_prob\": 0.1, \n",
    "                    \"num_attention_heads\": 2, \n",
    "                    \"type_vocab_size\": 2, \n",
    "                    \"max_position_embeddings\": 128, \n",
    "                    \"num_hidden_layers\": 2, \n",
    "                    \"intermediate_size\": 512, \n",
    "                    \"attention_probs_dropout_prob\": 0.1}\n",
    "\n",
    "model = BertForMaskedLM(config=BertConfig(**bert_tiny_config))\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:06, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.390600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.451000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = './test_experiment'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= os.path.join(output_dir, 'model'),          # output directory\n",
    "    save_strategy='no',  #dont make checkpoints, easier to just retrain than continu given the experiment\n",
    "    max_steps = 3,\n",
    "    per_device_train_batch_size=2,  # batch size per device during training\n",
    "    #per_device_eval_batch_size=256,   # batch size for evaluation\n",
    "    learning_rate=1e-5,     \n",
    "    logging_dir= os.path.join(output_dir, 'model', 'logs'),            # directory for storing logs\n",
    "    logging_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    model=model,                         # the instantiated  Transformers model to be trained\n",
    "    train_dataset=train_data,         # training dataset\n",
    "    eval_dataset=None            # evaluation dataset\n",
    ")\n",
    "\n",
    "train_output = trainer.train()\n",
    "#trainer.save_model(os.path.join(output_dir, 'model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(output_dir, 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=10.423988660176596, metrics={'train_runtime': 11.3354, 'train_samples_per_second': 0.265, 'total_flos': 1703342940.0, 'epoch': 1.5, 'init_mem_cpu_alloc_delta': 1116446720, 'init_mem_gpu_alloc_delta': 17471488, 'init_mem_cpu_peaked_delta': 15683584, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 128274432, 'train_mem_gpu_alloc_delta': 52412928, 'train_mem_cpu_peaked_delta': 20480, 'train_mem_gpu_peaked_delta': 46882816})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir './logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array([[101, 1996, 2622, 9535, 11029, 26885, 1997], \n",
    "         [101,2198, 9535, 11029, 1010, 2011, 8965, 3854, 22033, 9050, 3064, 102],\n",
    "         [101, 2102, 2023, 26885, 2003, 2005, 1996, 2224, 1997, 3087, 5973, 2012, 102]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset import StrategizedTokenizerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**custom_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLUE and SentEval benchmarking\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-816fbf17191f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to C:\\Users\\s145733\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87a1478a82449b99c9276c1f872ace2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=376971.0, style=ProgressStyle(descripti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to C:\\Users\\s145733\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset('glue', 'cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\s145733\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jiant'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-733335fb88de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjiant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrunscript\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjiant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunscript\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jiant'"
     ]
    }
   ],
   "source": [
    "from jiant.proj.simple import runscript as run\n",
    "import jiant.scripts.download_data.runscript as downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(128, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=128, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_DIR = \"../GLUE\"\n",
    "\n",
    "# Download the Data\n",
    "downloader.download_data([\"mrpc\"], f\"{EXP_DIR}/mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the arguments for the Simple API\n",
    "args = run.RunConfiguration(\n",
    "   run_name=\"simple\",\n",
    "   exp_dir=EXP_DIR,\n",
    "   data_dir=f\"{EXP_DIR}/tasks\",\n",
    "   hf_pretrained_model_name_or_path=\"roberta-base\",\n",
    "   tasks=\"mrpc\",\n",
    "   train_batch_size=16,\n",
    "   num_train_epochs=3\n",
    ")\n",
    "\n",
    "# Run!\n",
    "run.run_simple(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
